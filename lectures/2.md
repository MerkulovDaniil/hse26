---
title: Матрично-векторное дифференцирование. Линейный поиск
author: Даня Меркулов
institute: ФКН ВШЭ
format: 
   beamer:
      pdf-engine: xelatex
      aspectratio: 169
      fontsize: 9pt
      section-titles: true
      incremental: true
      include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
header-includes:
  - \newcommand{\bgimage}{../files/back2.jpeg}
---

# Матрично-векторное дифференцирование

## Градиент

:::: {.columns}

::: {.column width="60%"}

Пусть $f(x):\mathbb{R}^n\to\mathbb{R}$, тогда вектор, содержащий все частные производные первого порядка:

$$
\nabla f(x) = \dfrac{df}{dx} = \begin{pmatrix}
    \frac{\partial f}{\partial x_1} \\
    \frac{\partial f}{\partial x_2} \\
 \vdots \\
    \frac{\partial f}{\partial x_n}
\end{pmatrix}
$$

. . .


называется градиентом функции $f(x)$. Этот вектор указывает направление наискорейшего возрастания. Таким образом, вектор $-\nabla f(x)$ указывает направление наискорейшего убывания функции в точке. Кроме того, вектор градиента всегда ортогонален линии уровня в точке.

:::

::: {.column width="40%"}

::: {.callout-example}
Для функции $f(x, y) = x^2 + y^2$ градиент равен: 
$$
\nabla f(x, y) =
\begin{bmatrix}
2x \\
2y \\
\end{bmatrix}
$$
Он указывает направление наискорейшего возрастания функции.
:::

::: {.callout-question} 
Как связана норма градиента с крутизной функции?
:::
:::

::::


## Гессиан

:::: {.columns}

::: {.column width="60%"}

Пусть $f(x):\mathbb{R}^n\to\mathbb{R}$, тогда матрица, содержащая все частные производные второго порядка:



$$
\nabla^2 f(x) = \left[\frac{\partial^2 f}{\partial x_i\partial x_j}\right]_{i,j=1}^n = \begin{pmatrix}
    \frac{\partial^2 f}{\partial x_1 \partial x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_1\partial x_n} \\
    \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2 \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
 \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \dots  & \frac{\partial^2 f}{\partial x_n \partial x_n}
\end{pmatrix}
$$

. . .


Если $f:\mathbb{R}^n\to\mathbb{R}^m$, то вторые производные образуют тензор третьего порядка. Его $(k)$-й срез - гессиан скалярной функции $f_k: \nabla^2 f_k(x)$.

:::

::: {.column width="40%"}


::: {.callout-example} 
Для функции $f(x, y) = x^2 + y^2$ гессиан равен:

$$
H_f(x, y) = \begin{bmatrix} 2 & 0 \\
0 & 2 \\
\end{bmatrix}
$$
:::

Эта матрица содержит информацию о кривизне функции в разных направлениях.

::: {.callout-question} 
Как можно использовать гессиан для определения выпуклости или вогнутости функции?
:::
:::
::::


## Теорема Шварца

:::: {.columns}

::: {.column width="50%"}

Пусть есть функция $f: \mathbb{R}^n \rightarrow \mathbb{R}$. Если смешанные частные производные $\frac{\partial^2 f}{\partial x_i \partial x_j}$ и $\frac{\partial^2 f}{\partial x_j \partial x_i}$ непрерывны на открытом множестве, содержащем точку $a$, то они равны в ней. То есть,
$$
\frac{\partial^2 f}{\partial x_i \partial x_j} (a) = \frac{\partial^2 f}{\partial x_j \partial x_i} (a)
$$

. . .

То есть, гессиан симметричен:

$$
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i} \quad \nabla^2 f(x)  =(\nabla^2 f(x))^T
$$

Эта симметричность упрощает вычисления и анализ, связанные с гессианом в различных приложениях, особенно в оптимизации.

:::

::: {.column width="50%"}
::: {.callout-example}

## [Контрпример Шварца](https://fmin.xyz/docs/theory/Matrix_calculus.html#hessian)

$$
f(x,y) = 
\begin{cases}
    \frac{xy\left(x^2 - y^2\right)}{x^2 + y^2} & \text{ для } (x,\, y) \ne (0,\, 0),\\
    0 & \text{ для } (x, y) = (0, 0).
\end{cases}
$$
![](schwartz.pdf){width=90%}

Можно проверить, что $\frac{\partial^2 f}{ \partial x \partial y} (0, 0) \neq \frac{\partial^2 f}{ \partial y \partial x} (0, 0)$, хотя смешанные частные производные существуют, и в во всех остальных точках симметричность выполняется.
:::
:::

::::

## Якобиан


:::: {.columns}

::: {.column width="50%"}

Обобщением понятия градиента на случай векторнозначной функции $f(x):\mathbb{R}^n\to\mathbb{R}^m$ является следующая матрица:

$$
J_f = f'(x) = \dfrac{df}{dx^T} = \begin{pmatrix}
    \frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} & \dots  & \frac{\partial f_m}{\partial x_1} \\
    \frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} & \dots  & \frac{\partial f_m}{\partial x_2} \\
 \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial f_1}{\partial x_n} & \frac{\partial f_2}{\partial x_n} & \dots  & \frac{\partial f_m}{\partial x_n}
\end{pmatrix}
$$


Она содержит информацию о скорости изменения функции по входным переменным.

::: {.callout-question} 
Можно ли связать эти три определения выше (градиент, якобиан и гессиан) с помощью одного утверждения?
:::

:::

::: {.column width="50%"}

::: {.callout-example}
Для функции  
$$
f(x, y) = \begin{bmatrix}
x + y \\
x - y \\
\end{bmatrix}, 
$$
Якобиан равен: 
$$
J_f(x, y) = \begin{bmatrix}
1 & 1 \\
1 & -1 \\
\end{bmatrix}
$$ 
:::

::: {.callout-question} 
Как матрица Якоби связана с градиентом для скалярных функций?
:::


:::

::::


## Итог

$$
f(x) : X \to Y; \qquad \frac{\partial f(x)}{\partial x} \in G
$$

|             X             |       Y        |             G             |                      Название                       |
|:----------------:|:----------------:|:----------------:|:-----------------:|
|       $\mathbb{R}$        |  $\mathbb{R}$  |       $\mathbb{R}$        |              $f'(x)$ (производная)               |
|      $\mathbb{R}^n$       |  $\mathbb{R}$  |      $\mathbb{R}^n$       |  $\left(\frac{\partial f}{\partial x_i}\right)_{i=1}^n$ (градиент)  |
|      $\mathbb{R}^n$       | $\mathbb{R}^m$ | $\mathbb{R}^{n \times m}$ | $\dfrac{\partial f_i}{\partial x_j}$ (якобиан) |
| $\mathbb{R}^{m \times n}$ |  $\mathbb{R}$  | $\mathbb{R}^{m \times n}$ |      $\dfrac{\partial f}{\partial x_{ij}}$      |

## Аппроксимация Тейлора первого порядка

:::: {.columns}

::: {.column width="70%"}
Аппроксимация Тейлора первого порядка, также известная как линейное приближение, строится вблизи некоторой точки $x_0$. Если $f: \mathbb{R}^n \rightarrow \mathbb{R}$ - дифференцируемая функция, то ее аппроксимация первого порядка задается следующим образом:

$$
f_{x_0}^I(x) = f(x_0) + \nabla f(x_0)^T (x - x_0)
$$

где: 

* $f(x_0)$ - значение функции в точке $x_0$.
* $\nabla f(x_0)$ - градиент функции в точке $x_0$.

. . .

Часто, чтобы упростить теоретический анализ, функцию в окрестности точки заменяют её линейной аппроксимацией.
:::

::: {.column width="30%"}
![Аппроксимация Тейлора первого порядка в окрестности точки $x_0$](first_order_taylor.pdf)
:::

::::

## Аппроксимация Тейлора второго порядка

:::: {.columns}

::: {.column width="70%"}
Аппроксимация Тейлора второго порядка, также известная как квадратичное приближение, использует информацию о кривизне функции. Для дважды дифференцируемой функции $f: \mathbb{R}^n \rightarrow \mathbb{R}$ квадратичная аппроксимация в окрестности $x_0$ имеет вид:

$$
f_{x_0}^{II}(x) = f(x_0) + \nabla f(x_0)^T (x - x_0) + \frac{1}{2} (x - x_0)^T \nabla^2 f(x_0) (x - x_0)
$$

Где $\nabla^2 f(x_0)$ - гессиан функции $f$ в точке $x_0$.

. . .

Когда линейного приближения функции недостаточно, можно рассмотреть замену $f(x)$ на $f_{x_0}^{II}(x)$ в окрестности точки $x_0$. 
:::

::: {.column width="30%"}
![Аппроксимация Тейлора второго порядка в окрестности точки $x_0$](second_order_taylor.pdf)
:::

::::

## Дифференциалы

::: {.callout-theorem}
Пусть $f:U\to V$, $x\in U$ - внутренняя точка. Пусть $D : U \rightarrow V$ - линейный оператор. Мы говорим, что функция $f$ дифференцируема в точке $x$ с производной $D$, если для всех достаточно малых $h, x+h\in U$ выполняется следующее разложение: 
$$ 
f(x + h) = f(x) + D[h] + o(\|h\|)
$$
Если не существует линейного оператора $D$, удовлетворяющего этому разложению, то $f$ не дифференцируема в точке $x$.
:::

## Дифференциалы

После получения дифференциальной записи $df$ мы можем получить градиент, используя следующую формулу:

$$
df(x) = \langle \nabla f(x), dx\rangle
$$

. . .

Далее, если у нас есть дифференциал в такой форме и мы хотим вычислить вторую производную матричной/векторной функции, мы фиксируем первый дифференциал $dx := dx_1$ (т.е. в вычислениях считаем его константой) и берём дифференциал ещё раз $d(df) = d^2f(x)$

$$
d^2f(x) = \langle \nabla^2 f(x) dx_1, dx\rangle = \langle H_f(x) dx_1, dx\rangle
$$

## Свойства дифференциалов

Пусть $A$ и $B$ - постоянные матрицы, а $X$ и $Y$ - переменные (или матричные функции).

:::: {.columns}

::: {.column width="50%"}

- $dA = 0$
- $d(\alpha X) = \alpha (dX)$
- $d(AXB) = A(dX )B$
- $d(X+Y) = dX + dY$
- $d(X^T) = (dX)^T$
- $d(XY) = (dX)Y + X(dY)$
- $d\langle X, Y\rangle = \langle dX, Y\rangle+ \langle X, dY\rangle$

:::

::: {.column width="50%"}

- $d\left( \dfrac{X}{\phi}\right) = \dfrac{\phi dX - (d\phi) X}{\phi^2}$
- $d\left( \det X \right) = \det X \langle X^{-T}, dX \rangle$
- $d\left(\text{tr } X \right) = \langle I, dX\rangle$
- $df(g(x)) = \dfrac{df}{dg} \cdot dg(x)$
- $H = (J(\nabla f))^T$
- $d(X^{-1})=-X^{-1}(dX)X^{-1}$

:::

::::

## Матричное дифференцирование. Пример 1 {.t}

::: {.callout-example}
Найти $df, \nabla f(x)$, если $f(x) = \langle x, Ax\rangle -b^T x + c$. 
:::

## Матричное дифференцирование. Пример 2

::: {.callout-example}
Найти $df, \nabla f(x)$, если $f(x) = \ln \langle x, Ax\rangle$. 
:::

. . .

1. Заметим, что $A$ должна быть положительно определенной, потому что $\langle x, Ax\rangle$ аргумент логарифма и для любого $x$ аргумент логарифма должен быть положительным. Таким образом, $A \in \mathbb{S}^n_{++}$. Сначала найдем дифференциал: 
$$
\begin{split}
 df &= d \left( \ln \langle x, Ax\rangle \right) = \dfrac{d \left( \langle x, Ax\rangle \right)}{ \langle x, Ax\rangle} = \dfrac{\langle dx, Ax\rangle +  \langle x, d(Ax)\rangle}{ \langle x, Ax\rangle} = \\
 &= \dfrac{\langle Ax, dx\rangle + \langle x, Adx\rangle}{ \langle x, Ax\rangle} = \dfrac{\langle Ax, dx\rangle + \langle A^T x, dx\rangle}{ \langle x, Ax\rangle} = \dfrac{\langle (A + A^T) x, dx\rangle}{ \langle x, Ax\rangle} 
\end{split}
$$
2. Наша основная цель - получить форму $df = \langle \cdot, dx\rangle$. Имея ввиду $A + A^T = 2A$, получаем:
$$
df = \left\langle  \dfrac{2 A x}{ \langle x, Ax\rangle} , dx\right\rangle
$$
Таким образом, градиент равен $\nabla f(x) = \dfrac{2 A x}{ \langle x, Ax\rangle}$

## Матричное дифференцирование. Пример 3 {.t}

::: {.callout-example}
Найти $df, \nabla f(X)$, если $f(X) = \langle S, X\rangle - \log \det X$. 
:::

# Линейный поиск

## Задача 

Предположим, у нас есть задача минимизации функции $f(x): \mathbb{R} \to \mathbb{R}$ одной переменной:
$$
f(x) \to \min_{x \in \mathbb{R}}
$$

. . .

Иногда мы рассматриваем похожую задачу поиска минимума функции на отрезке $[a,b]$:
$$
f(x) \to \min_{x \in [a,b]}
$$

. . .


:::{.callout-example}
Типичным примером задачи линейного поиска является выбор подходящего шага для алгоритма градиентного спуска:
$$
\begin{split}
x_{k+1} = x_k - \alpha \nabla f(x_k) \\
\alpha = \text{argmin } f(x_{k+1}) 
\end{split}
$$

:::

. . .


Линейный поиск - фундаментальный инструмент оптимизации, который используют для решения других задач оптимизации. Для упрощения предположим, что $f(x)$ *унимодальна*, то есть, неформально говоря, имеет единственную впадину.

## Унимодальная функция

:::{.callout-definition}
Функция $f(x)$ называется **унимодальной** на отрезке $[a, b]$, если существует такой $x^* \in [a, b]$, что $f(x_1) > f(x_2) \;\;\; \forall a \le x_1 < x_2 < x^*$ и $f(x_1) < f(x_2) \;\;\; \forall x^* < x_1 < x_2 \leq b$
:::

. . .


![Примеры унимодальных функций](unimodal.pdf)

## Ключевое свойство унимодальных функций

Пусть $f(x)$ является унимодальной функцией на отрезке $[a, b]$. Тогда если $x_1 < x_2 \in [a, b]$, то:

* Если $f(x_1) \leq f(x_2)$, то $x^* \in [a, x_2]$
* Если $f(x_1) \geq f(x_2)$, то $x^* \in [x_1, b]$

. . .

**Доказательство** Докажем первое утверждение. Предположим, что $f(x_1) \leq f(x_2)$, но $x^* > x_2$. Тогда, поскольку $x_1 < x_2 < x^*$, из определения унимодальности функции $f(x)$ следует, что должно выполняться неравенство $f(x_1) > f(x_2)$. Мы получили противоречие.

. . .

:::: {.columns}
::: {.column width="33%"}
![]("Unimodal lemm1.pdf")
:::
 
. . .

::: {.column width="33%"}
![]("Unimodal lemm2.pdf")
:::

. . .

::: {.column width="33%"}
![]("Unimodal lemm3.pdf")
:::

::::

## Метод дихотомии


:::: {.columns}

::: {.column width="50%"}
Мы хотим решить следующую задачу:

$$
f(x) \to \min_{x \in [a,b]}
$$

Делим отрезок на две равные части и выбираем ту, которая содержит решение задачи, основываясь на ключевом свойстве, описанном выше. 

Наша цель после одной итерации метода -  локализовать решение в отрезке в два раза меньшей длины.
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy1.pdf){width=88%}
:::

::::


## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Вычисляем значение функции в середине отрезка
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy2.pdf){width=88%}
:::

::::


## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Чтобы применить ключевое свойство, мы выполняем еще одно вычисление значения функции.
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy3.pdf){width=88%}
:::

::::

## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Выбираем целевой отрезок. В случае на изображении нас все устраивает, потому что новый отрезок локализации решения является половиной исходного. 

Так происходит не всегда.
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy4.pdf){width=88%}
:::

::::

## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Рассмотрим другую унимодальную функцию. 
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy5.pdf){width=88%}
:::

::::

## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Вычисляем значение функции в середине отрезка.

:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy6.pdf){width=88%}
:::

::::

## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Делаем еще одно вычисление значения функции.
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy7.pdf){width=88%}
:::

::::

## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
Выбираем целевой отрезок. Легко видеть, что полученный отрезок не является половиной исходного. Его длина равна $\frac{3}{4} (b-a)$. Чтобы исправить это, нам нужен еще один шаг алгоритма.
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy8.pdf){width=88%}
:::

::::

## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
После дополнительного вычисления значения функции мы точно получим $\frac{2}{3} \frac{3}{4}(b-a) = \frac{1}{2}(b-a)$
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy9.pdf){width=88%}
:::

::::

## Метод дихотомии{.noframenumbering}

:::: {.columns}

::: {.column width="50%"}
В итоге, каждая последующая итерация требует не более двух вычислений значения функции.
:::

::: {.column width="50%"}
![Метод дихотомии для унимодальной функции](Dichotomy10.pdf){width=88%}
:::

::::


## Метод дихотомии. Алгоритм

```python
def bisection_search(f, a, b, epsilon):
   c = (a + b) / 2.0
   fc = f(c)
   while (b - a) > epsilon:
      y = (a + c) / 2.0
      fy = f(y)
      if fy <= fc:       # минимум точно в [a, c]
         b = c
         c = y
         fc = fy
         continue
      z = (b + c) / 2.0
      fz = f(z)
      if fc <= fz:       # минимум в [y, z]
         a = y
         b = z
      else:              # минимум в [c, b]
         a = c
         c = z
         fc = fz
    return c
```

## Метод дихотомии. Оценка
Длина отрезка на $k$-й итерации:

$$
\Delta_{k} = b_{k} - a_{k} = \dfrac{1}{2^k}(b-a)
$$


. . .

Для унимодальных функций это верно, если мы выбираем середину отрезка в качестве выхода итерации $x_{k}$: 

$$
|x_{k} - x_*| \leq \dfrac{\Delta_{k}}{2} \leq \dfrac{1}{2^{k+1}}(b-a) \leq (0.5)^{k} \cdot \frac{b-a}{2}
$$


. . .

Заметим, что на каждой итерации мы обращаемся к оракулу (вычисляем значение функции) не более двух раз, поэтому количество вызовов функции равно $N = 2 \cdot k$, что означает:

$$
|x_{k} - x_*| \leq (0.5)^{\frac{N}{2}} \cdot \frac{b-a}{2} \leq  (0.707)^{N}  \frac{b-a}{2}
$$

. . .

Обозначив правую часть последнего неравенства за $\varepsilon$, мы получаем количество итераций метода, необходимое для достижения точности $\varepsilon$:

$$
K = \left\lceil \log_2 \dfrac{b-a}{\varepsilon} - 1 \right\rceil
$$

## Метод золотого сечения

Идея очень похожа на метод дихотомии. На отрезке выбираются две точки - левая и правая точки золотого сечения. Ключевая идея метода заключается в том, что на следующей итерации одна из точек останется точкой золотого сечения.

![Идея, позволяющая уменьшить количество вызовов функции](golden_search.pdf)

## Метод золотого сечения. Алгоритм

```python
def golden_search(f, a, b, epsilon):
   tau = (sqrt(5) + 1) / 2
   y = a + (b - a) / tau**2
   z = a + (b - a) / tau
   fy = f(y)
   fz = f(z)
   while b - a > epsilon:
      if fy <= fz:
         b = z
         z, fz = y, fy
         y = a + (b - a) / (tau * tau)
         fy = f(y)          # 1 новый вызов
      else:
         a = y
         y, fy = z, fz
         z = a + (b - a) / tau
         fz = f(z)          # 1 новый вызов
   return (a + b) / 2.0
```

## Метод золотого сечения. Оценка

$$
|x_{k} - x_*| \leq \frac{b_{k} - a_{k}}{2} = \left( \frac{1}{\tau} \right)^{N} \frac{b - a}{2} \approx 0.618^k\frac{b - a}{2}
$$

где $\tau = \frac{\sqrt{5} + 1}{2}$.

* Знаменатель геометрической прогрессии для метода золотого сечения **больше**, чем для метода дихотомии: $0.618 > 0.5$.
* Количество вызовов функции **меньше** для метода золотого сечения, чем для метода дихотомии: $0.707$ больше (значит медленнее), чем $0.618$. Для каждой итерации метода дихотомии (кроме первой), функция вызывается не более двух раз, в то время как для метода золотого сечения, она вызывается не более одного раза за итерацию.

## Метод параболической интерполяции

Три точки, не лежащие на одной прямой, однозначно определяют параболу, проходящую через них. Идея метода — аппроксимировать функцию такой параболой и в качестве следующего приближения взять точку её минимума. Предположим, у нас есть три точки $x_1 < x_2 < x_3$, такие что отрезок $[x_1, x_3]$ содержит минимум функции $f(x)$. Тогда мы должны решить следующую систему уравнений:

. . .


$$
ax_i^2 + bx_i + c = f_i = f(x_i), i = 1,2,3 
$$

Заметим, что эта система линейна, мы должны решить ее относительно $a,b,c$. Минимум этой параболы вычисляется по формуле:  

. . .


$$
u = -\dfrac{b}{2a} = x_2 - \dfrac{(x_2 - x_1)^2(f_2 - f_3) - (x_2 - x_3)^2(f_2 - f_1)}{2\left[ (x_2 - x_1)(f_2 - f_3) - (x_2 - x_3)(f_2 - f_1)\right]}
$$

Заметим, что если $f_2 < f_1, f_2 < f_3$, то $u$ будет лежать в $[x_1, x_3]$

## Метод параболической интерполяции. Алгоритм [^2]

\scriptsize
```python
def parabola_search(f, x1, x2, x3, epsilon):
   f1, f2, f3 = f(x1), f(x2), f(x3)
   while x3 - x1 > epsilon:
      u = x2 - ((x2 - x1)**2*(f2 - f3) - (x2 - x3)**2*(f2 - f1))/(2*((x2 - x1)*(f2 - f3) - (x2 - x3)*(f2 - f1)))
      fu = f(u)

      if x2 <= u:
         if f2 <= fu:
            x1, x2, x3 = x1, x2, u
            f1, f2, f3 = f1, f2, fu
         else:
            x1, x2, x3 = x2, u, x3
            f1, f2, f3 = f2, fu, f3
      else:
         if fu <= f2:
            x1, x2, x3 = x1, u, x2
            f1, f2, f3 = f1, fu, f2
         else:
            x1, x2, x3 = u, x2, x3
            f1, f2, f3 = fu, f2, f3
   return (x1 + x3)/2
```


[^2]: Сходимость метода локально сверхлинейная, что означает, что мы можем получить выгоду от использования этого метода только в некоторой окрестности оптимума. [*Здесь*](https://people.math.sc.edu/kellerlv/Quadratic_Interpolation.pdf) доказательство сверхлинейной сходимости порядка $1.32$.

---

[![](inaccurate_taylor.jpeg)](https://fmin.xyz/docs/theory/inaccurate_taylor.mp4)


## Неточный линейный поиск

:::: {.columns}

::: {.column width="50%"}
Нам не всегда нужно точно решать задачу минимизации. Иногда достаточно найти приближенное решение. Это часто встречается при выборе шага в методах оптимизации.
$$
\begin{split}
x_{k+1} = x_k - \alpha \nabla f(x_k) \\
\alpha = \text{arg}\min\limits_{\alpha \geq 0} f(x_{k+1}) 
\end{split}
$$

. . .


Рассмотрим скалярную функцию $\phi(\alpha)$ в точке $x_k$: 
$$
\phi(\alpha) = f(x_k - \alpha\nabla f(x_k)), \alpha \geq 0
$$

. . .


Первое приближение $\phi(\alpha)$ в окрестности $\alpha = 0$ равно:
$$
\phi(\alpha) \approx \phi_0^I(\alpha) = f(x_k) - \alpha\nabla f(x_k)^T \nabla f(x_k)
$$
:::

::: {.column width="50%"}
![Иллюстрация аппроксимации Тейлора $\phi^I_0(\alpha)$](inexact.pdf){width=88%}
:::

::::

## Неточный линейный поиск. Условие достаточного убывания

:::: {.columns}

::: {.column width="50%"}
Условие неточного линейного поиска, известное как условие Армихо, требует, чтобы $\alpha$ обеспечивало достаточное убывание функции $f$:
$$
f(x_k - \alpha \nabla f (x_k)) \leq f(x_k) - c_1 \cdot \alpha\nabla f(x_k)^T \nabla f(x_k)
$$

. . .


для некоторой постоянной $c_1 \in (0,1)$. Заметим, что установка $c_1 = 1$ соответствует первому приближению Тейлора $\phi(\alpha)$. 

Однако этому условию могут соответствовать очень малые значения $\alpha$, потенциально замедляющие процесс решения. Обычно на практике используется $c_1 \approx 10^{-4}$.

. . .


:::{.callout-example}
На практике выбор подходящего значения $c_1$ может быть очень важным. Например, в задачах машинного обучения неправильное значение $c_1$ может привести к очень медленной сходимости или пропуску минимума.
:::

:::

::: {.column width="50%"}

![Иллюстрация условия достаточного убывания с коэффициентом $c_1$](sufficient decrease.pdf){width=80%}
:::

::::

## Неточный линейный поиск. Условия Гольдштейна


:::: {.columns}

::: {.column width="50%"}
Рассмотрим две линейные скалярные функции $\phi_1(\alpha)$ и $\phi_2(\alpha)$:
$$
\phi_1(\alpha) = f(x_k) - c_1 \alpha \|\nabla f(x_k)\|^2
$$

$$
\phi_2(\alpha) = f(x_k) - c_2 \alpha \|\nabla f(x_k)\|^2
$$

. . .


Условия Гольдштейна-Армихо требуют, чтобы функция $\phi(\alpha)$ лежала между $\phi_1(\alpha)$ и $\phi_2(\alpha)$. Обычно $c_1 = \rho$ и $c_2 = 1 - \rho$, с $\rho \in (0, 0.5)$.

:::

::: {.column width="50%"}
![Иллюстрация условий Гольдштейна](Goldstein.pdf){width=88%}
:::

::::

## Неточный линейный поиск. Условие ограничения на кривизну


:::: {.columns}

::: {.column width="50%"}
Чтобы избежать слишком коротких шагов, вводится дополнительное ограничение:
$$
-\nabla f (x_k - \alpha \nabla f(x_k))^T \nabla f(x_k) \geq c_2 \nabla f(x_k)^T(- \nabla f(x_k))
$$

. . .

для некоторого $c_2 \in (c_1,1)$. Здесь $c_1$ из условия Армихо. 

Левая часть является производной $\nabla_\alpha \phi(\alpha)$, гарантирующей, что наклон $\phi(\alpha)$ в целевой точке не менее чем в $c_2$ раз больше начального наклона $\nabla_\alpha \phi(\alpha)(0)$. 

Обычно для методов Ньютона и квазиньютоновских методов используется $c_2 \approx 0.9$ . Вместе условие достаточного убывания и ограничение на кривизну образуют условия Вульфа.

:::

::: {.column width="50%"}
![Иллюстрация условия ограничения на кривизну](Curvature.pdf){width=88%}
:::

::::

## Неточный линейный поиск. Условия Вульфа

:::: {.columns}

::: {.column width="50%"}

$$
-\nabla f (x_k - \alpha \nabla f(x_k))^T \nabla f(x_k) \geq c_2 \nabla f(x_k)^T(- \nabla f(x_k))
$$

Вместе, условие достаточного убывания и ограничение на кривизну образуют условия Вульфа.

:::{.callout-theorem}
Пусть $f : \mathbb{R}^n \to \mathbb{R}$ непрерывно дифференцируема, и пусть $\phi(\alpha) = f(x_k - \alpha \nabla f(x_k))$. Предположим, что $\nabla f(x_k)^T p_k < 0$, где $p_k = -\nabla f(x_k)$ - направление спуска. Также предположим, что $f$ ограничена снизу вдоль луча $\{x_k + \alpha p_k \mid \alpha > 0\}$. Мы хотим показать, что для $0 < c_1 < c_2 < 1$, существуют интервалы шагов, удовлетворяющие условиям Вульфа.
:::

:::

::: {.column width="50%"}

![Иллюстрация условий Вульфа](Wolfe.pdf){width=88%}
:::

::::

## Неточный линейный поиск. Условия Вульфа. Доказательство

:::: {.columns}

::: {.column width="50%"}

1. Поскольку $\phi(\alpha) = f(x_k + \alpha p_k)$ ограничена снизу и $l(\alpha) = f(x_k) + \alpha c_1 \nabla f(x_k)^T p_k$ неограничена снизу (как $\nabla f(x_k)^T p_k < 0$), график $l(\alpha)$ должен пересекать график $\phi(\alpha)$ по крайней мере один раз. Пусть $\alpha' > 0$ будет наименьшим таким значением, удовлетворяющим:
$$
f(x_k + \alpha' p_k) \leq f(x_k) + \alpha' c_1 \nabla f(x_k)^T p_k. \tag{1}
$$
Это гарантирует выполнение **условия достаточного убывания**.

1. По теореме о среднем значении, существует $\alpha'' \in (0, \alpha')$ такое, что:
$$
f(x_k + \alpha' p_k) - f(x_k) = \alpha' \nabla f(x_k + \alpha'' p_k)^T p_k. \tag{2}
$$
Подставляя $f(x_k + \alpha' p_k)$ из (1) в (2), мы получаем:
$$
\alpha' \nabla f(x_k + \alpha'' p_k)^T p_k \leq \alpha' c_1 \nabla f(x_k)^T p_k.
$$

:::

. . .

::: {.column width="50%"}
Делим на $\alpha' > 0$, получаем:
$$
\nabla f(x_k + \alpha'' p_k)^T p_k \leq c_1 \nabla f(x_k)^T p_k. \tag{3}
$$

3. Поскольку $c_1 < c_2$ и $\nabla f(x_k)^T p_k < 0$, неравенство $c_1 \nabla f(x_k)^T p_k < c_2 \nabla f(x_k)^T p_k$ выполняется. Это означает, что существует $\alpha''$ такое, что:
$$
\nabla f(x_k + \alpha'' p_k)^T p_k \leq c_2 \nabla f(x_k)^T p_k. \tag{4}
$$
Неравенства (3) и (4) вместе гарантируют выполнение условий Вульфа.

1. Для сильных условий Вульфа, условие ограничения на кривизну:
$$
\left| \nabla f(x_k + \alpha p_k)^T p_k \right| \leq c_2 \left| \nabla f(x_k)^T p_k \right| \tag{5}
$$
выполняется, потому что $\nabla f(x_k + \alpha p_k)^T p_k$ отрицательно и ограничено снизу $c_2 \nabla f(x_k)^T p_k$.

1. Из-за гладкости $f$, существует интервал вокруг $\alpha''$ , где выполняются условия Вульфа (и, следовательно, сильные условия Вульфа). Таким образом, доказательство завершено.
:::

::::



## Бэктрекинг

Бэктрекинг - это техника для нахождения шага, удовлетворяющего условию Армихо, условиям Гольдштейна или другим критериям неточного линейного поиска. Она начинает с относительно большого шага и итеративно уменьшает его до тех пор, пока не будет выполнено условие.

. . .


### Алгоритм:

1. Выберите начальный шаг, $\alpha_0$, и параметры $\beta \in (0, 1)$ и $c_1 \in (0, 1)$.
2. Проверьте, удовлетворяет ли выбранный шаг выбранному условию (например, условию Армихо).
3. Если условие выполнено, остановитесь; в противном случае, установите $\alpha := \beta \alpha$ и повторите шаг 2.

. . .


Шаг $\alpha$ обновляется как 

$$
\alpha_{k+1} := \beta \alpha_k
$$

в каждой итерации до тех пор, пока выбранное условие не будет выполнено.

:::{.callout-example}
В задачах машинного обучения линейный поиск с бэктрекингом может использоваться для регулировки скорости обучения. Если функция потерь не уменьшается достаточно, скорость обучения уменьшается мультипликативно до тех пор, пока не будет выполнено, например, условие Армихо.
:::

## Численная иллюстрация

![Сравнение различных алгоритмов линейного поиска](line_search_comp.pdf)

[Открыть в Colab $\clubsuit$](https://colab.research.google.com/github/MerkulovDaniil/optim/blob/master/assets/Notebooks/Line_search.ipynb)

## Градиентный спуск с линейным поиском

[![](gd_ls.pdf)](https://fmin.xyz/docs/visualizations/ls_gd.mp4)



# Итоги

## Итоги

:::: {.columns .nonincremental}

::: {.column width="50%"}

### Определения

1. Унимодальная функция.
1. Метод дихотомии.
1. Метод золотого сечения.
1. Метод параболической интерполяции.
1. Условие достаточного убывания для неточного линейного поиска.
1. Условия Гольдштейна для неточного линейного поиска.
1. Условие ограничения на кривизну для неточного линейного поиска.
1. Градиент функции $f(x): \mathbb{R}^n \to \mathbb{R}$.
1. Гессиан функции $f(x): \mathbb{R}^n \to \mathbb{R}$.
1. Якобиан функции $f(x): \mathbb{R}^n \to \mathbb{R}^m$.
1. Формула для аппроксимации Тейлора первого порядка $f^I_{x_0}(x)$ функции $f(x): \mathbb{R}^n \to \mathbb{R}$ в точке $x_0$.
1. Формула для аппроксимации Тейлора второго порядка $f^{II}_{x_0}(x)$ функции $f(x): \mathbb{R}^n \to \mathbb{R}$ в точке $x_0$.

:::

::: {.column width="50%"}

13. Связь дифференциала функции $df$ и градиента $\nabla f$ для функции $f(x): \mathbb{R}^n \to \mathbb{R}$.
1. Связь второго дифференциала функции $d^2f$ и гессиана $\nabla^2 f$ для функции $f(x): \mathbb{R}^n \to \mathbb{R}$.

### Теоремы

1. Метод дихотомии и золотого сечения для унимодальных функций. Скорость сходимости.

:::

::::