---
title: Автоматическое дифференцирование
author: Семинар
institute: ФКН ВШЭ
format:
    beamer:
        pdf-engine: xelatex
        aspectratio: 169
        fontsize: 9pt
        section-titles: false
        incremental: true
        include-in-header: ../files/xeheader.tex  # Custom LaTeX commands and preamble
---

# Напоминание с лекции

# Автоматическое дифференцирование

## Прямой режим
![Иллюстрация прямого правила дифференцирования сложной функции для вычисления производной $v_i$ по $w_k$.](auto_diff_forward.pdf){width=300}

* Использует прямое правило дифференцирования сложной функции
* Имеет сложность $d \times \mathcal{O}(T)$ операций

## Обратный режим
![Иллюстрация обратного правила дифференцирования сложной функции для вычисления производной $L$ по узлу $v_i$.](auto_diff_reverse.pdf){width=300}


* Использует обратное правило дифференцирования сложной функции
* Сохраняет информацию из прямого прохода
* Имеет сложность $\mathcal{O}(T)$ операций

# Задачи на автоматическое дифференцирование

## Простой пример

::: {.callout-example}
$$
f(x_1, x_2) = x_1 \cdot x_2 + \sin x_1
$$

Вычислим производные $\dfrac{\partial f}{\partial x_i}$ с помощью прямого и обратного режимов.
:::

. . .

![Иллюстрация вычислительного графа $f(x_1, x_2)$.](autograd_example.pdf){width=300}


## Автоматическое дифференцирование с JAX



:::: {.columns}

::: {.column width="50%"}
::: {.callout-tip icon="false" title="Пример №1"}
$$f(X) = \text{tr}(AX^{-1}B)$$

$$\nabla f = - X^{-T} A^T B^T X^{-T}$$

:::

. . .

:::

::: {.column width="50%"}
::: {.callout-tip icon="false" title="Пример №2"}
$$g(x) = \frac{1}{3} \|x\|_2^3$$

$$\nabla^2 g = \|x\|_2^{-1} x x^T + \|x\|_2 I_n$$
:::
:::

::::

. . .

\
\

Вычислим градиенты и гессианы функций $f$ и $g$ [\faPython](https://colab.research.google.com/drive/14FXSFirBR7OI76p1z72n353Ve9LmwL90#scrollTo=61Ryf-1eWeZP&line=1&uniqifier=1)


## Задача 1

::: {.callout-question}
Какой из режимов автоматического дифференцирования вы бы выбрали (прямой/обратный) для следующего вычислительного графа арифметических операций?
:::

![Какой режим вы бы выбрали для вычисления градиентов?](ad_choose.pdf){width=175}

## Задача 2

:::: {.columns}

::: {.column width="50%"}

Предположим, что у нас есть обратимая матрица $A$ и вектор $b$, вектор $x$ является решением линейной системы $Ax = b$, то есть можно записать аналитическое решение $x = A^{-1}b$.


\
\

::: {.callout-question}

Найдите производные $\dfrac{\partial L}{\partial A}, \dfrac{\partial L}{\partial b}$.

:::

:::

::: {.column width="50%"}

![$x$ может быть найден как решение линейной системы](linear_least_squares_layer.pdf){width=200}


:::
::::

## Распространение градиента через линейные наименьшие квадраты

:::: {.columns}

::: {.column width="40%"}

![$x$ может быть найден как решение линейной системы](linear_least_squares_layer.pdf)

:::

::: {.column width="60%"}

Предположим, что у нас есть обратимая матрица $A$ и вектор $b$, вектор $x$ является решением линейной системы $Ax = b$, то есть можно записать аналитическое решение $x = A^{-1}b$. В этом примере мы покажем, что вычисление всех производных $\dfrac{\partial L}{\partial A}, \dfrac{\partial L}{\partial b}, \dfrac{\partial L}{\partial x}$, то есть обратный проход, стоит примерно столько же, сколько и прямой проход.

. . .

Известно, что дифференциал функции не зависит от параметризации:

$$
dL = \left\langle\dfrac{\partial L}{\partial x}, dx \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

Учитывая линейную систему, имеем:

$$
\begin{split}
Ax &= b \\
dAx + Adx = db &\to dx = A^{-1}(db - dAx)
\end{split}
$$

:::

::::

## Распространение градиента через линейные наименьшие квадраты

:::: {.columns}

::: {.column width="40%"}

![$x$ может быть найден как решение линейной системы](linear_least_squares_layer.pdf)

:::

::: {.column width="60%"}

Прямая подстановка даёт нам:

$$
\left\langle\dfrac{\partial L}{\partial x}, A^{-1}(db - dAx) \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

$$
\left\langle -A^{-T}\dfrac{\partial L}{\partial x} x^T, dA \right\rangle + \left\langle A^{-T}\dfrac{\partial L}{\partial x},db \right\rangle = \left\langle\dfrac{\partial L}{\partial A}, dA \right\rangle + \left\langle\dfrac{\partial L}{\partial b}, db \right\rangle
$$

. . .

Следовательно:

$$
\dfrac{\partial L}{\partial A} = -A^{-T}\dfrac{\partial L}{\partial x} x^T \quad \dfrac{\partial L}{\partial b} =  A^{-T}\dfrac{\partial L}{\partial x}
$$

. . .

Интересно, что наиболее вычислительно затратная часть здесь — это обращение матрицы, что совпадает со сложностью прямого прохода. Иногда даже можно сохранить сам результат, что делает обратный проход ещё дешевле.

:::

::::

## Задача 3

:::: {.columns}

::: {.column width="50%"}

Предположим, что у нас есть прямоугольная матрица $W \in \mathbb{R}^{m \times n}$, которая имеет сингулярное разложение:

\
\

$$
W = U \Sigma V^T, \quad U^TU = I, \quad V^TV = I,
$$
$$
\Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
$$

\
\
Регуляризатор $R(W) = \text{tr}(\Sigma)$ в любой функции потерь поощряет решения с низким рангом.

::: {.callout-question}

Найдите производную $\dfrac{\partial R}{\partial W}$.

:::
:::

::: {.column width="50%"}

![Вычислительный граф для сингулярного регуляризатора](svd_singular_regularizer_comp_graph.pdf){width=200}

:::
::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

Предположим, что у нас есть прямоугольная матрица $W \in \mathbb{R}^{m \times n}$, которая имеет сингулярное разложение:

$$
W = U \Sigma V^T, \quad U^TU = I, \quad V^TV = I, \quad \Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(m,n)})
$$

1. Аналогично предыдущему примеру:

    $$
    \begin{split}
    W &= U \Sigma V^T \\
    dW &= dU \Sigma V^T + U d\Sigma V^T + U \Sigma dV^T \\
    U^T dW V &= U^TdU \Sigma V^TV + U^TU d\Sigma V^TV + U^TU \Sigma dV^TV \\
    U^T dW V &= U^TdU \Sigma + d\Sigma + \Sigma dV^TV
    \end{split}
    $$

:::

::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

2. Заметим, что $U^T U = I \to dU^TU + U^T dU = 0$. Но также $dU^TU = (U^T dU)^T$, что означает, что матрица $U^TdU$ антисимметрична:

    $$
    (U^T dU)^T +  U^T dU = 0 \quad \to \quad \text{diag}( U^T dU) = (0, \ldots, 0)
    $$

    Та же логика применима к матрице $V$:

    $$
    \text{diag}(dV^T V) = (0, \ldots, 0)
    $$

3. При этом матрица $d \Sigma$ диагональна, что означает (смотри пункт 1), что

    $$
    \text{diag}(U^T dW V) = d \Sigma
    $$

    Здесь с обеих сторон у нас диагональные матрицы.

:::

::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

4. Теперь мы можем разложить дифференциал функции потерь как функцию от $\Sigma$ — такие задачи возникают в машинном обучении, когда нужно ограничить ранг матрицы:

    $$
    \begin{split}
    dL &= \left\langle\dfrac{\partial L}{\partial \Sigma}, d\Sigma \right\rangle \\
    &= \left\langle\dfrac{\partial L}{\partial \Sigma}, \text{diag}(U^T dW V)\right\rangle \\
    &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V) \right)
    \end{split}
    $$

:::

::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

5. Поскольку у нас диагональные матрицы внутри произведения, след диагональной части матрицы будет равен следу всей матрицы:

    $$
    \begin{split}
    dL &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T \text{diag}(U^T dW V) \right) \\
    &= \text{tr}\left(\dfrac{\partial L}{\partial \Sigma}^T U^T dW V \right)  \\
    &= \left\langle\dfrac{\partial L}{\partial \Sigma}, U^T dW V \right\rangle \\
    &= \left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle
    \end{split}
    $$

:::

::::

## Распространение градиента через SVD

:::: {.columns}

::: {.column width="30%"}

![](svd_singular_regularizer_comp_graph.pdf)

:::

::: {.column width="70%"}

6. Наконец, используя другую параметризацию дифференциала

    $$
    \left\langle U \dfrac{\partial L}{\partial \Sigma} V^T, dW \right\rangle = \left\langle\dfrac{\partial L}{\partial W}, dW \right\rangle
    $$

    $$
    \dfrac{\partial L}{\partial W} =  U \dfrac{\partial L}{\partial \Sigma} V^T,
    $$

    Этот красивый результат позволяет связать градиенты $\dfrac{\partial L}{\partial W}$ и $\dfrac{\partial L}{\partial \Sigma}$.
:::

::::

## Вычислительный эксперимент с JAX

Убедимся численно, что мы правильно вычислили производные в задачах 2 и 3 [\faPython](https://colab.research.google.com/drive/14FXSFirBR7OI76p1z72n353Ve9LmwL90#scrollTo=LlqwKMaPR0Sf)


# Чекпоинтинг градиента

## Архитектура прямого распространения

![Вычислительный граф для получения градиентов простой нейронной сети прямого распространения с $n$ слоями. Активации обозначены через $f$. Градиент функции потерь по активациям и параметрам обозначен через $b$.](backprop.pdf){width=350}

. . .

::: {.callout-important}

Результаты, полученные для узлов $f$, необходимы для вычисления узлов $b$.

:::

## Стандартное обратное распространение

![Вычислительный граф для получения градиентов простой нейронной сети прямого распространения с $n$ слоями. Фиолетовым цветом обозначены узлы, хранящиеся в памяти.](vanilla_backprop.pdf){width=350}

. . .

* Все активации $f$ сохраняются в памяти после прямого прохода.

. . .


::: {.callout-tip icon="false" appearance="simple"}

* Оптимален по вычислениям: каждый узел вычисляется только один раз.

:::

. . .

::: {.callout-important icon="false" appearance="simple"}

* Высокое потребление памяти. Использование памяти растёт линейно с количеством слоёв в нейронной сети.

:::


## Экономное по памяти обратное распространение

![Вычислительный граф для получения градиентов простой нейронной сети прямого распространения с $n$ слоями. Фиолетовым цветом обозначены узлы, хранящиеся в памяти.](poor_mem_backprop.pdf){width=350}

. . .

* Каждая активация $f$ пересчитывается по мере необходимости.

. . .


::: {.callout-tip icon="false" appearance="simple"}

* Оптимален по памяти: нет необходимости хранить все активации в памяти.

:::

. . .

::: {.callout-important icon="false" appearance="simple"}

* Вычислительно неэффективен. Количество вычислений узлов растёт как $n^2$, тогда как в стандартном подходе — как $n$: каждый из $n$ узлов пересчитывается порядка $n$ раз.

:::

## Обратное распространение с чекпоинтами

![Вычислительный граф для получения градиентов простой нейронной сети прямого распространения с $n$ слоями. Фиолетовым цветом обозначены узлы, хранящиеся в памяти.](checkpoint_backprop.pdf){width=350}

. . .

* Компромисс между **стандартным** и **экономным по памяти** подходами. Стратегия состоит в том, чтобы пометить подмножество активаций нейронной сети как чекпоинты, которые будут храниться в памяти.

. . .


::: {.callout-tip icon="false" appearance="simple"}

* Более быстрый пересчёт активаций $f$. При вычислении узла $b$ во время обратного прохода нужно пересчитать только узлы между этим $b$ и последним предшествующим ему чекпоинтом.

:::

. . .

::: {.callout-tip icon="false" appearance="simple"}

* Потребление памяти зависит от количества чекпоинтов. Эффективнее, чем **стандартный** подход.

:::

## Визуализация чекпоинтинга градиента


Анимированная визуализация вышеописанных подходов [\faGithub](https://github.com/cybertronai/gradient-checkpointing)


Пример использования чекпоинтинга градиента [\faGithub](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/gradient-checkpointing-nin.ipynb)


## Оценка следа методом Хатчинсона ^[[A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines - M.F. Hutchinson, 1990](https://www.tandfonline.com/doi/abs/10.1080/03610919008812866)]

Этот пример иллюстрирует оценку следа гессиана нейронной сети с использованием метода Хатчинсона — алгоритма, который позволяет получить такую оценку из произведений матрицы на вектор:

Пусть $X \in \mathbb{R}^{d \times d}$ и $v \in \mathbb{R}^d$ — случайный вектор такой, что $\mathbb{E}[vv^T] = I$. Тогда,

:::: {.columns}
::: {.column width="40%"}

$$
\text{tr}(X) = \mathbb{E}[v^TXv] = \frac{1}{V}\sum_{i=1}^{V}v_i^TXv_i.
$$


Пример использования оценки следа методом Хатчинсона [\faPython](https://colab.research.google.com/drive/1aLx_-Sv2tTTKz0NCEFcedqQyopBUczJH#scrollTo=DZTgqcHoa8O3)

:::
::: {.column width="60%"}
![Несколько запусков оценки следа методом Хатчинсона с разными начальными значениями генератора случайных чисел.](Hutchinson_trace_est.pdf)
:::
::::
