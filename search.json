[
  {
    "objectID": "notebooks/s1_lora_trump.html",
    "href": "notebooks/s1_lora_trump.html",
    "title": "",
    "section": "",
    "text": "!pip install -q transformers datasets peft accelerate bitsandbytes"
  },
  {
    "objectID": "notebooks/s1_lora_trump.html#train",
    "href": "notebooks/s1_lora_trump.html#train",
    "title": "",
    "section": "Train",
    "text": "Train\n\nimport torch\nimport re\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, PeftModelForCausalLM\nfrom transformers import BitsAndBytesConfig\nfrom copy import deepcopy\n\ntorch.random.manual_seed(0)\n\n&lt;torch._C.Generator at 0x7fe5ae6cc250&gt;\n\n\n\n# Load the Trump Tweets dataset\ndataset = load_dataset(\"yunfan-y/trump-tweets-cleaned\")\n\n# Load the tokenizer and the base model\nmodel_name = \"unsloth/Llama-3.2-3B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n)\n\n\n# Define a function for generating text\ndef generate_example(prompt, model, tokenizer, max_length=50):\n    model.eval()  # Убедитесь, что модель в режиме генерации\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        num_return_sequences=1,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.95\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\ndef clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n    # Remove retweets\n    tweet = re.sub(r'^RT\\s+', '', tweet)\n    # Remove user @ references and '#' from hashtags\n    tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\n    # Remove special characters and numbers\n    # tweet = re.sub(r'[^A-Za-z\\d\\s]', '', tweet)\n    # Convert to lowercase\n    return tweet.strip()\n\n# Preprocessing the data\ndef preprocess_function(examples):\n    tweets = examples[\"text\"]  # Use \"text\" instead of \"content\"\n    inputs = [f\"You are Donald Trump writing a tweet about politics. Your tweet: {clean_tweet(tweet)}\" for tweet in tweets]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n    \ntokenizer.pad_token = tokenizer.eos_token\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n\n# Split the dataset into training and validation sets\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n\n# Generate text before training\nprint(\"=== Text Generation Before Training ===\")\nprompt = \"Immigrants\"\nprint(generate_example(prompt, model, tokenizer))\n\n# Prepare LoRA configuration\nlora_config = LoraConfig(\n    r=64,  # Rank of the LoRA update matrices\n    lora_alpha=32,  # LoRA scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA\n    lora_dropout=0.1,  # Dropout rate for LoRA\n    bias=\"none\",  # Do not train biases\n    task_type=\"CAUSAL_LM\"  # Task type for causal language modeling\n)\n\n# Wrap the model with PEFT\npeft_model = get_peft_model(model, lora_config)\noutput_dir = \"./trump_lora\"\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=output_dir,  # Directory for saving the model\n    eval_strategy=\"steps\",  # No evaluation dataset\n    logging_steps=10,\n    save_strategy=\"steps\",\n    save_steps=10,\n    learning_rate=3e-4,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=4,\n    num_train_epochs=1,\n    max_steps=100,\n    weight_decay=0.01,\n    gradient_accumulation_steps=16,\n    warmup_steps=100,\n    logging_dir=\"./logs\",\n    fp16=True,\n    report_to='none',\n)\n\n\n# Define the Trainer with eval_dataset\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model\npeft_model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Generate text after training\nprint(\"=== Text Generation After Training ===\")\nprint(generate_example(prompt, peft_model, tokenizer))\n\nprint(\"Model fine-tuned and saved!\")\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\n=== Text Generation Before Training ===\nImmigrants and the American Dream: The Political and Economic Legacy of Early New York\nImmigrants and the American Dream: The Political and Economic Legacy of Early New York by Anthony F. C. Wallace\nEnglish | February 3, 199\n\n\n\n      \n      \n      [100/100 14:01, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n10\n100.107800\n4.980176\n\n\n20\n32.612900\n0.334947\n\n\n30\n5.274200\n0.308010\n\n\n40\n4.525100\n0.257844\n\n\n50\n3.852100\n0.193377\n\n\n60\n2.864500\n0.175147\n\n\n70\n2.653900\n0.171719\n\n\n80\n2.623700\n0.169075\n\n\n90\n2.646900\n0.167437\n\n\n100\n2.612200\n0.165744\n\n\n\n\n\n\nRepo card metadata block was not found. Setting CardData to empty.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\n=== Text Generation After Training ===\nImmigrants’ rights advocates are worried that changes to immigration policies are making it harder for people to seek asylum in the United States. And those changes have also made it harder to prove that someone is eligible for asylum, according to the experts interviewed by\nModel fine-tuned and saved!"
  },
  {
    "objectID": "notebooks/s1_lora_trump.html#inference",
    "href": "notebooks/s1_lora_trump.html#inference",
    "title": "",
    "section": "Inference",
    "text": "Inference\nSeems like when we load fine-tuned model, it overwrites original model, so restart notebook just in case before inference\n\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = 'false'\ni = 6 # device number to use\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{i}'\n\n\nimport torch\nimport re\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, PeftModelForCausalLM\nfrom transformers import BitsAndBytesConfig\nfrom copy import deepcopy\n\ntorch.random.manual_seed(0)\n\n&lt;torch._C.Generator at 0x7f4c68630310&gt;\n\n\n\n# Load the Trump Tweets dataset\ndataset = load_dataset(\"yunfan-y/trump-tweets-cleaned\")\n\n# Load the tokenizer and the base model\nmodel_name = \"unsloth/Llama-3.2-3B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n)\n\n\n# Define a function for generating text\ndef generate_example(prompt, model, tokenizer, max_length=50):\n    model.eval()  # Убедитесь, что модель в режиме генерации\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        num_return_sequences=1,\n        do_sample=True,\n        temperature=0.8,\n        top_p=0.95\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\ndef clean_tweet(tweet):\n    # Remove URLs\n    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n    # Remove retweets\n    tweet = re.sub(r'^RT\\s+', '', tweet)\n    # Remove user @ references and '#' from hashtags\n    tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\n    # Remove special characters and numbers\n    # tweet = re.sub(r'[^A-Za-z\\d\\s]', '', tweet)\n    # Convert to lowercase\n    return tweet.strip()\n\n# Preprocessing the data\ndef preprocess_function(examples):\n    tweets = examples[\"text\"]  # Use \"text\" instead of \"content\"\n    inputs = [f\"You are Donald Trump writing a tweet about politics. Your tweet: {clean_tweet(tweet)}\" for tweet in tweets]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n    \ntokenizer.pad_token = tokenizer.eos_token\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n\n# Split the dataset into training and validation sets\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n\nOriginal model\n\nfrom torch.utils.data import DataLoader\nimport math\ntrain_test_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.01, seed=42)\ntrain_dataset = train_test_split[\"train\"]\neval_dataset = train_test_split[\"test\"]\n\n# Evaluate both models on the evaluation dataset\nprint(\"=== Comparing Model Performance on Evaluation Dataset ===\")\n\n# Evaluate original model\nmodel.eval()\neval_loss = 0\neval_steps = 0\neval_data_loader = DataLoader(eval_dataset, batch_size=4, collate_fn=lambda x: {k: torch.tensor([d[k] for d in x]).to(model.device) for k in x[0].keys()})\nwith torch.no_grad():\n    for batch in eval_data_loader:\n        outputs = model(**{k: v for k, v in batch.items() if k != 'labels'}, labels=batch['labels'])\n        eval_loss += outputs.loss.item()\n        eval_steps += 1\noriginal_perplexity = math.exp(eval_loss / eval_steps)\nprint(f\"Original Model Perplexity: {original_perplexity:.2f}\")\nprint(f\"Original Model Loss: {eval_loss:.2f}\")\n\n=== Comparing Model Performance on Evaluation Dataset ===\nOriginal Model Perplexity: 732.55\nOriginal Model Loss: 554.11\n\n\n\ntest_prompts = [\n    \"I think, Donald Trump\",\n    \"I think, Barack Obama\",\n    \"I think, Joe Biden\",\n    \"I think, the Democrats are\",\n    \"We need to\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(3):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump will be the next President of the United States. There is a chance that he won’t but if he doesn’t then I’ll be very disappointed. To me, it’s the only way that America can survive.\nAmerica\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, the newly elected 45th President of the United States has just pulled a fast one on us. I think, he is going to be a great President. I’m not going to hold my breath, however,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump is the most intelligent President the US ever had, and the current one, Barack Obama is the most stupid one. I know it is hard to believe, but it is a fact. Trump is the most intelligent, because\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama would be the first African American president, who will make history in the USA, but he is not the first Black president. That is actually George Washington Carver, the inventor and scientist who was the first African American president\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama will win the US election. He has a great chance, since the opposition, John McCain, has made a big mistake. He has promised to give the same tax reductions to people with a high income as to those with\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a really nice man. I mean, his wife is really nice too. I mean, I can't think of a better word to describe them. And I like his daughter Malia. She seems really sweet.\n\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden, the vice president, is the strongest candidate that we’ve had in 25 years.\nIf you want to do more than just talk about how terrible you think Donald Trump is, you have to offer a specific vision of\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is the first president in recent history who has the potential to be reelected to the presidency. The reason is he is a person with a sense of humor, he is a good orator, and he has a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden’s political career has been at the brink of collapse since he was elected Vice President. It seems to me that his political career is going to be over sooner than later. The political environment is changing. His party is losing\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are in trouble this time. They are the ones who have been saying that the Republicans can not be trusted. I think that they have the right to be scared. They are getting very, very, very, very,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are now afraid of losing their super-majority in the House and Senate. And they are now trying to get the votes by offering amnesty to illegal immigrants.\nThe Democrats are now trying to pass amnesty, but the Republicans are\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are going to try and impeach Trump for the things they said they’d do. But it didn’t work in 1998 and it won’t work now. They tried to impeach Bush for lying about Iraq,\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to start doing things differently. We must begin to take care of our planet if we want to pass it on to our children. The way we travel is contributing to global warming and pollution. We need to look at ways to make our\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to think hard about the impact of our choices in life.\nThe life of a 6 year old girl was saved by her mother’s choice to make a different kind of dinner.\nWe need to think hard about the impact of our choices\n---\nWe need to look at the whole picture and get our minds around it. We need to understand what we are up against. We need to be able to see the big picture and be able to make sense of it.\n---\n\n\n\n\ntest_prompts = [\n    \"The United States\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has been working with its partners in the United Nations Security Council to impose sanctions on the Syrian government for its alleged use of chemical weapons in April against civilians in the town of Khan Sheikhoun in Idlib governorate. The US,\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States government is a massive employer, and is always looking for qualified candidates to fill a wide variety of open employment positions in locations across the country. Below you’ll find a Qualification Summary for an active, open job listing from the Department\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States of America is a federal country made up of fifty states and one federal district. It is the world’s largest industrial, military and economic power, and is the leading global superpower.\nThe United States of America is the third largest\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Postal Service is a government-owned corporation that has been operating under the radar for a long time. It is the sole mail delivery company in the United States and is one of the biggest companies in the world, with more than 1\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is the only country in the world that does not have a national health insurance system. In 2019, 28.5 million people had no health insurance coverage, and in 2021, 37 million people remained uninsured\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is currently ranked No. 28 on the list of the world’s countries by life expectancy at birth. It is estimated that the US will have a population of 325 million in 2015. Currently, there are nearly \n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is home to an estimated 1.2 million refugees, about half of whom arrived after 9/11. For decades, America has welcomed refugees, granting permanent residence to those fleeing persecution and war. But, in recent years\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a nation with a long history of slavery and the legacy of slavery still affects many people today. One such legacy is the practice of slavery in the United States. This practice has been around for centuries and has had a significant impact\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has been home to a number of notable African Americans, including athletes, politicians, entertainers, and military figures. One of the most famous was Jackie Robinson, who broke the MLB color barrier in 1947. Here are \n---\nThe United States and the rest of the world are facing an unprecedented crisis of misinformation. As the COVID-19 pandemic and other crises have unfolded, millions of people have been bombarded with lies and fake news on social media and other platforms. Mis\n---\n\n\n\n\ntest_prompts = [\n    \"Hillary Clinton is a\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a master of the backroom deal. She's not afraid to reach across the aisle to forge a deal that can get something done. Even though it might not be her ideal, she's willing to compromise to get something done.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a hypocrite for calling for tougher laws to stop sexual harassment in the workplace — and she should know better.\nLast week, the former first lady and secretary of state blasted a federal judge who released the record of a settlement between President\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a former U.S. Secretary of State, a former U.S. Senator from New York, and the 67th U.S. Secretary of State. She served as the First Lady of the United States from 1993 to\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a former United States Senator from New York, and the First Lady of the United States. She is a former U.S. Secretary of State and a presidential candidate in the 2008 Democratic primaries. She is running for the Democratic\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a racist. The former secretary of state and presidential hopeful has a long record of promoting racially charged stereotypes that were designed to undermine the civil rights movement.\nBut a recent poll shows that she’s also a racist among her own party.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a liar. She lies like a rug. The evidence is clear and strong. She lies and then she lies some more. That’s why she’s not fit to be president, but that’s not her only problem. She’s\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a terrible liar. It’s not that she’s a terrible person, it’s that she’s a terrible politician. She should have known better than to even try to lie her way out of this one. She’s so good at\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a \"warmonger\" and should not be president, Donald Trump said on Monday, setting up a likely confrontation with the former secretary of state as the presidential election looms.\nIn a speech at the New York Hilton hotel on\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a dangerous woman\nby Mark Steyn / September 11, 2015 / Leave a comment\nHillary Clinton and her husband Bill Clinton at the 2015 Clinton Global Initiative © Alain Jocard/AFP/Getty Images\nThe\n---\nHillary Clinton is a former U.S. Secretary of State and former First Lady of the United States. A graduate of Yale University and Yale Law School, she served as U.S. Senator for New York from 2001 to 2009,\n---\n\n\n\n\n\nLora model\nMost of the prompts give rather the same results. Probably, I could not find any good prompts for this model. But qualitative results (loss/perplexity on evaluation dataset) speak for itself.\n\n# Evaluate LORA model\noutput_dir = \"./trump_lora\"\npeft_model = PeftModelForCausalLM.from_pretrained(model, output_dir)\npeft_eval_loss = 0\npeft_eval_steps = 0\nwith torch.no_grad():\n    for batch in eval_data_loader:\n        outputs = peft_model(**{k: v for k, v in batch.items() if k != 'labels'}, labels=batch['labels'])\n        peft_eval_loss += outputs.loss.item()\n        peft_eval_steps += 1\npeft_perplexity = math.exp(peft_eval_loss / peft_eval_steps)\nprint(f\"LORA Model Perplexity: {peft_perplexity:.2f}\")\nprint(f\"LORA Model Loss: {peft_eval_loss:.2f}\")\nprint(f\"Perplexity Improvement: {original_perplexity - peft_perplexity:.2f}\")\nprint(f\"Loss improvement: {eval_loss - peft_eval_loss:.2f}\")\n\nLORA Model Perplexity: 1.18\nLORA Model Loss: 13.93\nPerplexity Improvement: 731.37\nLoss improvement: 540.18\n\n\n\ntest_prompts = [\n    \"I think, Donald Trump\",\n    \"I think, Barack Obama\",\n    \"I think, Joe Biden\",\n    \"I think, the Democrats are\",\n    \"We need to\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(3):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, for all his faults, is the most interesting presidential candidate since perhaps William Jennings Bryan. The thing is, I can’t be sure what Trump is thinking. He’s an enigma.\nNow, I don’t want\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump, a lot, about politics, and about the country, and about the world, and about everything else. He is a great thinker, and he thinks about everything. The reason he is successful is because he is a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Donald Trump just won the Presidential election! He will be a great President for our Country! I’m proud to call myself a Donald Trump fan! He’s smart, a great businessman, a great TV personality and a great President!\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama, has set a dangerous precedent with his latest tweet regarding politics and the press. Let me explain.\nThere are a lot of writers out there who just write about politics, and they don’t understand politics at all. They\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a great man, I don't care what his politics are. He's a great man and we need a lot more men like him.\nI'm the best friend you'll ever have. I'm the worst enemy\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Barack Obama is a good man. I hope he is reelected as president of the United States. He has done a great job as president of the United States. I hope he will be reelected as president of the\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is a great person and a great politician. He has always been a great president and I think that he has always done a great job of leading the United States and I think that he is a great leader and I think\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is one of the best presidents the United States of America has ever had. He is the man who brought down the Berlin Wall and the Iron Curtain. He helped end the Cold War and the threat of nuclear war.\nHe\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, Joe Biden is the best for America. He is a very intelligent and kind person, who really cares about others. He knows what is going on in America and what the best for America is. He has a lot of experience in\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are being too clever by half. They are playing a game of “chicken” with the Republicans. The Republicans have not played the game of “chicken” very well. The Democrats have not played the game of\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are scared of Trump because he is a very strong leader, unlike Obama. He has a lot of political skills and I hope he wins the next election.\nI think, the Democrats are scared of Trump because he is a\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nI think, the Democrats are a better party than the Republicans. I would say the Republicans are a joke. And the only thing they really have going is their big money. I think they have the most corrupt politicians in the world. I just\n---\n\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to protect our environment and the lives of all creatures on this planet. We must protect our planet, our natural resources, our environment, and the animals that inhabit it. We must all take action to save the Earth.\nWe all have\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nWe need to have a plan on how to deal with a crisis and the best place to start is with a crisis plan. It is important to know what type of crisis you might be facing and to have a plan in place that can be used\n---\nWe need to protect the environment from pollution by plastic. But what about the plastic inside of us? What is your body made of? Your body is made of different parts and they all serve a different purpose. We need these parts to stay alive\n---\n\n\n\n\ntest_prompts = [\n    \"The United States\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Supreme Court has ruled that federal law does not grant standing to individuals who simply make claims that they were damaged by another party’s actions. In Spokeo, Inc. v. Robins, the Court explained that “standing is\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States’ trade deficit with China was $34.9 billion in the third quarter of 2017, the Commerce Department reported on Tuesday. The trade deficit with China was $375.2 billion in 2017, up from $\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Air Force announced today that the B-52H Stratofortress has earned a top spot on the prestigious 2017 Time Magazine’s Top 100 of the World’s Greatest Machines list. The venerable long-range bomber took\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States Department of State has announced that it is accepting applications for the Diversity Visa Lottery program for the year 2020. The DV-2020 lottery will offer 55,000 visas for immigrants to the USA who would like to live\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is set to host its first ever esports World Cup in Dallas, Texas at the Dallas Convention Center from August 2nd to 4th. The event will be held in the same venue as the International eSports Federation (iSF\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a nation of immigrants, with people from around the world coming to America in search of a better life. However, the process of immigrating to the United States can be complex and daunting, especially for those who are not familiar\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States has become a global superpower as the result of the country's abundant natural resources, efficient markets, and large and motivated population. The United States has a great deal of natural resources, including forests, minerals, oil, and gas\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States is a country of immigrants, and immigrants have always been an important part of our culture. This is why we are such a diverse and multicultural nation, with people from all over the world coming to make their lives here. If you\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nThe United States of America is a federal republic of 50 states and the District of Columbia (DC). The 50 states are the principal political divisions of the country. The District of Columbia is a federal district and is not a state. There\n---\nThe United States is home to many unique attractions, including the Statue of Liberty and the Golden Gate Bridge. The country is also well known for its delicious cuisine, which ranges from the iconic burgers and pizza to the diverse range of international dishes. From\n---\n\n\n\n\ntest_prompts = [\n    \"Hillary Clinton is a\",\n]\n\n\nfor prompt in test_prompts:\n    for _ in range(10):\n        print(generate_example(prompt, peft_model, tokenizer))\n        print(\"---\")\n    print()\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a bad role model for our children.\nThis should be the biggest concern for all parents with children of voting age, but especially parents who support a candidate for President.\nI am not a Clinton supporter, but I do believe that the\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a good writer. She’s a good speaker. She’s a good politician. I don’t believe she’s a good person. And if you want to be president you need to be a good person.\nI don’t know if\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a woman of many contradictions: one of the smartest in the world, and yet a political novice with little experience running for office; a brilliant negotiator, and yet a poor campaigner who has never been able to rally her\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a liar, a hypocrite, and a crook. She has been for years. She is a disgusting person who does not deserve the office of President. She is a weakling who would not be a good leader, she\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a real estate investor and owner of a company with a multimillion-dollar portfolio of real estate investments. Her investments include a building in New York City and in Florida. She has also invested in businesses in the United States and abroad.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a terrible candidate for president of the United States. She is bad on domestic issues. She is bad on foreign issues. She is bad on economics. And she is bad on the Constitution. She would be the worst president we have\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a self-serving politician who uses her position to sell access and influence to donors and special interests, including foreign governments. She is the most corrupt and untrustworthy candidate ever to seek the presidency. She is a liar who has been\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a true fighter and one of the most extraordinary women of all time. Her book Living History is a great read. I’m sure she will continue to do great things for America and the world.\n---\n\n\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n\n\nHillary Clinton is a major liar and manipulator who would do anything to gain power.\nHer policies are a disaster for America and the world. Her agenda is a threat to our freedoms and the Constitution.\nShe has a history of dishonesty and corruption\n---\nHillary Clinton is a lying scum sucking whore who is only interested in selling her ass for a buck and lining her pockets. I don't know why she is even allowed to run for office. She is a disgrace to the country and should be\n---"
  },
  {
    "objectID": "notebooks/s10_ex2.html",
    "href": "notebooks/s10_ex2.html",
    "title": "",
    "section": "",
    "text": "How can such methods be applied to non-quadratic problems? For example, for binary logistic regression: \nf(x)=\\dfrac{\\mu}{2}\\| x \\|^2_2 + \\dfrac{1}{m}\\sum_{i=1}^{m}log(1+exp(-y_i\\langle a_i, x \\rangle)) \\longrightarrow \\min_{x\\in\\mathbb{R}^n}\n\nWe can use the Fletcher-Reeves or Polak-Ribier method. Add the iteration in function ConjugateGradientPR() of the Polak-Ribier method to the code:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.datasets as skldata\nimport jax\nfrom jax import numpy as jnp\nfrom scipy.optimize import minimize_scalar\n\nnp.random.seed(228)\n\ndef generate_problem(m=1000, n=300, mu=1):\n    np.random.seed(228)\n    # Generating synthetic data\n    n = 300  # Number of features\n    m = 1000  # Number of samples\n\n    # Create a binary classification problem\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, n_informative=n//3, random_state=0)\n    X = jnp.array(X)\n    y = jnp.array(y)\n\n    # Regularized logistic regression cost function\n    @jax.jit\n    def f(w):\n        return jnp.linalg.norm(w)**2*mu/2 +  jnp.mean(jnp.logaddexp(jnp.zeros(X.shape[0]), -y * (X @ w)))\n    \n    grad_f = jax.jit(jax.grad(f))\n    x_0 = jax.random.normal(jax.random.PRNGKey(0), (n,))\n\n    return f, grad_f, x_0\n\n# Optimization methods\ndef gradient_descent(f, grad_f, x_0, step_size, iterations):\n    x = x_0.copy()\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad_f(x)))\n    for _ in range(iterations):\n        x -= step_size * grad_f(x)\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad_f(x)))\n    return values, gradients\n\ndef steepest_descent(f, grad_f, x_0, iterations):\n    x = x_0.copy()\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad_f(x)))\n    for _ in range(iterations):\n        grad = grad_f(x)\n        res = minimize_scalar(lambda alpha: f(x - alpha * grad), bounds = (1e-8,1e1), method='Bounded', options={'maxiter': 50})\n        step_size = res.x\n        x -= step_size * grad\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad))\n    return values, gradients\n\ndef ConjugateGradientFR(f, grad_f, x0, iterations, restart=False):\n    x = x0\n    grad = grad_f(x)\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad_f(x)))\n    d = -grad\n    it = 0\n    while it &lt; iterations:\n        res = minimize_scalar(lambda alpha: f(x + alpha * d), bounds = (1e-9,1e1), method='Bounded', options={'maxiter': 50})\n        alpha = res.x\n        x = x + alpha * d\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad))\n        grad_next = grad_f(x)\n        beta = grad_next.dot(grad_next) / grad.dot(grad)\n        d = -grad_next + beta * d\n        grad = grad_next.copy()\n        it += 1\n        if restart and it % restart == 0:\n            grad = grad_f(x)\n            d = -grad\n        \n    return values, gradients\n\ndef ConjugateGradientPR(f, grad_f, x0, iterations, restart=False):\n    x = x0\n    grad = grad_f(x)\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad))\n    d = -grad\n    it = 0\n    while it &lt; iterations:\n        # Line search for the optimal alpha\n        pass \n        ## YOUR CODE HERE ##\n        ## HINT: use scipy.minimize_scalar() ##\n\n        # Calculate beta using Polak-Ribière formula\n        pass\n        ## YOUR CODE HERE ##\n        ## HINT: use lecture notes ##\n        \n    return values, gradients\n\n\ndef run_experiment(params):\n    f, grad_f, x_0 = generate_problem(n=params[\"n\"], m=params[\"m\"], mu=params[\"mu\"])\n\n    if params[\"restart\"] is None:\n        results = {\n            \"methods\": {\n                \"Gradient Descent\": gradient_descent(f, grad_f, x_0, params[\"alpha\"], params[\"iterations\"]),\n                \"Steepest Descent\": steepest_descent(f, grad_f, x_0, params[\"iterations\"]),\n                \"Conjugate Gradients PR\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"]),\n                \"Conjugate Gradients FR\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"]),\n            },\n            \"problem\":{\n                \"params\": params\n            }\n        }\n    else:\n        results = {\n            \"methods\": {\n                \"Gradient Descent\": gradient_descent(f, grad_f, x_0, params[\"alpha\"], params[\"iterations\"]),\n                \"Steepest Descent\": steepest_descent(f, grad_f, x_0, params[\"iterations\"]),\n                \"Conjugate Gradients PR\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"]),\n                f\"Conjugate Gradients PR. restart {params['restart']}\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"], restart=params[\"restart\"]),\n                \"Conjugate Gradients FR\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"]),\n                f\"Conjugate Gradients FR. restart {params['restart']}\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"], restart=params[\"restart\"]),\n            },\n            \"problem\":{\n                \"params\": params\n            }\n        }\n    return results\n\n\ndef plot_results(results):\n    linestyles = {\n        \"Gradient Descent\": \"r-\",\n        \"Steepest Descent\": \"b-.\",\n        \"Conjugate Gradients FR\": \"g--\",\n        f\"Conjugate Gradients FR. restart {results['problem']['params']['restart']}\": \"g-\",\n        \"Conjugate Gradients PR\": \"c--\",\n        f\"Conjugate Gradients PR. restart {results['problem']['params']['restart']}\": \"c-\",\n    }\n    plt.figure(figsize=(10, 3.5))\n    m = results[\"problem\"][\"params\"][\"m\"]\n    mu = results[\"problem\"][\"params\"][\"mu\"]\n    n = results[\"problem\"][\"params\"][\"n\"]\n    restart = results[\"problem\"][\"params\"][\"restart\"]\n    \n    plt.suptitle(f\"Regularized binary logistic regression. n={n}. m={m}. μ={mu}\")\n\n    plt.subplot(1, 2, 1)\n    for method, result_  in results[\"methods\"].items():\n        plt.semilogy(result_[0], linestyles[method])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$f(x)$')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(1, 2, 2)\n    for method, result_ in results[\"methods\"].items():\n        plt.semilogy(result_[1], linestyles[method], label=method)\n    plt.ylabel(r'$\\|\\nabla f(x)\\|_2$')\n    plt.xlabel('Iteration')\n    plt.grid(linestyle=\":\")\n\n    # Place the legend below the plots\n    if results['problem']['params']['restart'] == None:\n        plt.figlegend(loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.00))\n        plt.tight_layout(rect=[0, 0.05, 1, 1])\n    else:\n        plt.figlegend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.02))\n        plt.tight_layout(rect=[0, 0.1, 1, 1])\n    # Adjust layout to make space for the legend below\n    # plt.savefig(f\"cg_non_linear_{m}_{n}_{mu}_{restart}.pdf\")\n    plt.show()\n\nRun experiments for different \\mu and dimentions:\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 0,\n    \"alpha\": 1e-1,\n    \"iterations\": 200,\n    \"restart\": None\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 1,\n    \"alpha\": 3e-2,\n    \"iterations\": 200,\n    \"restart\": None\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 1,\n    \"alpha\": 3e-2,\n    \"iterations\": 200,\n    \"restart\": 20\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 10,\n    \"alpha\": 1e-2,\n    \"iterations\": 200,\n    \"restart\": None\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 10,\n    \"alpha\": 1e-2,\n    \"iterations\": 200,\n    \"restart\": 20\n}\n\nresults = run_experiment(params)\nplot_results(results)"
  },
  {
    "objectID": "notebooks/s10_pathological_example.html",
    "href": "notebooks/s10_pathological_example.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import diags\n\ndef build_W(n, t):\n    main_diag = [t] + [1 + t] * (n - 2) + [1 + t]\n    off_diag = [np.sqrt(t)] * (n - 1)\n    return diags([main_diag, off_diag, off_diag], [0, 1, -1], shape=(n, n)).toarray()\n\ndef conjugate_gradient(W, b, max_iter=None):\n    n = len(b)\n    x = np.zeros(n)\n    r = b - W @ x\n    p = r.copy()\n    rsold = r.dot(r)\n    residuals = [rsold]\n    if max_iter is None:\n        max_iter = n\n    for _ in range(max_iter):\n        Ap = W @ p\n        alpha = rsold / p.dot(Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rsnew = r.dot(r)\n        residuals.append(rsnew)\n        if rsnew &lt; 1e-15:\n            break\n        p = r + (rsnew / rsold) * p\n        rsold = rsnew\n    return x, residuals\n\n# Parameters\nt = 0.1\nn = 10\n\n# Building problem and solution\nW = build_W(n, t)\nb = np.zeros(n)\nb[0] = 1\nx, residuals = conjugate_gradient(W, b)\n\n\n# Graph\nplt.figure(figsize=(10, 6))\nplt.semilogy(residuals, 'o-', label='Real residuals (||b - Wx_k||²)', markersize=8)\nplt.xlabel('Iteration number')\nplt.ylabel('Residual norm (log scale)')\nplt.title('Convergence of the CG method for a pathological example')\nplt.legend()\nplt.grid(True, which='both', linestyle='--', alpha=0.5)\nplt.show()"
  },
  {
    "objectID": "notebooks/s2_brent.html",
    "href": "notebooks/s2_brent.html",
    "title": "",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize_scalar\n\n# Define two test functions\ndef f1(x):\n    return np.sin(x) + 0.1 * (x ** 2)\n\ndef f2(x):\n    return np.exp(-x**2) + 0.5 * np.cos(3 * x)\n\n# Optimization settings\nmethods = ['brent', 'golden']\nfunctions = [f1, f2]\nfunction_names = [r'$f_1(x) = sin(x) + 0.1 \\cdot x^2$', r'$f_2(x) = exp(-x^2) + 0.5 \\cdot cos(3x)$']\n\n# Store results for plotting\nresults = {}\n\nfor func, func_name in zip(functions, function_names):\n    results[func_name] = {}\n    for method in methods:\n        function_call_values = []\n\n        # Wrap the function to track calls\n        def wrapped_func(x):\n            value = func(x)\n            function_call_values.append(value)\n            return value\n\n        result = minimize_scalar(wrapped_func, method=method)\n\n        # Collect data for plotting\n        results[func_name][method] = {\n            'function_values': function_call_values,\n            'num_calls': len(function_call_values),\n            'x_min': result.x,\n            'fun_min': result.fun\n        }\n\n# Plot results\nfor func_name in function_names:\n    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n\n    # Plot function value vs. iteration number\n    for method in methods:\n        ax.plot(\n            range(len(results[func_name][method]['function_values'])),\n            [fv - results[func_name][method]['fun_min'] for fv in results[func_name][method]['function_values']],\n            label=f'{method.capitalize()} Method'\n        )\n    ax.set_yscale('log')\n    ax.set_title(func_name)\n    ax.set_xlabel('Iteration Number')\n    ax.set_ylabel('Function Value - Optimal Value (log scale)')\n    ax.legend()\n    ax.grid()\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "notebooks/s13_svm_hingeloss.html",
    "href": "notebooks/s13_svm_hingeloss.html",
    "title": "",
    "section": "",
    "text": "A problem of the form is solved: \nL(w) = \\frac{\\lambda}{2} \\lVert w \\rVert^2 + \\frac{1}{N} \\sum_{i=1}^{N} \\max \\left\\{ 0,\\ 1 - y_i w^T x_i \\right\\}.\n It can be shown that the subgradient \ng(w) = \\lambda w + \\frac{1}{N} \\sum_{i=1}^{N}\n\\begin{cases}\n  -y_i x_i, & \\text{if  } \\  y_i w^T x_i &lt; 1, \\\\\n  0, & \\text{otherwise}.\n\\end{cases}\n The function \\ell_i(w) = \\max\\left\\{ 0,\\ 1 - y_i w^T x_i \\right\\} is called hinge loss.\nLet’s generate data for classification:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nN = 100  # number of objects\nX1 = np.random.randn(N//2, 2) + np.array([2, 2])\nX2 = np.random.randn(N//2, 2) + np.array([-2, -2])\n\nX = np.vstack([X1, X2])          # the matrix of objects x features\ny = np.hstack([np.ones(N//2), -np.ones(N//2)])  # class labels: +1, -1\n\n# Let's mix everything together for clarity\nperm = np.random.permutation(N)\nX = X[perm]\ny = y[perm]\n\nObjective (hinge loss):\n\ndef hinge_svm_objective(w, X, y, lambd):\n    \"\"\"\n    Returns the value of the functional:\n      L(w) = (lambda/2)*||w||^2 + (1/N)*sum( max(0, 1 - y_i * w^T x_i) ).\n    \"\"\"\n    margins = 1 - y * (X.dot(w))\n    hinge_losses = np.maximum(0, margins)\n    return (lambd/2)*np.sum(w**2) + np.mean(hinge_losses)\n\nThe subgradient of the functional:\n\ndef hinge_svm_subgradient(w, X, y, lambd):\n    \"\"\"\n    Returns the subgradient g(w) for:\n      g(w) = lambda*w + (1/N)*sum( -y_i*x_i, если y_i*w^T x_i &lt; 1 ).\n    \"\"\"\n    N = X.shape[0]\n    margins = 1 - y * (X.dot(w))\n    active_mask = (margins &gt; 0).astype(float)  # где hinge&gt;0\n    \n    # Суммируем -y_i*x_i по всем активным (margin&gt;0)\n    grad_hinge = -(y * active_mask)[:, np.newaxis] * X\n    grad_hinge = grad_hinge.mean(axis=0)\n    \n    g = lambd*w + grad_hinge\n    return g\n\nSubgradient descent implementation:\n\ndef subgradient_descent(\n    X, y, lambd=0.01, \n    max_iter=150, \n    step_rule='constant', \n    c=1.0,\n    f_star=0.0\n):\n    \"\"\"\n    Subgradient descent for SVM with hinge loss:\n    \n    Параметры:\n      X, y       : selection\n      lambd      : the regularization coefficient\n      max_iter   : iteration number\n      step_rule  : step selection strategy:\n                     - 'constant'       -&gt; alpha_k = c\n                     - '1_over_k'       -&gt; alpha_k = c/k\n                     - '1_over_sqrt_k'  -&gt; alpha_k = c/sqrt(k)\n                     - 'polyak'         -&gt; alpha_k = (f(w_k) - f_star)/||g_k||^2\n      c          : constant for the step (used in all but not for 'polyak')\n      f_star     : estimated minimum (for polyak)\n    \n    Возвращает:\n      w_history   : the list of vectors w at each iteration\n      loss_history: the list of values of the functional L(w) at each iteration\n    \"\"\"\n    w = np.zeros(X.shape[1])\n    w_history = [w.copy()]\n    loss_history = [hinge_svm_objective(w, X, y, lambd)]\n    \n    for k in range(1, max_iter+1):\n        g = hinge_svm_subgradient(w, X, y, lambd)\n        loss_current = hinge_svm_objective(w, X, y, lambd)\n        \n        # Выбираем шаг\n        if step_rule == 'constant':\n            alpha = c\n        elif step_rule == '1_over_k':\n            alpha = c / k\n        elif step_rule == '1_over_sqrt_k':\n            alpha = c / np.sqrt(k)\n        elif step_rule == 'polyak':\n            # Polyak's step: (f(w) - f_star) / ||g||^2 (with a zero cut-off from below)\n            denom = np.dot(g, g)\n            if denom &lt; 1e-15:\n                alpha = 0.0\n            else:\n                alpha = (loss_current - f_star) / denom\n                alpha = max(alpha, 0.0)  # чтобы шаг не был отрицательным\n        else:\n            raise ValueError(\"Неизвестная стратегия шага\")\n        \n        # Update w\n        w = w - alpha*g\n        \n        w_history.append(w.copy())\n        loss_history.append(hinge_svm_objective(w, X, y, lambd))\n    \n    return w_history, loss_history\n\nRunning an experiment for different step rules:\n\nmax_iter = 50\nlambd = 0.01\n\n# For example Polyak step size will be considered f^*=0 (simplification for demonstration)\nf_star_demo = 0.0\n\nstrategies = {\n    'constant':         1.5,   # constant step size\n    '1_over_k':         5.0,   # for 1/k\n    '1_over_sqrt_k':    5.0,   # for 1/sqrt(k)\n    'polyak':           None   # for Polyak is None\n}\n\nresults = {}\nfor rule, c_val in strategies.items():\n    w_hist, loss_hist = subgradient_descent(\n        X, y, lambd=lambd, max_iter=max_iter,\n        step_rule=rule,\n        c=c_val if c_val is not None else 1.0,\n        f_star=f_star_demo\n    )\n    results[rule] = (w_hist, loss_hist)\n\nVizualization:\n\nplt.figure()\nfor rule in strategies.keys():\n    loss_hist = results[rule][1]\n    plt.plot(loss_hist, label=f\"Stepsize rule: {rule}\")\nplt.xlabel(\"Iteration number\")\nplt.ylabel(\"L(w)\")\nplt.title(\"Comparison of subgradient descent (SVM, hinge loss)\\n with different steps\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nplt.figure()\nplt.scatter(X[y==1, 0], X[y==1, 1], label=\"Class +1\")\nplt.scatter(X[y==-1, 0], X[y==-1, 1], label=\"Class -1\")\n\nx_vals = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200)\nfor rule in strategies.keys():\n    w_final = results[rule][0][-1]\n    if abs(w_final[1]) &lt; 1e-15:\n        x_const = -0.0\n        plt.plot([x_const, x_const], [X[:,1].min()-1, X[:,1].max()+1],\n                 label=f\"Dividing line ({rule})\")\n    else:\n        y_vals = -(w_final[0]/w_final[1]) * x_vals\n        plt.plot(x_vals, y_vals, label=f\"Dividing line ({rule})\")\n\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.title(\"Dividing lines after 150 iterations\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/s5_probability_simplex_projection.html",
    "href": "notebooks/s5_probability_simplex_projection.html",
    "title": "",
    "section": "",
    "text": "import time\nimport random\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nHere we can find implementaton of the 1st \\mathcal{O}(n \\log n) algorithm, using Quicksort:\n\ndef project_simplex_sort(y):\n    \"\"\"\n    Projects the vector y onto the unit simplex {x &gt;= 0, sum(x) = 1}.\n    Difficulty: O(n log n).\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    n = len(y)\n    \n    # If the sum is already &lt;= 1 and all coordinates are non-negative,\n    # then this is already a point on the simplex (you need to check).\n    # But the classics usually assume sum(y) &gt;= 1, \n    # nevertheless, we will add protection:\n    if np.all(y &gt;= 0) and np.abs(y.sum() - 1.0) &lt; 1e-12:\n        return y.copy()\n    \n    # Sort in descending order\n    y_sorted = np.sort(y)[::-1]\n    y_cumsum = np.cumsum(y_sorted)\n    \n    # Finding rho\n    # We are looking for the largest k for which y_sorted[k] - (cumsum[k]-1)/(k+1) &gt; 0\n    rho = 0\n    for k in range(n):\n        val = y_sorted[k] - (y_cumsum[k] - 1.0)/(k + 1)\n        if val &gt; 0:\n            rho = k + 1\n    \n    # Counting the theta threshold\n    theta = (y_cumsum[rho - 1] - 1.0) / rho\n    \n    # Building x\n    x = np.maximum(y - theta, 0.0)\n    return x\n\nAnd here we can find an implementation of an algorithm of average complexity \\mathcal{O}(n), but in the worst case \\mathcal{O}(n^2). The idea is that we consistently (as a recursive or iterative approach) search for the “pivot” threshold so that about half of the elements end up on one side of the threshold. Due to randomization, the average work time is obtained \\mathcal{O}(n):\n\ndef project_simplex_linear(y):\n    \"\"\"\n    Projects the vector y onto the unit simplex,\n    using the idea of a quick pivot selection.\n    Average difficulty: O(n).\n    \"\"\"\n    y = np.asarray(y, dtype=float)\n    n = len(y)\n    \n    # If the sum is not more than 1 and y &gt;= 0, then it is already in the simplex\n    if np.all(y &gt;= 0) and y.sum() &lt;= 1.0:\n        return y.copy()\n    \n    # Auxiliary function for recursive search\n    def find_pivot_and_sum(indices, current_sum, current_count):\n        if not indices:\n            return current_sum, current_count, [], True\n        \n        # Randomly choosing the index for the pivot\n        pivot_idx = random.choice(indices)\n        pivot_val = y[pivot_idx]\n        \n        # Dividing the elements into &gt;= pivot and &lt; pivot\n        bigger = []\n        smaller = []\n        sum_bigger = 0.0\n        \n        for idx in indices:\n            val = y[idx]\n            if val &gt;= pivot_val:\n                bigger.append(idx)\n                sum_bigger += val\n            else:\n                smaller.append(idx)\n        \n        # Checking to see if we have reached the condition\n        # sum_{waltz&gt;= pivot_val} (val - pivot_val) &lt; 1 ?\n        # Considering that we already have current_sum/current_count\n        new_sum = current_sum + sum_bigger\n        new_count = current_count + len(bigger)\n        \n        # Condition: sum_{v&gt;= pivot} (pivot) = new_sum - new_count * pivot\n        # Compare with 1\n        if (new_sum - new_count * pivot_val) &lt; 1.0:\n            # So pivot_val can still be (or higher)\n            # -&gt; moving towards the \"smaller ones\" (where we can raise the pivot)\n            return (new_sum, new_count, smaller, False)\n        else:\n            # pivot_val is too big, we need to go to the \"big ones\",\n            # i.e. those that are exactly &gt;= pivot, we stay with them\n            # (which may be even higher than the actual threshold).\n            # But pivot_idx itself is also being removed from the proceedings. \n            # (since we know for sure that pivot_val &lt; the true threshold).\n            if pivot_idx in bigger:\n                bigger.remove(pivot_idx)\n                new_sum -= pivot_val\n                new_count -= 1\n            return (current_sum, current_count, bigger, False)\n    \n    indices = list(range(n))\n    s = 0.0    \n    c = 0      \n    while indices:\n        s, c, indices, done = find_pivot_and_sum(indices, s, c)\n        if done:\n            break\n    \n    # When finished, we have \"rho =c\" and \"sum =s\"\n    # theta = (s - 1)/c\n    theta = (s - 1.0)/c\n    x = np.maximum(y - theta, 0)\n    return x\n\nLet’s generate several large-dimensional vectors (for example, from 10.000 to 500.000) and measure the running time of both simplex projection algorithms:\n\ndef check_projection_simplex(x, tol=1e-9):\n    \"\"\"\n    Проверяет, что x проецирован на единичный симплекс:\n    1) x_i &gt;= 0 для всех i\n    2) sum(x_i) ~ 1 (с некоторой точностью)\n    \"\"\"\n    if (x &lt; -tol).any():\n        return False\n    s = x.sum()\n    return abs(s - 1.0) &lt; tol\n\ndef generate_dims(start, stop, step):\n    return np.arange(start, stop + step, step).tolist()\n\ndims = generate_dims(10_000, 1_950_000, 50_000)\ntimes_sort = []\ntimes_linear = []\n\nnp.random.seed(42)\n\nfor d in dims:\n    y = np.random.rand(d) * 2.0\n    \n    start = time.perf_counter()\n    x_sort = project_simplex_sort(y)\n    t_sort = time.perf_counter() - start\n    \n    start = time.perf_counter()\n    x_lin = project_simplex_linear(y)\n    t_lin = time.perf_counter() - start\n    \n    times_sort.append(t_sort)\n    times_linear.append(t_lin)\n    \n    assert check_projection_simplex(x_sort), \"Sort-based projection incorrect!\"\n    assert check_projection_simplex(x_lin),  \"Linear-based projection incorrect!\"\n    \n    print(f\"dim={d}, time_sort={t_sort:.4f}s, time_lin={t_lin:.4f}s\")\n\n# Построим графики\nplt.figure(figsize=(8, 5))\nplt.plot(dims, times_sort, 'o--', label='Sort-based O(n log n)')\nplt.plot(dims, times_linear, 'o--', label='Quickselect-based O(n) (avg)')\nplt.xlabel('Dimension n')\nplt.ylabel('Time (s)')\nplt.title('Comparison of projection time on a simplex (Sort vs Quickselect)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\ndim=10000, time_sort=0.0037s, time_lin=0.0033s\ndim=60000, time_sort=0.0153s, time_lin=0.0085s\ndim=110000, time_sort=0.0227s, time_lin=0.0115s\ndim=160000, time_sort=0.0325s, time_lin=0.0303s\ndim=210000, time_sort=0.0428s, time_lin=0.0373s\ndim=260000, time_sort=0.0510s, time_lin=0.0434s\ndim=310000, time_sort=0.0610s, time_lin=0.0210s\ndim=360000, time_sort=0.0708s, time_lin=0.0269s\ndim=410000, time_sort=0.0802s, time_lin=0.0416s\ndim=460000, time_sort=0.0919s, time_lin=0.0506s\ndim=510000, time_sort=0.0997s, time_lin=0.0847s\ndim=560000, time_sort=0.1141s, time_lin=0.0397s\ndim=610000, time_sort=0.1240s, time_lin=0.1871s\ndim=660000, time_sort=0.1349s, time_lin=0.0911s\ndim=710000, time_sort=0.1449s, time_lin=0.1910s\ndim=760000, time_sort=0.1525s, time_lin=0.1030s\ndim=810000, time_sort=0.1680s, time_lin=0.0513s\ndim=860000, time_sort=0.1702s, time_lin=0.1326s\ndim=910000, time_sort=0.1824s, time_lin=0.1944s\ndim=960000, time_sort=0.1931s, time_lin=0.1624s\ndim=1010000, time_sort=0.2021s, time_lin=0.1996s\ndim=1060000, time_sort=0.2140s, time_lin=0.1413s\ndim=1110000, time_sort=0.2287s, time_lin=0.1847s\ndim=1160000, time_sort=0.2557s, time_lin=0.2943s\ndim=1210000, time_sort=0.2475s, time_lin=0.2495s\ndim=1260000, time_sort=0.2578s, time_lin=0.2290s\ndim=1310000, time_sort=0.2626s, time_lin=0.1429s\ndim=1360000, time_sort=0.2762s, time_lin=0.2241s\ndim=1410000, time_sort=0.2805s, time_lin=0.2289s\ndim=1460000, time_sort=0.2921s, time_lin=0.1103s\ndim=1510000, time_sort=0.3057s, time_lin=0.1602s\ndim=1560000, time_sort=0.3176s, time_lin=0.2236s\ndim=1610000, time_sort=0.3205s, time_lin=0.2146s\ndim=1660000, time_sort=0.3392s, time_lin=0.3725s\ndim=1710000, time_sort=0.3442s, time_lin=0.2719s\ndim=1760000, time_sort=0.3570s, time_lin=0.3431s\ndim=1810000, time_sort=0.3645s, time_lin=0.1760s\ndim=1860000, time_sort=0.3777s, time_lin=0.1566s\ndim=1910000, time_sort=0.4000s, time_lin=0.2485s\ndim=1960000, time_sort=0.4205s, time_lin=0.1840s\n\n\n\n\n\n\n\n\n\nThus, we see that the second algorithm is always superior to the first one on average, but in some few cases the first one is superior to the second one. Apparently, in these cases, the most “inconvenient” cases for the second algorithm are implemented."
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "",
    "section": "",
    "text": "Linear algebra basics\n\n[5 points] Sensitivity Analysis in Linear Systems Consider a nonsingular matrix A \\in \\mathbb{R}^{n \\times n} and a vector b \\in \\mathbb{R}^n. Suppose that due to measurement or computational errors, the vector b is perturbed to \\tilde{b} = b + \\delta b.\n\nDerive an upper bound for the relative error in the solution x of the system Ax = b in terms of the condition number \\kappa(A) and the relative error in b.\n\nProvide a concrete example using a 2 \\times 2 matrix where \\kappa(A) is large (say, \\geq 100500).\n\n[5 points] Effect of Diagonal Scaling on Rank Let A \\in \\mathbb{R}^{n \\times n} be a matrix with rank r. Suppose D \\in \\mathbb{R}^{n \\times n} is a diagonal matrix. Determine the rank of the product DA. Explain your reasoning.\n[8 points] Unexpected SVD Compute the Singular Value Decomposition (SVD) of the following matrices:\n\nA_1 = \\begin{bmatrix} 2 \\\\ 2 \\\\ 8 \\end{bmatrix}\nA_2 = \\begin{bmatrix} 0 & x \\\\ x & 0 \\\\ 0 & 0 \\end{bmatrix}, where x is the sum of your birthdate numbers (day + month).\n\n[10 points] Effect of normalization on rank Assume we have a set of data points x^{(i)}\\in\\mathbb{R}^{n},\\,i=1,\\dots,m, and decide to represent this data as a matrix \nX =\n\\begin{pmatrix}\n  | & & | \\\\\n  x^{(1)} & \\dots & x^{(m)} \\\\\n  | & & | \\\\\n\\end{pmatrix} \\in \\mathbb{R}^{n \\times m}.\n\nWe suppose that \\text{rank}\\,X = r.\nIn the problem below, we ask you to find the rank of some matrix M related to X. In particular, you need to find relation between \\text{rank}\\,X = r and \\text{rank}\\,M, e.g., that the rank of M is always larger/smaller than the rank of X or that \\text{rank}\\,M = \\text{rank}\\,X \\big / 35. Please support your answer with legitimate arguments and make the answer as accurate as possible.\nNote that border cases are possible depending on the structure of the matrix X. Make sure to cover them in your answer correctly.\nIn applied statistics and machine learning, data is often normalized. One particularly popular strategy is to subtract the estimated mean \\mu and divide by the square root of the estimated variance \\sigma^2. i.e. \nx \\rightarrow (x - \\mu) \\big / \\sigma.\n After the normalization, we get a new matrix \n\\begin{split}\nY &:=\n\\begin{pmatrix}\n  | & & | \\\\\n  y^{(1)} & \\dots & y^{(m)} \\\\\n  | & & | \\\\\n\\end{pmatrix},\\\\\ny^{(i)} &:= \\frac{x^{(i)} - \\frac{1}{m}\\sum_{j=1}^{m} x^{(j)}}{\\sigma}.\n\\end{split}\n What is the rank of Y if \\text{rank} \\; X = r? Here \\sigma is a vector and the division is element-wise. The reason for this is that different features might have different scales. Specifically: \n\\sigma_i = \\sqrt{\\frac{1}{m}\\sum_{j=1}^{m} \\left(x_i^{(j)}\\right)^2 - \\left(\\frac{1}{m}\\sum_{j=1}^{m} x_i^{(j)}\\right)^2}.\n\n[20 points] Image Compression with Truncated SVD Explore image compression using Truncated Singular Value Decomposition (SVD). Understand how varying the number of singular values affects the quality of the compressed image. Implement a Python script to compress a grayscale image using Truncated SVD and visualize the compression quality.\n\nTruncated SVD: Decomposes an image A into U, S, and V matrices. The compressed image is reconstructed using a subset of singular values.\nMathematical Representation: \n  A \\approx U_k \\Sigma_k V_k^T\n  \n\nU_k and V_k are the first k columns of U and V, respectively.\n\\Sigma_k is a diagonal matrix with the top k singular values.\nRelative Error: Measures the fidelity of the compressed image compared to the original. \n  \\text{Relative Error} = \\frac{\\| A - A_k \\|}{\\| A \\|}\n  \n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nfrom skimage import io, color\nimport requests\nfrom io import BytesIO\n\ndef download_image(url):\n    response = requests.get(url)\n    img = io.imread(BytesIO(response.content))\n    return color.rgb2gray(img)  # Convert to grayscale\n\ndef update_plot(i, img_plot, error_plot, U, S, V, original_img, errors, ranks, ax1, ax2):\n    # Adjust rank based on the frame index\n    if i &lt; 70:\n        rank = i + 1\n    else:\n        rank = 70 + (i - 69) * 10\n\n    reconstructed_img = ... # YOUR CODE HERE \n\n    # Calculate relative error\n    relative_error = ... # YOUR CODE HERE\n    errors.append(relative_error)\n    ranks.append(rank)\n\n    # Update the image plot and title\n    img_plot.set_data(reconstructed_img)\n    ax1.set_title(f\"Image compression with SVD\\n Rank {rank}; Relative error {relative_error:.2f}\")\n\n    # Remove axis ticks and labels from the first subplot (ax1)\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n\n    # Update the error plot\n    error_plot.set_data(ranks, errors)\n    ax2.set_xlim(1, len(S))\n    ax2.grid(linestyle=\":\")\n    ax2.set_ylim(1e-4, 0.5)\n    ax2.set_ylabel('Relative Error')\n    ax2.set_xlabel('Rank')\n    ax2.set_title('Relative Error over Rank')\n    ax2.semilogy()\n\n    # Set xticks to show rank numbers\n    ax2.set_xticks(range(1, len(S)+1, max(len(S)//10, 1)))  # Adjust the step size as needed\n    plt.tight_layout()\n\n    return img_plot, error_plot\n\n\ndef create_animation(image, filename='svd_animation.mp4'):\n    U, S, V = np.linalg.svd(image, full_matrices=False)\n    errors = []\n    ranks = []\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 8))\n    img_plot = ax1.imshow(image, cmap='gray', animated=True)\n    error_plot, = ax2.plot([], [], 'r-', animated=True)  # Initial empty plot for errors\n\n    # Add watermark\n    ax1.text(1, 1.02, '@fminxyz', transform=ax1.transAxes, color='gray', va='bottom', ha='right', fontsize=9)\n\n    # Determine frames for the animation\n    initial_frames = list(range(70))  # First 70 ranks\n    subsequent_frames = list(range(70, len(S), 10))  # Every 10th rank after 70\n    frames = initial_frames + subsequent_frames\n\n    ani = animation.FuncAnimation(fig, update_plot, frames=len(frames), fargs=(img_plot, error_plot, U, S, V, image, errors, ranks, ax1, ax2), interval=50, blit=True)\n    ani.save(filename, writer='ffmpeg', fps=8, dpi=300)\n\n    # URL of the image\n    url = \"\"\n\n    # Download the image and create the animation\n    image = download_image(url)\n    create_animation(image)\n\n\n\nConvergence rates\n\n[6 points] Determine (it means to prove the character of convergence if it is convergent) the convergence or divergence of a given sequences\n\nr_{k} = \\frac{1}{\\sqrt{k+5}}.\nr_{k} = 0.101^k.\nr_{k} = 0.101^{2^k}.\n\n[8 points] Let the sequence \\{r_k\\} be defined by \nr_{k+1} =\n\\begin{cases}\n\\frac{1}{2}\\,r_k, & \\text{if } k \\text{ is even}, \\\\\nr_k^2, & \\text{if } k \\text{ is odd},\n\\end{cases}\n with initial value 0 &lt; r_0 &lt; 1. Prove that \\{r_k\\} converges to 0 and analyze its convergence rate. In your answer, determine whether the overall convergence is linear, superlinear, or quadratic.\n[6 points] Determine the following sequence \\{r_k\\} by convergence rate (linear, sublinear, superlinear). In the case of superlinear convergence, determine whether there is quadratic convergence. \nr_k = \\dfrac{1}{k!}\n\n[8 points] Consider the recursive sequence defined by \nr_{k+1} = \\lambda\\,r_k + (1-\\lambda)\\,r_k^p,\\quad k\\ge0,\n where \\lambda\\in [0,1) and p&gt;1. Which additional conditions on r_0 should be satisfied for the sequence to converge? Show that when \\lambda&gt;0 the sequence converges to 0 with a linear rate (with asymptotic constant \\lambda), and when \\lambda=0 determine the convergence rate in terms of p. In particular, for p=2 decide whether the convergence is quadratic."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "",
    "section": "",
    "text": "Занятие 1\n    \n        📄 Презентация • 📝 Заметки • 👷‍♂️ Seminar • ▶️ Youtube • 💿 Скачать\n    \n    Вспоминаем линейную алгебру. Некоторые матричные разложения. Скорость сходимости.\n\n    Занятие 2\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Одномерная оптимизация. Неточная одномерная оптимизация. Градиент. Гессиан. Матрично-векторное дифференцирование.\n\n    Занятие 3\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Автоматическое дифференцирование. Вычислительный граф.\n\n    Занятие 4\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Выпуклость. Выпуклые множества. Выпуклые функции. Неравенство Йенсена. Сильно выпуклые функции. Условие Поляка - Лоясиевича. Минимумы линейных нейронных сетей.\n\n    Занятие 5\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Условия оптимальности. Функция Лагранжа. Задачи с ограничениями-равенствами. Задачи с ограничениями-равенствами. Теорема Каруша - Куна - Таккера.\n\n    Занятие 6\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Двойственность.\n\n    Занятие 7\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Задача линейного программирования. Симплекс метод.\n\n    Занятие 8\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Градиентный спуск. Теоремы сходимости в гладком случае (выпуклые, сильно выпуклые, PL). Верхние и нижние оценки сходимости.\n\n    Занятие 9\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Ускоренные градиентные методы. Метод Поляка, Нестерова.\n\n    Занятие 10\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Метод сопряженных градиентов.\n\n    Занятие 11\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Метод Ньютона. Квазиньютоновские методы.\n\n    Занятие 12\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Градиентные методы в условных задачах оптимизации - метод проекции градиента. Метод Франк - Вульфа. Идея метода зеркального спуска.\n\n    Занятие 13\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Субградиент. Субдифференциал. Субградиентный спуск. Теоремы сходимости в негладком случае. Особенности работы градиентного метода в практических негладких задачах.\n\n    Занятие 14\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Проксимальный градиентный метод.\n\n    Занятие 15\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Стохастический градиентный спуск.\n\n    Занятие 16\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Методы редукции дисперсии: SAG, SVRG, SAGA. Адаптивные стохастические градиентные методы.\n\n    Занятие 17\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Обучение нейронных сетей с точки зрения методов оптимизации. Обобщающая способность моделей машинного обучения. Double Descent. Grokking. Mode connectivity.\n\n    Занятие 18\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Вопросы обучения больших моделей. Lars, Lamb. Learning rate schedulers. Warm-up. MultiGPU training.\n\n    Занятие 19\n    \n        📄 Презентация • 👷‍♂️ Seminar\n    \n    Введение в двойственные методы оптимизации. Метод двойственного градиентного подъёма. Метод модифицированной функции Лагранжа. ADMM.\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Методы оптимизации в машинном обучении",
    "section": "",
    "text": "Методы оптимизации в машинном обучении\n\nКурс для студентов 3-го курса ФКН ВШЭ. 1 лекция + 1 семинар в неделю\nКурс охватывает темы выпуклой, невыпуклой, непрерывной оптимизации, особенно мотивированные задачами и приложениями в Машинном Обучении. Рассматриваются разные темы - от фундаментальных материалов до недавних исследований\nОписание курса\n\nYour browser does not support the video tag.\n\n\nTeam\n\n\n    \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Даниил Меркулов\n                    \n                    Преподаватель\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Алексей Ребриков\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Всеволод Куйбида\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Екатерина Бородич\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Илья Забара\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Кирилл Муравьев\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Степан Дергачев\n                    \n                    Ментор проектов\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Ян Максимов\n                    \n                    Семинарист\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Алина Мирзоева\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Анастасия Белоцерковец\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Амир Афлятунов\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Арина Зайцева\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Артем Митин\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Артём Потарусов\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Владислав Гарбуз\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Григорий Злотин\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Даниил Тихонов\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Максим Панов\n                    \n                    Ассистент\n                  \n                \n              \n        \n            \n                \n                  \n                    \n                  \n                  \n                    \n                      Полина Рыльцева\n                    \n                    Ассистент\n                  \n                \n              \n        \n    \n\nNo matching items"
  },
  {
    "objectID": "notebooks/s9_hobbit_village.html",
    "href": "notebooks/s9_hobbit_village.html",
    "title": "",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport copy\nsns.set()\n\nBelow one can find function plotting the village\n\ndef plot_village(coordinates, l=1):\n    # Checking, that all the coordinates are less than l\n    assert (coordinates &lt;= l).all(), 'All the houses should be in a village'\n\n    # Draw horizontal line\n    plt.hlines(0, 0, l)\n    plt.xlim(0, l)\n    plt.ylim(-0.5, 0.5)\n\n    # Draw house points\n    y = np.zeros(np.shape(coordinates))\n    plt.title('The Hobbit Village')\n    plt.plot(coordinates,y,'o',ms = 10)\n    plt.axis('off')\n    plt.xlabel('Coordinates')\n    fig = plt.gcf()\n    fig.set_size_inches(15, 1)\n    plt.show()\n\n\nN = 25\nl = 10\nx = np.random.rand(N)*l\n\nplot_village(x, l)\n\n\n\n\n\n\n\n\nThe inhabitants of a one-dimensional village want to connect to the Internet, so they need a central service station from which a cable will stretch to all the houses in the village. Let the price of the cable to be pulled from the station to each house independently be determined by some function p(d) . Then it is clear that the village will have to pay the following amount for access to the World Wide Web: \nP(w, x) = \\sum\\limits_{i=1}^N p(d_i) = \\sum\\limits_{i=1}^N p(|w - x_i|)\n w - station location, x_i - location of i house.\nWrite analytical solution w^* for minimization P(w,x), if p(d) = d^2\n ==YOUR ANSWER== \nWrite loss function P(x,w)\n\n### YOUR CODE HERE\n\nPlot loss function on the range (0, l)\n\n### YOUR CODE\n\nWrite gradient of loss function\n\n### YOUR CODE HERE\n\nPlot gradient of loss function on the range (0,l). Which point on the graph is of particular interest? Why?\n\n### YOUR CODE\n\nWrite function gradient_descent, which returns w_k after a fixed number of steps.\n\nw_{k+1} = w_k - \\mu \\nabla P(w_k)\n\n\ndef gradient_descent(x, dP, w0, mu, Nsteps):\n    ### YOUR CODE HERE\n    pass\n\nModify gradient_descent to return all optimization trajectory. Plot loss function trajectory for the following learning rates \\mu = 0.01, 0.1, 0.15, 0.19, 0.20, 0.21.\nDraw conclusions.\n\ndef gradient_descent(x, dP, w0, mu, Nsteps):\n    ### YOUR CODE HERE\n    pass\n\nAccelerate the process: use HB method and NAG Plot loss function trajectory for \\mu = 0.01 and different values of momentum parameters. Draw conclusions.\nHint: use optimal parameters for step size and momentum from lecture\n\n### YOUR CODE HERE"
  },
  {
    "objectID": "notebooks/s4_benchmarx_convex.html",
    "href": "notebooks/s4_benchmarx_convex.html",
    "title": "Support Vector Machine",
    "section": "",
    "text": "!pip install numpy matplotlib jax scipy scikit-optimize ucimlrepo optax\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\nimport jax\nfrom jax import numpy as jnp, grad\nfrom scipy.optimize import minimize_scalar\nimport jax.numpy as jnp\nfrom jax import grad, jit, hessian\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\nfrom ucimlrepo import fetch_ucirepo\nfrom optax.losses import safe_softmax_cross_entropy as cros_entr\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize_scalar\nimport sklearn.datasets as skldata\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\nArray([  0, 228], dtype=uint32)\n\n\n\n@jit\ndef logistic_loss(w, X, y, mu=1):\n    m, n = X.shape\n    return jnp.sum(jnp.logaddexp(0, -y * (X @ w))) / m + mu / 2 * jnp.sum(w**2)\n\ndef generate_problem(m=1000, n=300, mu=1):\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, n_informative=n//2, random_state=0)\n    X = jnp.array(X)\n    y = jnp.array(y)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n    return X_train, y_train, X_test, y_test\n\ndef compute_optimal(X, y, mu):\n    w = cp.Variable(X.shape[1])\n    objective = cp.Minimize(cp.sum(cp.logistic(cp.multiply(-y, X @ w))) / len(y) + mu / 2 * cp.norm(w, 2)**2)\n    problem = cp.Problem(objective)\n    problem.solve()\n    return w.value, problem.value\n\n@jit\ndef compute_accuracy(w, X, y):\n    # Compute predicted probabilities using the logistic (sigmoid) function\n    preds_probs = jax.nn.sigmoid(X @ w)\n    # Convert probabilities to class predictions: -1 if p &lt; 0.5, else 1\n    preds = jnp.where(preds_probs &lt; 0.5, 0, 1)\n    # Calculate accuracy as the average of correct predictions\n    accuracy = jnp.mean(preds == y)\n    return accuracy\n\n\n\n# @jit\ndef compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu):\n    f = lambda w: logistic_loss(w, X_train, y_train, mu)\n    metrics = {\n        \"f_gap\": [jnp.abs(f(x) - f_star) for x in trajectory],\n        \"x_gap\": [jnp.linalg.norm(x - x_star) for x in trajectory],\n        \"time\": times,\n        \"train_acc\": [compute_accuracy(x, X_train, y_train) for x in trajectory],\n        \"test_acc\": [compute_accuracy(x, X_test, y_test) for x in trajectory],\n    }\n    return metrics\n\ndef gradient_descent(w_0, X, y, learning_rate=0.01, num_iters=100, mu=0):\n    trajectory = [w_0]\n    times = [0]\n    w = w_0\n    f = lambda w: logistic_loss(w, X, y, mu)\n    iter_start = time.time()\n    for i in range(num_iters):\n        grad_val = grad(f)(w)\n        if learning_rate == \"linesearch\":\n            # Simple line search implementation\n            phi = lambda alpha: f(w - alpha*grad_val)\n            result = minimize_scalar(fun=phi, \n                                     bounds=(1e-3, 2e2)\n                              )\n            step_size = result.x\n        else:\n            step_size = learning_rate\n        w -= step_size * grad_val\n        iter_time = time.time()\n        trajectory.append(w)\n        times.append(iter_time - iter_start)\n    return trajectory, times\n\ndef run_experiments(params):\n    mu = params[\"mu\"]\n    m, n = params[\"m\"], params[\"n\"]\n    methods = params[\"methods\"]\n    results = {}\n\n    X_train, y_train, X_test, y_test = generate_problem(m, n, mu)\n    n_features = X_train.shape[1]  # Number of features\n    params[\"n_features\"] = n_features\n    \n    x_0 = jax.random.normal(jax.random.PRNGKey(0), (n_features, ))\n    x_star, f_star = compute_optimal(X_train, y_train, mu)\n\n    for method in methods:\n        learning_rate = method[\"learning_rate\"]\n        iterations = method[\"iterations\"]\n        trajectory, times = gradient_descent(x_0, X_train, y_train, learning_rate, iterations, mu)\n        label = method[\"method\"] + \" \" + str(learning_rate)\n        results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n\n    return results, params\n\ndef plot_results(results, params):\n    plt.figure(figsize=(11, 5))\n    mu = params[\"mu\"]\n    \n    if mu &gt; 1e-2:\n        plt.suptitle(f\"Strongly convex binary logistic regression. mu={mu}.\")\n    else:\n        plt.suptitle(f\"Convex binary logistic regression. mu={mu}.\")\n\n    plt.subplot(2, 4, 1)\n    for method, metrics in results.items():\n        plt.plot(metrics['f_gap'])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 2)\n    for method, metrics in results.items():\n        plt.plot(metrics['x_gap'], label=method)\n    plt.xlabel('Iteration')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 3)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"train_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Train accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 4)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"test_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Test accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 5)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['f_gap'])\n    plt.xlabel('Time')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 6)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['x_gap'])\n    plt.xlabel('Time')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 7)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"train_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Train accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 8)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"test_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Test accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    # Place the legend below the plots\n    plt.figlegend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.00))\n    # Adjust layout to make space for the legend below\n    filename = \"\"\n    for method, metrics in results.items():\n        filename += method\n    filename += f\"_{mu}.pdf\"\n    plt.tight_layout(rect=[0, 0.05, 1, 1])\n    plt.savefig(filename)\n    plt.show()\n\n&lt;&gt;:133: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:165: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:133: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:165: SyntaxWarning: invalid escape sequence '\\|'\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_87087/2871042674.py:133: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_87087/2871042674.py:165: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n\n\n\nparams = {\n    \"mu\": 0,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-1,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-1,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\nplot_results(results, params)\n\n\n\n\n\n\n\n\n\nparams = {\n    \"mu\": 1e-1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 1e-1,\n            \"iterations\": 900,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 1.1e-1,\n            \"iterations\": 900,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\nplot_results(results, params)\n\n\n\n\n\n\n\n\nNow we also have convergence in terms of distance to the solution, but it does not influence accuracy.\n\nSupport Vector Machine\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\nimport jax\nfrom jax import numpy as jnp, grad\nfrom scipy.optimize import minimize_scalar\nimport jax.numpy as jnp\nfrom jax import grad, jit, hessian\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\nfrom ucimlrepo import fetch_ucirepo\nfrom optax.losses import safe_softmax_cross_entropy as cros_entr\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize_scalar\nimport sklearn.datasets as skldata\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\nArray([  0, 228], dtype=uint32)\n\n\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\n# Generate a synthetic binary classification dataset\ndef generate_svm_data(m=1000, n=300):\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, \n                                       n_informative=n//2, random_state=42)\n    y = 2 * y - 1  # Convert labels to {-1, 1}\n    return X, y\n\n# Solve SVM using convex optimization\ndef compute_svm_optimal(X, y, C=1.0):\n    m, n = X.shape\n    w = cp.Variable(n)\n    b = cp.Variable()\n    slack = cp.Variable(m)\n\n    # SVM objective: minimize 1/2 ||w||^2 + C * sum(slack)\n    objective = cp.Minimize(0.5 * cp.norm(w, 2)**2 + C * cp.sum(slack))\n\n    # Constraints:  y_i (x_i^T w + b) &gt;= 1 - slack_i,  slack_i &gt;= 0\n    constraints = [\n        cp.multiply(y, X @ w + b) &gt;= 1 - slack,\n        slack &gt;= 0\n    ]\n\n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n\n    return w.value, b.value, problem.value\n\n# Hinge loss function\n@jax.jit\ndef hinge_loss(w, b, X, y, C=1.0):\n    margins = y * (X @ w + b)\n    loss = jnp.maximum(0, 1 - margins)\n    return 0.5 * jnp.sum(w**2) + C * jnp.mean(loss)\n\n# Compute accuracy\n@jax.jit\ndef compute_accuracy(w, b, X, y):\n    preds = jnp.sign(X @ w + b)\n    return jnp.mean(preds == y)\n\n# Perform gradient descent for SVM optimization\ndef gradient_descent_svm(w_0, b_0, X, y, learning_rate=0.01, num_iters=100, C=1.0):\n    trajectory = [w_0]\n    times = [0]\n    w, b = w_0, b_0\n    f = lambda w, b: hinge_loss(w, b, X, y, C)\n    \n    iter_start = time.time()\n    for i in range(num_iters):\n        grad_w, grad_b = grad(f, argnums=(0, 1))(w, b)\n        w -= learning_rate * grad_w\n        b -= learning_rate * grad_b\n\n        iter_time = time.time()\n        trajectory.append(w)\n        times.append(iter_time - iter_start)\n    \n    return trajectory, times\n\n# Run SVM Experiments\ndef run_experiments_svm(params):\n    C = params[\"C\"]\n    m, n = params[\"m\"], params[\"n\"]\n    methods = params[\"methods\"]\n    results = {}\n\n    X, y = generate_svm_data(m, n)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    \n    # Standardize data\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    w_0 = jax.random.normal(jax.random.PRNGKey(0), (n,))\n    b_0 = 0.0\n\n    w_star, b_star, f_star = compute_svm_optimal(X_train, y_train, C)\n\n    for method in methods:\n        learning_rate = method[\"learning_rate\"]\n        iterations = method[\"iterations\"]\n        trajectory, times = gradient_descent_svm(w_0, b_0, X_train, y_train, learning_rate, iterations, C)\n        label = method[\"method\"] + \" \" + str(learning_rate)\n        results[label] = {\n            \"f_gap\": [jnp.abs(hinge_loss(w, b_0, X_train, y_train, C) - f_star) for w in trajectory],\n            \"x_gap\": [jnp.linalg.norm(w - w_star) for w in trajectory],\n            \"time\": times,\n            \"train_acc\": [compute_accuracy(w, b_0, X_train, y_train) for w in trajectory],\n            \"test_acc\": [compute_accuracy(w, b_0, X_test, y_test) for w in trajectory],\n        }\n\n    return results, params\n\n# Plot Results\ndef plot_results_svm(results, params):\n    plt.figure(figsize=(11, 5))\n    C = params[\"C\"]\n    \n    plt.suptitle(f\"SVM Training Results. C={C}\")\n\n    plt.subplot(2, 4, 1)\n    for method, metrics in results.items():\n        plt.plot(metrics['f_gap'])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 2)\n    for method, metrics in results.items():\n        plt.plot(metrics['x_gap'], label=method)\n    plt.xlabel('Iteration')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 3)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"train_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Train accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 4)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"test_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Test accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 5)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['f_gap'])\n    plt.xlabel('Time')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 6)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics['x_gap'])\n    plt.xlabel('Time')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 7)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"train_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Train accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(2, 4, 8)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"time\"], metrics[\"test_acc\"])\n    plt.xlabel('Time')\n    plt.ylabel('Test accuracy')\n    plt.grid(linestyle=\":\")\n\n    plt.figlegend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.00))\n    plt.tight_layout(rect=[0, 0.05, 1, 1])\n    plt.show()\n\n&lt;&gt;:119: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:149: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:119: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:149: SyntaxWarning: invalid escape sequence '\\|'\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_86548/3657982053.py:119: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_86548/3657982053.py:149: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n\n\n\nparams = {\n    \"C\": 0,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-3,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-3,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments_svm(params)\nplot_results_svm(results, params)\n\n\n\n\n\n\n\n\nHere we only minimize \\frac{1}{2} ||w||_2^2 without hinge loss, we have linear convergence both for function error and distance to the solution.\n\nparams = {\n    \"C\": 0.1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 3e-3,\n            \"iterations\": 2000,\n        },\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 7e-3,\n            \"iterations\": 2000,\n        },\n    ]\n}\n\nresults, params = run_experiments_svm(params)\nplot_results_svm(results, params)\n\n\n\n\n\n\n\n\nHere we minimize regularized hinge loss and see that we have accuracy improvement."
  },
  {
    "objectID": "notebooks/s11_quasinewton_cg_gd.html",
    "href": "notebooks/s11_quasinewton_cg_gd.html",
    "title": "",
    "section": "",
    "text": "How can such methods be applied to non-quadratic problems? For example, for binary logistic regression: \nf(x)=\\dfrac{\\mu}{2}\\| x \\|^2_2 + \\dfrac{1}{m}\\sum_{i=1}^{m}log(1+exp(-y_i\\langle a_i, x \\rangle)) \\longrightarrow \\min_{x\\in\\mathbb{R}^n}\n\nWe can use the Fletcher-Reeves or Polyak-Ribier method. Add the iteration in function ConjugateGradientPR() of the Polyak-Ribier method to the code:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.datasets as skldata\nimport jax\nfrom jax import numpy as jnp\nfrom scipy.optimize import minimize_scalar\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('pdf', 'svg')\nplt.rcParams['figure.dpi'] = 300\n\n\n\nnp.random.seed(228)\n\ndef generate_problem(m=1000, n=300, mu=1):\n    np.random.seed(228)\n    # Generating synthetic data\n    n = 300  # Number of features\n    m = 1000  # Number of samples\n\n    # Create a binary classification problem\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, n_informative=n//3, random_state=0)\n    X = jnp.array(X)\n    y = jnp.array(y)\n\n    # Regularized logistic regression cost function\n    @jax.jit\n    def f(w):\n        return jnp.linalg.norm(w)**2*mu/2 +  jnp.mean(jnp.logaddexp(jnp.zeros(X.shape[0]), -y * (X @ w)))\n    \n    grad_f = jax.jit(jax.grad(f))\n    x_0 = jax.random.normal(jax.random.PRNGKey(0), (n,))\n\n    return f, grad_f, x_0\n\n# Optimization methods\ndef gradient_descent(f, grad_f, x_0, step_size, iterations):\n    x = x_0.copy()\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad_f(x)))\n    for _ in range(iterations):\n        x -= step_size * grad_f(x)\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad_f(x)))\n    return values, gradients\n\ndef steepest_descent(f, grad_f, x_0, iterations):\n    x = x_0.copy()\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad_f(x)))\n    for _ in range(iterations):\n        grad = grad_f(x)\n        res = minimize_scalar(lambda alpha: f(x - alpha * grad), bounds = (1e-8,1e1), method='Bounded', options={'maxiter': 50})\n        step_size = res.x\n        x -= step_size * grad\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad))\n    return values, gradients\n\ndef ConjugateGradientFR(f, grad_f, x0, iterations, restart=False):\n    x = x0\n    grad = grad_f(x)\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad_f(x)))\n    d = -grad\n    it = 0\n    while it &lt; iterations:\n        res = minimize_scalar(lambda alpha: f(x + alpha * d), bounds = (1e-9,1e1), method='Bounded', options={'maxiter': 50})\n        alpha = res.x\n        x = x + alpha * d\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad))\n        grad_next = grad_f(x)\n        beta = grad_next.dot(grad_next) / grad.dot(grad)\n        d = -grad_next + beta * d\n        grad = grad_next.copy()\n        it += 1\n        if restart and it % restart == 0:\n            grad = grad_f(x)\n            d = -grad\n        \n    return values, gradients\n\ndef ConjugateGradientPR(f, grad_f, x0, iterations, restart=False):\n    x = x0\n    grad = grad_f(x)\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad))\n    d = -grad\n    it = 0\n    while it &lt; iterations:\n        # Line search for the optimal alpha\n        res = minimize_scalar(lambda alpha: f(x + alpha * d), method='Brent', options={'maxiter': 50})\n        alpha = res.x\n        x += alpha * d\n        grad_next = grad_f(x)\n        grad_diff = grad_next - grad\n        beta = (grad @ grad_diff) / (d @ grad_diff)\n        beta = max(0, beta)\n        d = -grad_next + beta * d\n        grad = grad_next.copy()\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad))\n        it += 1 \n        if restart and it % restart == 0:\n            d = -grad\n    return values, gradients\n\ndef SR1(f, grad_f, x0, iterations):\n    x = x0\n    n = len(x0)\n    C = np.eye(n)  # Initial approximation of inverse Hessian\n    grad = grad_f(x)\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad))\n    \n    for it in range(iterations):\n        # Search direction using inverse\n        d = -C @ grad\n        \n        # Line search\n        res = minimize_scalar(lambda alpha: f(x + alpha * d), bounds=(1e-9, 1e1), \n                            method='Bounded', options={'maxiter': 50})\n        alpha = res.x\n        \n        # Update position\n        x_new = x + alpha * d\n        grad_new = grad_f(x_new)\n        \n        # SR1 inverse update\n        s = alpha * d  # Step\n        y = grad_new - grad  # Gradient difference\n        \n        # Update C only if denominator is not too close to zero\n        v = s - C @ y\n        denominator = v.T @ y\n        if abs(denominator) &gt; 1e-8 * np.linalg.norm(v) * np.linalg.norm(y):\n            C = C + np.outer(v, v) / denominator\n            \n        x = x_new\n        grad = grad_new\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad))\n        \n    return values, gradients\n\ndef BFGS(f, grad_f, x0, iterations):\n    x = x0\n    n = len(x0)\n    H = np.eye(n)  # Initial approximation of inverse Hessian\n    grad = grad_f(x)\n    values, gradients = [], []\n    values.append(f(x))\n    gradients.append(np.linalg.norm(grad))\n    \n    for it in range(iterations):\n        # Search direction\n        d = -H @ grad\n        \n        # Line search\n        res = minimize_scalar(lambda alpha: f(x + alpha * d), bounds=(1e-9, 1e1), \n                            method='Bounded', options={'maxiter': 50})\n        alpha = res.x\n        \n        # Update position\n        x_new = x + alpha * d\n        grad_new = grad_f(x_new)\n        \n        # BFGS update\n        s = alpha * d  # Step\n        y = grad_new - grad  # Gradient difference\n        \n        # Skip update if denominator is too close to zero\n        denominator = y.T @ s\n        # if abs(denominator) &gt; 1e-8 * np.linalg.norm(y) * np.linalg.norm(s):\n        rho = 1.0 / denominator\n        I = np.eye(n)\n        V = I - rho * np.outer(s, y)\n        H = V.T @ H @ V + rho * np.outer(s, s)\n            \n        x = x_new\n        grad = grad_new\n        values.append(f(x))\n        gradients.append(np.linalg.norm(grad))\n        \n    return values, gradients\n\n\ndef run_experiment(params):\n    f, grad_f, x_0 = generate_problem(n=params[\"n\"], m=params[\"m\"], mu=params[\"mu\"])\n\n    if params[\"restart\"] is None:\n        results = {\n            \"methods\": {\n                \"Gradient Descent\": gradient_descent(f, grad_f, x_0, params[\"alpha\"], params[\"iterations\"]),\n                \"Steepest Descent\": steepest_descent(f, grad_f, x_0, params[\"iterations\"]),\n                \"Conjugate Gradients PR\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"]),\n                \"Conjugate Gradients FR\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"]),\n                \"SR1\": SR1(f, grad_f, x_0, params[\"iterations\"]),\n                \"BFGS\": BFGS(f, grad_f, x_0, params[\"iterations\"])\n            },\n            \"problem\":{\n                \"params\": params\n            }\n        }\n    else:\n        results = {\n            \"methods\": {\n                \"Gradient Descent\": gradient_descent(f, grad_f, x_0, params[\"alpha\"], params[\"iterations\"]),\n                \"Steepest Descent\": steepest_descent(f, grad_f, x_0, params[\"iterations\"]),\n                \"Conjugate Gradients PR\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"]),\n                f\"Conjugate Gradients PR. restart {params['restart']}\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"], restart=params[\"restart\"]),\n                \"Conjugate Gradients FR\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"]),\n                f\"Conjugate Gradients FR. restart {params['restart']}\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"], restart=params[\"restart\"]),\n                \"SR1\": SR1(f, grad_f, x_0, params[\"iterations\"]),\n                \"BFGS\": BFGS(f, grad_f, x_0, params[\"iterations\"])\n            },\n            \"problem\":{\n                \"params\": params\n            }\n        }\n    return results\n\n\ndef plot_results(results):\n    linestyles = {\n        \"Gradient Descent\": \"r-\",\n        \"Steepest Descent\": \"b-.\",\n        \"Conjugate Gradients FR\": \"g--\",\n        f\"Conjugate Gradients FR. restart {results['problem']['params']['restart']}\": \"g-\",\n        \"Conjugate Gradients PR\": \"c--\",\n        f\"Conjugate Gradients PR. restart {results['problem']['params']['restart']}\": \"c-\",\n        \"SR1\": \"m--\",\n        \"BFGS\": \"k-\"\n    }\n    plt.figure(figsize=(10, 3.5))\n    m = results[\"problem\"][\"params\"][\"m\"]\n    mu = results[\"problem\"][\"params\"][\"mu\"]\n    n = results[\"problem\"][\"params\"][\"n\"]\n    restart = results[\"problem\"][\"params\"][\"restart\"]\n    \n    plt.suptitle(f\"Regularized binary logistic regression. n={n}. m={m}. μ={mu}\")\n\n    plt.subplot(1, 2, 1)\n    for method, result_  in results[\"methods\"].items():\n        plt.semilogy(result_[0], linestyles[method])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$f(x)$')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(1, 2, 2)\n    for method, result_ in results[\"methods\"].items():\n        plt.semilogy(result_[1], linestyles[method], label=method)\n    plt.ylabel(r'$\\|\\nabla f(x)\\|_2$')\n    plt.xlabel('Iteration')\n    plt.grid(linestyle=\":\")\n\n    # Place the legend below the plots\n    if results['problem']['params']['restart'] == None:\n        plt.figlegend(loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.00))\n        plt.tight_layout(rect=[0, 0.05, 1, 1])\n    else:\n        plt.figlegend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.02))\n        plt.tight_layout(rect=[0, 0.1, 1, 1])\n    # Adjust layout to make space for the legend below\n    # plt.savefig(f\"cg_non_linear_{m}_{n}_{mu}_{restart}.pdf\")\n    plt.show()\n\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_39623/2494207930.py:8: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n  set_matplotlib_formats('pdf', 'svg')\n\n\nRun experiments for different \\mu and dimentions:\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 0,\n    \"alpha\": 1e-1,\n    \"iterations\": 200,\n    \"restart\": None\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n\n\n\n\n\n\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 1,\n    \"alpha\": 3e-2,\n    \"iterations\": 200,\n    \"restart\": None\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n\n\n\n\n\n\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 1,\n    \"alpha\": 3e-2,\n    \"iterations\": 500,\n    \"restart\": 300\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n\n\n\n\n\n\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 10,\n    \"alpha\": 1e-2,\n    \"iterations\": 200,\n    \"restart\": None\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n\n\n\n\n\n\n\n# Experiment parameters\nparams = {\n    \"n\": 300,\n    \"m\": 1000,\n    \"mu\": 10,\n    \"alpha\": 1e-2,\n    \"iterations\": 200,\n    \"restart\": 20\n}\n\nresults = run_experiment(params)\nplot_results(results)"
  },
  {
    "objectID": "notebooks/s10_ex1.html",
    "href": "notebooks/s10_ex1.html",
    "title": "",
    "section": "",
    "text": "f(x)=\\dfrac{1}{2}x^TAx-b^Tx\\longrightarrow \\min_{x\\in\\mathbb{R}^n}\n\nAdd the code for iterations of the conjugate gradient method in the function conjugate_gradient():\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(228)\n\n# Parameters\nn = 100  # Dimension of x\nmu = 10\nL = 100\n\n\ndef generate_problem(n=100, mu=mu, L=L, problem_type=\"clustered\"):\n    np.random.seed(228)\n    if problem_type == \"clustered\":\n        A = np.diagflat([mu*np.ones(n//4), (mu+ (L-mu)/3) * np.ones(n//4), (mu+ 2*(L-mu)/3)*np.ones(n//4), L*np.ones(n//4)])\n        U = np.random.rand(n, n)\n        Q, _ = np.linalg.qr(U)\n        A = Q.dot(A).dot(Q.T)\n        A = (A + A.T) * 0.5\n        x_opt = np.random.rand(n)\n        b = A@x_opt\n        x_0 = 5*np.random.randn(n)\n\n    elif problem_type == \"random\":\n        A = np.random.randn(n, n)\n        factual_L = max(np.linalg.eigvalsh(A.T@A))\n        A = A.T.dot(A)/factual_L*L + mu*np.eye(n)\n        x_opt = np.random.rand(n)\n        b = A@x_opt\n        x_0 = 3*np.random.randn(n)\n    \n    elif problem_type == \"uniform spectrum\":\n        A = np.diag(np.linspace(mu, L, n, endpoint=True))\n        x_opt = np.random.rand(n)\n        b = A@x_opt\n        x_0 = 3*np.random.randn(n)\n\n    elif problem_type == \"Hilbert\":\n        A = np.array([[1.0 / (i+j - 1) for i in range(1, n+1)] for j in range(1, n+1)])\n        b = np.ones(n)\n        x_0 = 3*np.random.randn(n)\n        x_opt = np.linalg.lstsq(A, b)[0]\n\n    elif problem_type == \"worst_cg\":\n        # Parameter t controls the condition number\n        t = 0.6  # Small t leads to worse conditioning\n        # Create tridiagonal matrix W\n        main_diag = np.ones(n)\n        main_diag[0] = t\n        main_diag[1:] = 1 + t\n        off_diag = np.sqrt(t) * np.ones(n-1)\n        A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n        \n        # Create b vector [1, 0, ..., 0]\n        b = np.zeros(n)\n        b[0] = 1\n        \n        # Since this is a specific problem, we compute x_opt explicitly\n        x_opt = np.linalg.solve(A, b)\n        x_0 = np.zeros(n)  # Start from zero vector\n        return A, b, x_0, x_opt\n\n    return A, b, x_0, x_opt\n\n# Optimization methods\ndef gradient_descent(f, grad_f, x_0, step_size, iterations, x_opt):\n    x = x_0.copy()\n    f_opt = f(x_opt)\n    values, gradients = [], []\n    values.append(abs(f(x) - f_opt))\n    gradients.append(np.linalg.norm(grad_f(x)))\n    for _ in range(iterations):\n        x -= step_size * grad_f(x)\n        values.append(abs(f(x) - f_opt))\n        gradients.append(np.linalg.norm(grad_f(x)))\n    return values, gradients\n\ndef steepest_descent(A, f, grad_f, x_0, iterations, x_opt):\n    x = x_0.copy()\n    f_opt = f(x_opt)\n    values, gradients = [], []\n    values.append(abs(f(x) - f_opt))\n    gradients.append(np.linalg.norm(grad_f(x)))\n    for _ in range(iterations):\n        grad = grad_f(x)\n        step_size = np.dot(grad.T, grad) / np.dot(grad.T, np.dot(A, grad))\n        x -= step_size * grad\n        values.append(abs(f(x) - f_opt))\n        gradients.append(np.linalg.norm(grad))\n    return values, gradients\n\ndef conjugate_gradient(A, b, x_0, iterations, x_opt):\n    x = x_0.copy()\n    f = lambda x: 0.5 * x.T @ A @ x - b.T @ x\n    f_opt = f(x_opt)\n\n    r = b - np.dot(A, x)\n    d = r.copy()\n    values, gradients = [f(x) - f_opt], [np.linalg.norm(r)]\n    for _ in range(iterations - 1):\n        ## YOUR CODE HERE ##\n        pass\n    return values, gradients\n\ndef run_experiment(params):\n    A, b, x_0, x_opt = generate_problem(n=params[\"n\"], mu=params[\"mu\"], L=params[\"L\"], problem_type=params[\"problem_type\"])\n    eigs = np.linalg.eigvalsh(A)\n    mu, L = min(eigs), max(eigs)\n\n    f = lambda x: 0.5 * x.T @ A @ x - b.T @ x\n    grad_f = lambda x: A@x - b\n\n    if mu &lt;= 1e-2:\n        alpha = 1/L\n    else:\n        alpha = 2/(mu+L)  # Step size\n    beta = (np.sqrt(L) - np.sqrt(mu))/(np.sqrt(L) + np.sqrt(mu))  # Momentum parameter\n\n    results = {\n        \"methods\": {\n            \"Gradient Descent\": gradient_descent(f, grad_f, x_0, alpha, params[\"iterations\"], x_opt),\n            \"Steepest Descent\": steepest_descent(A, f, grad_f, x_0, params[\"iterations\"], x_opt),\n            \"Conjugate Gradients\": conjugate_gradient(A, b, x_0, params[\"iterations\"], x_opt),\n        },\n        \"problem\":{\n            \"eigs\": eigs,\n            \"params\": params\n        }\n    }\n    return results\n\n\ndef plot_results(results):\n    linestyles = {\n        \"Gradient Descent\": \"r-\",\n        \"Steepest Descent\": \"b-.\",\n        \"Conjugate Gradients\": \"g--\"\n    }\n    plt.figure(figsize=(10, 3.5))\n    mu = results[\"problem\"][\"params\"][\"mu\"]\n    L = results[\"problem\"][\"params\"][\"L\"]\n    n = results[\"problem\"][\"params\"][\"n\"]\n    problem_type = results[\"problem\"][\"params\"][\"problem_type\"]\n    \n    if mu &gt; 1e-2:\n        plt.suptitle(f\"Strongly convex quadratics. n={n}, {problem_type} matrix. \")\n    else:\n        plt.suptitle(f\"Convex quadratics. n={n}, {problem_type} matrix. \")\n\n    plt.subplot(1, 3, 1)\n    eigs = results[\"problem\"][\"eigs\"]\n    plt.scatter(np.arange(len(eigs)), eigs)\n    plt.xlabel('Dimension')\n    plt.ylabel('Eigenvalues of A')\n    plt.grid(linestyle=\":\")\n    plt.title(\"Eigenvalues\")\n    if results[\"problem\"][\"params\"][\"problem_type\"] == \"Hilbert\":\n        plt.yscale(\"log\")\n\n    plt.subplot(1, 3, 2)\n    for method, result_  in results[\"methods\"].items():\n        plt.semilogy(result_[0], linestyles[method])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.grid(linestyle=\":\")\n    plt.title(\"Function gap\")\n\n    plt.subplot(1, 3, 3)\n    for method, result_ in results[\"methods\"].items():\n        plt.semilogy(result_[1], linestyles[method], label=method)\n    plt.ylabel(r'$\\|\\nabla f(x)\\|_2$')\n    plt.grid(linestyle=\":\")\n    plt.title(\"Norm of Gradient\")\n\n    # Place the legend below the plots\n    plt.figlegend(loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.00))\n    # Adjust layout to make space for the legend below\n    plt.tight_layout(rect=[0, 0.05, 1, 1])\n    # plt.savefig(f\"cg_{problem_type}_{mu}_{L}_{n}.pdf\")\n    plt.show()\n\nAfter implementing the conjugate gradient method, let’s look at the several experimental results of solving a quadratic problem for different matrices A:\n\n# Experiment parameters\nparams = {\n    \"n\": 60,\n    \"mu\": 1e-3,\n    \"L\": 100,\n    \"iterations\": 100,\n    \"problem_type\": \"random\",\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 60,\n    \"mu\": 10,\n    \"L\": 100,\n    \"iterations\": 100,\n    \"problem_type\": \"random\",\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 60,\n    \"mu\": 10,\n    \"L\": 1000,\n    \"iterations\": 500,\n    \"problem_type\": \"clustered\",\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 600,\n    \"mu\": 10,\n    \"L\": 1000,\n    \"iterations\": 500,\n    \"problem_type\": \"clustered\",\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 60,\n    \"mu\": 1,\n    \"L\": 10,\n    \"iterations\": 100,\n    \"problem_type\": \"Hilbert\",\n}\n\nresults = run_experiment(params)\nplot_results(results)"
  },
  {
    "objectID": "notebooks/s9_agd_problems.html",
    "href": "notebooks/s9_agd_problems.html",
    "title": "Comparison for quadratics",
    "section": "",
    "text": "Comparison for quadratics\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(228)\n\n# Parameters\nn = 100  # Dimension of x\nmu = 10\nL = 100\n\ndef format_num(x):\n    \"\"\"\n    Formats a number into scientific notation with two decimal places\n    for the significand and removes the extra 0 in the exponent.\n    Example: 1.14e-01 becomes 1.14e-1.\n    \"\"\"\n    s = f\"{x:.2e}\"\n    s = s.replace(\"e+0\", \"e+\").replace(\"e-0\", \"e-\")\n    return\n\ndef generate_problem(n=100, mu=mu, L=L, problem_type=\"clustered\"):\n    np.random.seed(228)\n    if problem_type == \"clustered\":\n        A = np.diagflat([mu * np.ones(n // 4), (mu + (L - mu) / 3) * np.ones(n // 4), \n                         (mu + 2 * (L - mu) / 3) * np.ones(n // 4), L * np.ones(n // 4)])\n        U = np.random.rand(n, n)\n        Q, _ = np.linalg.qr(U)\n        A = Q.dot(A).dot(Q.T)\n        A = (A + A.T) * 0.5\n        x_opt = np.random.rand(n)\n        b = A @ x_opt\n        x_0 = 5 * np.random.randn(n)\n\n    elif problem_type == \"random\":\n        A = np.random.randn(n, n)\n        factual_L = max(np.linalg.eigvalsh(A.T @ A))\n        A = A.T.dot(A) / factual_L * L + mu * np.eye(n)\n        x_opt = np.random.rand(n)\n        b = A @ x_opt\n        x_0 = 3 * np.random.randn(n)\n    \n    elif problem_type == \"uniform spectrum\":\n        A = np.diag(np.linspace(mu, L, n, endpoint=True))\n        x_opt = np.random.rand(n)\n        b = A @ x_opt\n        x_0 = 3 * np.random.randn(n)\n\n    elif problem_type == \"Hilbert\":\n        A = np.array([[1.0 / (i + j - 1) for i in range(1, n+1)] for j in range(1, n+1)])\n        b = np.ones(n)\n        x_0 = 3 * np.random.randn(n)\n        x_opt = np.linalg.lstsq(A, b, rcond=None)[0]\n\n    elif problem_type == \"worst_cg\":\n        t = 0.6  # Small t leads to worse conditioning\n        main_diag = np.ones(n)\n        main_diag[0] = t\n        main_diag[1:] = 1 + t\n        off_diag = np.sqrt(t) * np.ones(n-1)\n        A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n        b = np.zeros(n)\n        b[0] = 1\n        x_opt = np.linalg.solve(A, b)\n        x_0 = np.zeros(n)\n        return A, b, x_0, x_opt\n\n    return A, b, x_0, x_opt\n\n# Optimization methods now return (function_gap, domain_gap, grad_norm, alpha, beta)\n\ndef gradient_descent(f, grad_f, x_0, step_size, iterations, x_opt):\n    x = x_0.copy()\n    f_opt = f(x_opt)\n    function_gap = [abs(f(x) - f_opt)]\n    domain_gap = [np.linalg.norm(x - x_opt)]\n    grad_norms = [np.linalg.norm(grad_f(x))]\n    for _ in range(iterations):\n        x = x - step_size * grad_f(x)\n        function_gap.append(abs(f(x) - f_opt))\n        domain_gap.append(np.linalg.norm(x - x_opt))\n        grad_norms.append(np.linalg.norm(grad_f(x)))\n    return function_gap, domain_gap, grad_norms, step_size, 0\n\ndef heavy_ball(f, grad_f, x_0, iterations, x_opt, mu, L):\n    ### YOUR CODE HERE\n    return\n    return function_gap, domain_gap, grad_norms, alpha, beta\n\ndef nesterov_accelerated_gradient(f, grad_f, x_0, iterations, x_opt, mu, L):\n    ### YOUR CODE HERE\n    return\n    return function_gap, domain_gap, grad_norms, alpha, beta\n\ndef run_experiment(params):\n    A, b, x_0, x_opt = generate_problem(n=params[\"n\"], mu=params[\"mu\"], L=params[\"L\"], \n                                          problem_type=params[\"problem_type\"])\n    eigs = np.linalg.eigvalsh(A)\n    # Update mu and L based on the spectrum\n    mu_val, L_val = min(eigs), max(eigs)\n\n    f = lambda x: 0.5 * x.T @ A @ x - b.T @ x\n    grad_f = lambda x: A @ x - b\n\n    if mu_val &lt;= 1e-2:\n        alpha = 1 / L_val\n    else:\n        alpha = 2 / (mu_val + L_val)\n    beta = (np.sqrt(L_val) - np.sqrt(mu_val)) / (np.sqrt(L_val) + np.sqrt(mu_val))\n\n    results = {\n        \"methods\": {\n            \"Gradient Descent\": gradient_descent(f, grad_f, x_0, alpha, params[\"iterations\"], x_opt),\n            \"Heavy Ball\": heavy_ball(f, grad_f, x_0, params[\"iterations\"], x_opt, mu_val, L_val),\n            \"NAG\": nesterov_accelerated_gradient(f, grad_f, x_0, params[\"iterations\"], x_opt, mu_val, L_val),\n        },\n        \"problem\": {\n            \"eigs\": eigs,\n            \"params\": params\n        }\n    }\n    return results\n\ndef plot_results(results):\n    # Define linestyles for methods\n    linestyles = {\n        \"Gradient Descent\": \"r-\",\n        \"Heavy Ball\": \"g--\",\n        \"NAG\": \"m:\",\n    }\n    params = results[\"problem\"][\"params\"]\n    n = params[\"n\"]\n    problem_type = params[\"problem_type\"]\n    mu_val = params[\"mu\"]\n    L_val = params[\"L\"]\n\n    # Create a figure with 3 subplots for function gap, domain gap, and gradient norm\n    plt.figure(figsize=(10, 3))\n    plt.suptitle(f\"{'Strongly convex' if mu_val &gt; 1e-2 else 'Convex'} quadratics: n={n}, {problem_type} matrix, μ={mu_val}, L={L_val}\")\n\n    # Plot 1: Function gap\n    plt.subplot(1, 3, 1)\n    for method, result in results[\"methods\"].items():\n        # result = (function_gap, domain_gap, grad_norms, alpha, beta)\n        plt.semilogy(result[0], linestyles[method])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) - f^*|$')\n    plt.grid(linestyle=\":\")\n    plt.title(\"Function Gap\")\n\n    # Plot 2: Domain gap (||x - x*||)\n    plt.subplot(1, 3, 2)\n    for method, result in results[\"methods\"].items():\n        plt.semilogy(result[1], linestyles[method])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$\\|x - x^*\\|_2$')\n    plt.grid(linestyle=\":\")\n    plt.title(\"Domain Gap\")\n\n    # Plot 3: Norm of Gradient\n    plt.subplot(1, 3, 3)\n    for method, result in results[\"methods\"].items():\n        if method == \"NAG\":\n            if mu_val &gt; 1e-2:\n                label = f\"{method}. α={result[3]:.2e}, β={result[4]:.2e}\"\n            else:\n                label = f\"{method}\"\n        elif method == \"Heavy Ball\":\n            if mu_val &gt; 1e-2:\n                label = f\"{method}. α={result[3]:.2e}, β={result[4]:.2e}\"\n            else:\n                label = f\"{method}\"\n        elif method == \"Gradient Descent\":\n            label = f\"{method}. α={result[3]:.2e}\"\n        plt.semilogy(result[2], linestyles[method], label=label)\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$\\|\\nabla f(x)\\|_2$')\n    plt.grid(linestyle=\":\")\n    plt.title(\"Gradient Norm\")\n\n    # Place the legend below the plots\n    plt.figlegend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.05))\n    plt.tight_layout()\n    plt.savefig(f\"agd_{problem_type}_{mu_val}_{L_val}_{n}.pdf\", bbox_inches='tight')\n    plt.show()\n\n\n# Experiment parameters\nparams = {\n    \"n\": 60,\n    \"mu\": 0,\n    \"L\": 10,\n    \"iterations\": 800,\n    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 60,\n    \"mu\": 1,\n    \"L\": 10,\n    \"iterations\": 100,\n    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 60,\n    \"mu\": 1,\n    \"L\": 1000,\n    \"iterations\": 1000,\n    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 1000,\n    \"mu\": 1,\n    \"L\": 10,\n    \"iterations\": 100,\n    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n# Experiment parameters\nparams = {\n    \"n\": 1000,\n    \"mu\": 1,\n    \"L\": 1000,\n    \"iterations\": 1000,\n    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n}\n\nresults = run_experiment(params)\nplot_results(results)\n\n\n\nLogreg\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\nimport jax\nfrom jax import numpy as jnp, grad\nfrom scipy.optimize import minimize_scalar\nimport jax.numpy as jnp\nfrom jax import grad, jit, hessian\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\nfrom ucimlrepo import fetch_ucirepo\nfrom optax.losses import safe_softmax_cross_entropy as cros_entr\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.optimize import minimize_scalar\nimport sklearn.datasets as skldata\n\n# Set a random seed for reproducibility\nnp.random.seed(228)\njax.random.PRNGKey(228)\n\n@jit\ndef logistic_loss(w, X, y, mu=1):\n    m, n = X.shape\n    return jnp.sum(jnp.logaddexp(0, -y * (X @ w))) / m + mu / 2 * jnp.sum(w**2)\n\ndef generate_problem(m=1000, n=300, mu=1):\n    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, n_informative=n//2, random_state=0)\n    X = jnp.array(X)\n    y = jnp.array(y)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n    return X_train, y_train, X_test, y_test\n\ndef compute_optimal(X, y, mu):\n    w = cp.Variable(X.shape[1])\n    objective = cp.Minimize(cp.sum(cp.logistic(cp.multiply(-y, X @ w))) / len(y) + mu / 2 * cp.norm(w, 2)**2)\n    problem = cp.Problem(objective)\n    problem.solve()\n    return w.value, problem.value\n\n@jit\ndef compute_accuracy(w, X, y):\n    # Compute predicted probabilities using the logistic (sigmoid) function\n    preds_probs = jax.nn.sigmoid(X @ w)\n    # Convert probabilities to class predictions: -1 if p &lt; 0.5, else 1\n    preds = jnp.where(preds_probs &lt; 0.5, 0, 1)\n    # Calculate accuracy as the average of correct predictions\n    accuracy = jnp.mean(preds == y)\n    return accuracy\n\n\n\n# @jit\ndef compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu):\n    f = lambda w: logistic_loss(w, X_train, y_train, mu)\n    metrics = {\n        \"f_gap\": [jnp.abs(f(x) - f_star) for x in trajectory],\n        \"x_gap\": [jnp.linalg.norm(x - x_star) for x in trajectory],\n        \"time\": times,\n        \"train_acc\": [compute_accuracy(x, X_train, y_train) for x in trajectory],\n        \"test_acc\": [compute_accuracy(x, X_test, y_test) for x in trajectory],\n    }\n    return metrics\n\ndef gradient_descent(w_0, X, y, learning_rate=0.01, num_iters=100, mu=0):\n    trajectory = [w_0]\n    times = [0]\n    w = w_0\n    f = lambda w: logistic_loss(w, X, y, mu)\n    iter_start = time.time()\n    for i in range(num_iters):\n        grad_val = grad(f)(w)\n        if learning_rate == \"linesearch\":\n            # Simple line search implementation\n            phi = lambda alpha: f(w - alpha*grad_val)\n            result = minimize_scalar(fun=phi, \n                                     bounds=(1e-3, 2e2)\n                              )\n            step_size = result.x\n        else:\n            step_size = learning_rate\n        w -= step_size * grad_val\n        iter_time = time.time()\n        trajectory.append(w)\n        times.append(iter_time - iter_start)\n    return trajectory, times\n\ndef heavy_ball_method(w_0, X, y, learning_rate=0.01, momentum=0.9, num_iters=100, mu=0):\n    \"\"\"\n    Polyak Heavy-Ball Method implementation:\n    \n    Given:\n        x^+ = x - α ∇f(x)  (Gradient step)\n        d_k = β_k (x_k - x_{k-1})  (Momentum term)\n    \n    The update is:\n        x_{k+1} = x_k^+ + d_k = x_k - α ∇f(x_k) + d_k\n    \n    Parameters:\n        w_0         : initial point\n        X, y        : data used in the objective function (e.g. logistic loss)\n        learning_rate: α, step size for the gradient step\n        momentum   : β, momentum coefficient (can be constant or adjusted per iteration)\n        num_iters   : number of iterations to run\n        mu          : strong convexity parameter (not used in this explicit update)\n    \n    Returns:\n        trajectory  : list of iterates x_k (after the gradient update)\n        times       : list of times (elapsed) at each iteration\n    \"\"\"\n    ### YOUR CODE HERE        \n    return\n    return trajectory, times\n\ndef nesterov_accelerated_gradient(w_0, X, y, learning_rate=0.01, momentum=0.9, num_iters=100, mu=0):\n    \"\"\"\n    Nesterov Accelerated Gradient (NAG) implementation using explicit momentum:\n    \n    Given:\n        x^+ = x - α ∇f(x)  (Gradient step)\n        d_k = β_k (x_k - x_{k-1})  (Momentum term)\n    \n    The update is:\n        x_{k+1} = (x_k + d_k)^+ = (x_k + d_k) - α ∇f(x_k + d_k)\n    \n    Parameters:\n        w_0         : initial point\n        X, y        : data used in the objective function (e.g. logistic loss)\n        learning_rate: α, step size for the gradient step\n        momentum   : β, momentum coefficient (can be constant or adjusted per iteration)\n        num_iters   : number of iterations to run\n        mu          : strong convexity parameter (not used in this explicit update)\n    \n    Returns:\n        trajectory  : list of iterates x_k (after the gradient update)\n        times       : list of times (elapsed) at each iteration\n    \"\"\"\n    ### YOUR CODE HERE        \n    return\n    return trajectory, times\n\ndef run_experiments(params):\n    mu = params[\"mu\"]\n    m, n = params[\"m\"], params[\"n\"]\n    methods = params[\"methods\"]\n    results = {}\n\n    X_train, y_train, X_test, y_test = generate_problem(m, n, mu)\n    n_features = X_train.shape[1]  # Number of features\n    params[\"n_features\"] = n_features\n    \n    x_0 = jax.random.normal(jax.random.PRNGKey(0), (n_features, ))\n    x_star, f_star = compute_optimal(X_train, y_train, mu)\n\n    for method in methods:\n        if method[\"method\"] == \"GD\":\n            learning_rate = method[\"learning_rate\"]\n            iterations = method[\"iterations\"]\n            trajectory, times = gradient_descent(x_0, X_train, y_train, learning_rate, iterations, mu)\n            label = method[\"method\"] + \" \" + str(learning_rate)\n            results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n        elif method[\"method\"] == \"Heavy Ball\":\n            learning_rate   = method.get(\"learning_rate\", 0.01)\n            momentum        = method.get(\"momentum\", 0.9)\n            iterations = method[\"iterations\"]\n            trajectory, times = heavy_ball_method(x_0, X_train, y_train, learning_rate, momentum, iterations, mu)\n            # label = method[\"method\"] + \" \" + str(learning_rate)\n            label = f\"{method['method']}. α={learning_rate:.2e}. β={momentum:.2e}\"\n            results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n        \n        elif method[\"method\"] == \"NAG\":\n            learning_rate   = method.get(\"learning_rate\", 0.01)\n            momentum        = method.get(\"momentum\", 0.9)\n            iterations = method[\"iterations\"]\n            trajectory, times = nesterov_accelerated_gradient(x_0, X_train, y_train, learning_rate, momentum, iterations, mu)\n            label = method[\"method\"] + \" \" + str(learning_rate)\n            label = f\"{method['method']}. α={learning_rate:.2e}. β={momentum:.2e}\"\n            results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n\n    return results, params\n\ndef plot_results(results, params):\n    plt.figure(figsize=(11, 2.6))\n    mu = params[\"mu\"]\n    \n    if mu &gt; 1e-2:\n        plt.suptitle(f\"Strongly convex binary logistic regression. mu={mu}.\")\n    else:\n        plt.suptitle(f\"Convex binary logistic regression. mu={mu}.\")\n\n    plt.subplot(1, 4, 1)\n    for method, metrics in results.items():\n        plt.plot(metrics['f_gap'])\n    plt.xlabel('Iteration')\n    plt.ylabel(r'$|f(x) -f^*|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(1, 4, 2)\n    for method, metrics in results.items():\n        plt.plot(metrics['x_gap'], label=method)\n    plt.xlabel('Iteration')\n    plt.ylabel('$\\|x_k - x^*\\|$')\n    plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(1, 4, 3)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"train_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Train accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    plt.subplot(1, 4, 4)\n    for method, metrics in results.items():\n        plt.plot(metrics[\"test_acc\"])\n    plt.xlabel('Iteration')\n    plt.ylabel('Test accuracy')\n    # plt.yscale('log')\n    plt.grid(linestyle=\":\")\n\n    # Place the legend below the plots\n    plt.figlegend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.00))\n    # Adjust layout to make space for the legend below\n    filename = \"\"\n    for method, metrics in results.items():\n        filename += method\n    filename += f\"_{mu}.pdf\"\n    plt.tight_layout(rect=[0, 0.05, 1, 1])\n    plt.savefig(filename)\n    plt.show()\n\n&lt;&gt;:340: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:340: SyntaxWarning: invalid escape sequence '\\|'\n/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_99724/1722798055.py:340: SyntaxWarning: invalid escape sequence '\\|'\n  plt.ylabel('$\\|x_k - x^*\\|$')\n\n\n\nparams = {\n    \"mu\": 0,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\":9e4,\n            \"iterations\": 400,\n        },\n        {\n            \"method\": \"Heavy Ball\",\n            \"learning_rate\": 9e-1,\n            \"iterations\": 400,\n            \"momentum\": 0.99,\n        },\n        {\n            \"method\": \"NAG\",\n            \"learning_rate\": 9e-1,\n            \"iterations\": 400,\n            \"momentum\": 0.99,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\nplot_results(results, params)\n\n\nparams = {\n    \"mu\": 1,\n    \"m\": 1000,\n    \"n\": 100,\n    \"methods\": [\n        {\n            \"method\": \"GD\",\n            \"learning_rate\": 5e-2,\n            \"iterations\": 300,\n        },\n        {\n            \"method\": \"Heavy Ball\",\n            \"learning_rate\": 5e-2,\n            \"iterations\": 300,\n            \"momentum\": 0.9,\n        },\n        {\n            \"method\": \"NAG\",\n            \"learning_rate\": 5e-2,\n            \"iterations\": 300,\n            \"momentum\": 0.9,\n        },\n    ]\n}\n\nresults, params = run_experiments(params)\nplot_results(results, params)"
  },
  {
    "objectID": "notebooks/s_17_muon.html",
    "href": "notebooks/s_17_muon.html",
    "title": "",
    "section": "",
    "text": "!pip install git+https://github.com/KellerJordan/Muon\nimport math, torch\nfrom torch import nn, Tensor\nfrom torch.utils.data import DataLoader, TensorDataset\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n\n# ---------- Newton–Schulz orthogonaliser ----------\ndef zeropower_via_newtonschulz5(G: Tensor, steps: int = 5) -&gt; Tensor:\n    a, b, c = 3.4445, -4.7750, 2.0315\n    X = G.to(torch.bfloat16)\n    if X.size(-2) &gt; X.size(-1):\n        X = X.mT\n    X /= X.norm(dim=(-2, -1), keepdim=True).clamp(min=1e-7)\n    for _ in range(steps):\n        A = X @ X.mT\n        X = a * X + (b * A + c * A @ A) @ X\n    return (X.mT if G.size(-2) &gt; X.size(-1) else X).to(G.dtype)\n\n# ---------- single-device Muon ----------\nclass SimpleMuon(torch.optim.Optimizer):\n    def __init__(self, params, lr=0.02, momentum=0.95,\n                 weight_decay=0.01, nesterov=True, ns_steps=5):\n        super().__init__(params, dict(lr=lr, momentum=momentum,\n                                      weight_decay=weight_decay,\n                                      nesterov=nesterov, ns_steps=ns_steps))\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        if closure is not None:\n            with torch.enable_grad():\n                closure()\n        for g in self.param_groups:\n            lr, mom, wd, nest, k = (g[p] for p in\n                                    (\"lr\", \"momentum\", \"weight_decay\",\n                                     \"nesterov\", \"ns_steps\"))\n            for p in g[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.add(p, alpha=wd) if wd else p.grad\n                buf = self.state.setdefault(p, {}).setdefault(\n                    \"momentum_buffer\", torch.zeros_like(p))\n                buf.mul_(mom).add_(grad)\n                d_p = grad.add(buf, alpha=mom) if nest else buf\n                if p.ndim == 4:\n                    flat = d_p.view(p.size(0), -1)\n                    d_p = zeropower_via_newtonschulz5(flat, k).view_as(p)\n                elif p.ndim &gt;= 2:\n                    d_p = zeropower_via_newtonschulz5(d_p, k)\n                p.add_(d_p, alpha=-lr)\n\n# ---------- synthetic binary-classification data ----------\ntorch.manual_seed(42)\nN, D = 5_000, 100\ntrue_w = torch.randn(D)\nX = torch.randn(N, D)\ny = torch.bernoulli(torch.sigmoid(X @ true_w)).float()   # logistic model :contentReference[oaicite:2]{index=2}\nloader = DataLoader(TensorDataset(X, y), batch_size=128, shuffle=True)\n\n# ---------- logistic-regression model ----------\nclass LogReg(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        s = int(math.isqrt(dim))\n        assert s * s == dim\n        self.W = nn.Parameter(torch.randn(s, s) * 0.01)   # 2-D → Muon path :contentReference[oaicite:3]{index=3}\n        self.b = nn.Parameter(torch.zeros(()))\n    def forward(self, x): return torch.sigmoid(x @ self.W.flatten() + self.b)\n\n# ---------- training helper (now takes *list* of optimisers) ----------\ndef train(model, opts, epochs=15):\n    loss_fn = nn.BCELoss()                                # classic but stable :contentReference[oaicite:4]{index=4}\n    for ep in range(1, epochs + 1):\n        for xb, yb in loader:\n            loss = loss_fn(model(xb).squeeze(), yb)\n            for o in opts: o.zero_grad(set_to_none=True)\n            loss.backward()\n            for o in opts: o.step()\n        if ep % 5 == 0:\n            with torch.no_grad():\n                acc = ((model(X).squeeze() &gt; 0.5) == y).float().mean()\n            print(f\"epoch {ep:2d} | loss={loss.item():.4f} | acc={acc:.3f}\")\n\n# ---------- run both experiments ----------\ninit = LogReg(D).state_dict()\n\nprint(\"===&gt; SimpleMuon + AdamW(scalar)\")\nmu_model = LogReg(D); mu_model.load_state_dict(init)\nmu_opt   = SimpleMuon([mu_model.W])\nsc_opt   = torch.optim.AdamW([mu_model.b], lr=3e-4, betas=(0.9, 0.95),\n                             weight_decay=0.01)           # AdamW default :contentReference[oaicite:5]{index=5}\ntrain(mu_model, [mu_opt, sc_opt])\n\nprint(\"\\n===&gt; AdamW only\")\nad_model = LogReg(D); ad_model.load_state_dict(init)\nad_opt   = torch.optim.AdamW(ad_model.parameters(), lr=3e-4, betas=(0.9, 0.95),\n                             weight_decay=0.01)\ntrain(ad_model, [ad_opt])\n\n===&gt; SimpleMuon + AdamW(scalar)\nepoch  5 | loss=0.0364 | acc=0.946\nepoch 10 | loss=0.1867 | acc=0.943\nepoch 15 | loss=0.2646 | acc=0.941\n\n===&gt; AdamW only\nepoch  5 | loss=0.6231 | acc=0.874\nepoch 10 | loss=0.5356 | acc=0.903\nepoch 15 | loss=0.4566 | acc=0.919"
  }
]