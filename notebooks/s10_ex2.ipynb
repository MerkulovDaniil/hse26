{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: CG method for any convex or strongly convex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can such methods be applied to non-quadratic problems? For example, for binary logistic regression:\n",
    "$$\n",
    "f(x)=\\dfrac{\\mu}{2}\\| x \\|^2_2 + \\dfrac{1}{m}\\sum_{i=1}^{m}log(1+exp(-y_i\\langle a_i, x \\rangle)) \\longrightarrow \\min_{x\\in\\mathbb{R}^n}\n",
    "$$\n",
    "\n",
    "We can use the Fletcher-Reeves or Polak-Ribier method. Add the iteration in function `ConjugateGradientPR()` of the Polak-Ribier method to the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as skldata\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "np.random.seed(228)\n",
    "\n",
    "def generate_problem(m=1000, n=300, mu=1):\n",
    "    np.random.seed(228)\n",
    "    # Generating synthetic data\n",
    "    n = 300  # Number of features\n",
    "    m = 1000  # Number of samples\n",
    "\n",
    "    # Create a binary classification problem\n",
    "    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, n_informative=n//3, random_state=0)\n",
    "    X = jnp.array(X)\n",
    "    y = jnp.array(y)\n",
    "\n",
    "    # Regularized logistic regression cost function\n",
    "    @jax.jit\n",
    "    def f(w):\n",
    "        return jnp.linalg.norm(w)**2*mu/2 +  jnp.mean(jnp.logaddexp(jnp.zeros(X.shape[0]), -y * (X @ w)))\n",
    "    \n",
    "    grad_f = jax.jit(jax.grad(f))\n",
    "    x_0 = jax.random.normal(jax.random.PRNGKey(0), (n,))\n",
    "\n",
    "    return f, grad_f, x_0\n",
    "\n",
    "# Optimization methods\n",
    "def gradient_descent(f, grad_f, x_0, step_size, iterations):\n",
    "    x = x_0.copy()\n",
    "    values, gradients = [], []\n",
    "    values.append(f(x))\n",
    "    gradients.append(np.linalg.norm(grad_f(x)))\n",
    "    for _ in range(iterations):\n",
    "        x -= step_size * grad_f(x)\n",
    "        values.append(f(x))\n",
    "        gradients.append(np.linalg.norm(grad_f(x)))\n",
    "    return values, gradients\n",
    "\n",
    "def steepest_descent(f, grad_f, x_0, iterations):\n",
    "    x = x_0.copy()\n",
    "    values, gradients = [], []\n",
    "    values.append(f(x))\n",
    "    gradients.append(np.linalg.norm(grad_f(x)))\n",
    "    for _ in range(iterations):\n",
    "        grad = grad_f(x)\n",
    "        res = minimize_scalar(lambda alpha: f(x - alpha * grad), bounds = (1e-8,1e1), method='Bounded', options={'maxiter': 50})\n",
    "        step_size = res.x\n",
    "        x -= step_size * grad\n",
    "        values.append(f(x))\n",
    "        gradients.append(np.linalg.norm(grad))\n",
    "    return values, gradients\n",
    "\n",
    "def ConjugateGradientFR(f, grad_f, x0, iterations, restart=False):\n",
    "    x = x0\n",
    "    grad = grad_f(x)\n",
    "    values, gradients = [], []\n",
    "    values.append(f(x))\n",
    "    gradients.append(np.linalg.norm(grad_f(x)))\n",
    "    d = -grad\n",
    "    it = 0\n",
    "    while it < iterations:\n",
    "        res = minimize_scalar(lambda alpha: f(x + alpha * d), bounds = (1e-9,1e1), method='Bounded', options={'maxiter': 50})\n",
    "        alpha = res.x\n",
    "        x = x + alpha * d\n",
    "        values.append(f(x))\n",
    "        gradients.append(np.linalg.norm(grad))\n",
    "        grad_next = grad_f(x)\n",
    "        beta = grad_next.dot(grad_next) / grad.dot(grad)\n",
    "        d = -grad_next + beta * d\n",
    "        grad = grad_next.copy()\n",
    "        it += 1\n",
    "        if restart and it % restart == 0:\n",
    "            grad = grad_f(x)\n",
    "            d = -grad\n",
    "        \n",
    "    return values, gradients\n",
    "\n",
    "def ConjugateGradientPR(f, grad_f, x0, iterations, restart=False):\n",
    "    x = x0\n",
    "    grad = grad_f(x)\n",
    "    values, gradients = [], []\n",
    "    values.append(f(x))\n",
    "    gradients.append(np.linalg.norm(grad))\n",
    "    d = -grad\n",
    "    it = 0\n",
    "    while it < iterations:\n",
    "        # Line search for the optimal alpha\n",
    "        pass \n",
    "        ## YOUR CODE HERE ##\n",
    "        ## HINT: use scipy.minimize_scalar() ##\n",
    "\n",
    "        # Calculate beta using Polak-Ribière formula\n",
    "        pass\n",
    "        ## YOUR CODE HERE ##\n",
    "        ## HINT: use lecture notes ##\n",
    "        \n",
    "    return values, gradients\n",
    "\n",
    "\n",
    "def run_experiment(params):\n",
    "    f, grad_f, x_0 = generate_problem(n=params[\"n\"], m=params[\"m\"], mu=params[\"mu\"])\n",
    "\n",
    "    if params[\"restart\"] is None:\n",
    "        results = {\n",
    "            \"methods\": {\n",
    "                \"Gradient Descent\": gradient_descent(f, grad_f, x_0, params[\"alpha\"], params[\"iterations\"]),\n",
    "                \"Steepest Descent\": steepest_descent(f, grad_f, x_0, params[\"iterations\"]),\n",
    "                \"Conjugate Gradients PR\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"]),\n",
    "                \"Conjugate Gradients FR\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"]),\n",
    "            },\n",
    "            \"problem\":{\n",
    "                \"params\": params\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        results = {\n",
    "            \"methods\": {\n",
    "                \"Gradient Descent\": gradient_descent(f, grad_f, x_0, params[\"alpha\"], params[\"iterations\"]),\n",
    "                \"Steepest Descent\": steepest_descent(f, grad_f, x_0, params[\"iterations\"]),\n",
    "                \"Conjugate Gradients PR\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"]),\n",
    "                f\"Conjugate Gradients PR. restart {params['restart']}\": ConjugateGradientPR(f, grad_f, x_0, params[\"iterations\"], restart=params[\"restart\"]),\n",
    "                \"Conjugate Gradients FR\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"]),\n",
    "                f\"Conjugate Gradients FR. restart {params['restart']}\": ConjugateGradientFR(f, grad_f, x_0, params[\"iterations\"], restart=params[\"restart\"]),\n",
    "            },\n",
    "            \"problem\":{\n",
    "                \"params\": params\n",
    "            }\n",
    "        }\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    linestyles = {\n",
    "        \"Gradient Descent\": \"r-\",\n",
    "        \"Steepest Descent\": \"b-.\",\n",
    "        \"Conjugate Gradients FR\": \"g--\",\n",
    "        f\"Conjugate Gradients FR. restart {results['problem']['params']['restart']}\": \"g-\",\n",
    "        \"Conjugate Gradients PR\": \"c--\",\n",
    "        f\"Conjugate Gradients PR. restart {results['problem']['params']['restart']}\": \"c-\",\n",
    "    }\n",
    "    plt.figure(figsize=(10, 3.5))\n",
    "    m = results[\"problem\"][\"params\"][\"m\"]\n",
    "    mu = results[\"problem\"][\"params\"][\"mu\"]\n",
    "    n = results[\"problem\"][\"params\"][\"n\"]\n",
    "    restart = results[\"problem\"][\"params\"][\"restart\"]\n",
    "    \n",
    "    plt.suptitle(f\"Regularized binary logistic regression. n={n}. m={m}. μ={mu}\")\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for method, result_  in results[\"methods\"].items():\n",
    "        plt.semilogy(result_[0], linestyles[method])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel(r'$f(x)$')\n",
    "    plt.grid(linestyle=\":\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for method, result_ in results[\"methods\"].items():\n",
    "        plt.semilogy(result_[1], linestyles[method], label=method)\n",
    "    plt.ylabel(r'$\\|\\nabla f(x)\\|_2$')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.grid(linestyle=\":\")\n",
    "\n",
    "    # Place the legend below the plots\n",
    "    if results['problem']['params']['restart'] == None:\n",
    "        plt.figlegend(loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.00))\n",
    "        plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    else:\n",
    "        plt.figlegend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.02))\n",
    "        plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
    "    # Adjust layout to make space for the legend below\n",
    "    # plt.savefig(f\"cg_non_linear_{m}_{n}_{mu}_{restart}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiments for different $\\mu$ and dimentions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 300,\n",
    "    \"m\": 1000,\n",
    "    \"mu\": 0,\n",
    "    \"alpha\": 1e-1,\n",
    "    \"iterations\": 200,\n",
    "    \"restart\": None\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 300,\n",
    "    \"m\": 1000,\n",
    "    \"mu\": 1,\n",
    "    \"alpha\": 3e-2,\n",
    "    \"iterations\": 200,\n",
    "    \"restart\": None\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 300,\n",
    "    \"m\": 1000,\n",
    "    \"mu\": 1,\n",
    "    \"alpha\": 3e-2,\n",
    "    \"iterations\": 200,\n",
    "    \"restart\": 20\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 300,\n",
    "    \"m\": 1000,\n",
    "    \"mu\": 10,\n",
    "    \"alpha\": 1e-2,\n",
    "    \"iterations\": 200,\n",
    "    \"restart\": None\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 300,\n",
    "    \"m\": 1000,\n",
    "    \"mu\": 10,\n",
    "    \"alpha\": 1e-2,\n",
    "    \"iterations\": 200,\n",
    "    \"restart\": 20\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hse25_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
