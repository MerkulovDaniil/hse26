{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/KellerJordan/Muon\n",
    "import math, torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> SimpleMuon + AdamW(scalar)\n",
      "epoch  5 | loss=0.0364 | acc=0.946\n",
      "epoch 10 | loss=0.1867 | acc=0.943\n",
      "epoch 15 | loss=0.2646 | acc=0.941\n",
      "\n",
      "===> AdamW only\n",
      "epoch  5 | loss=0.6231 | acc=0.874\n",
      "epoch 10 | loss=0.5356 | acc=0.903\n",
      "epoch 15 | loss=0.4566 | acc=0.919\n"
     ]
    }
   ],
   "source": [
    "# ---------- Newton–Schulz orthogonaliser ----------\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int = 5) -> Tensor:\n",
    "    a, b, c = 3.4445, -4.7750, 2.0315\n",
    "    X = G.to(torch.bfloat16)\n",
    "    if X.size(-2) > X.size(-1):\n",
    "        X = X.mT\n",
    "    X /= X.norm(dim=(-2, -1), keepdim=True).clamp(min=1e-7)\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        X = a * X + (b * A + c * A @ A) @ X\n",
    "    return (X.mT if G.size(-2) > X.size(-1) else X).to(G.dtype)\n",
    "\n",
    "# ---------- single-device Muon ----------\n",
    "class SimpleMuon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.02, momentum=0.95,\n",
    "                 weight_decay=0.01, nesterov=True, ns_steps=5):\n",
    "        super().__init__(params, dict(lr=lr, momentum=momentum,\n",
    "                                      weight_decay=weight_decay,\n",
    "                                      nesterov=nesterov, ns_steps=ns_steps))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                closure()\n",
    "        for g in self.param_groups:\n",
    "            lr, mom, wd, nest, k = (g[p] for p in\n",
    "                                    (\"lr\", \"momentum\", \"weight_decay\",\n",
    "                                     \"nesterov\", \"ns_steps\"))\n",
    "            for p in g[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.add(p, alpha=wd) if wd else p.grad\n",
    "                buf = self.state.setdefault(p, {}).setdefault(\n",
    "                    \"momentum_buffer\", torch.zeros_like(p))\n",
    "                buf.mul_(mom).add_(grad)\n",
    "                d_p = grad.add(buf, alpha=mom) if nest else buf\n",
    "                if p.ndim == 4:\n",
    "                    flat = d_p.view(p.size(0), -1)\n",
    "                    d_p = zeropower_via_newtonschulz5(flat, k).view_as(p)\n",
    "                elif p.ndim >= 2:\n",
    "                    d_p = zeropower_via_newtonschulz5(d_p, k)\n",
    "                p.add_(d_p, alpha=-lr)\n",
    "\n",
    "# ---------- synthetic binary-classification data ----------\n",
    "torch.manual_seed(42)\n",
    "N, D = 5_000, 100\n",
    "true_w = torch.randn(D)\n",
    "X = torch.randn(N, D)\n",
    "y = torch.bernoulli(torch.sigmoid(X @ true_w)).float()   # logistic model :contentReference[oaicite:2]{index=2}\n",
    "loader = DataLoader(TensorDataset(X, y), batch_size=128, shuffle=True)\n",
    "\n",
    "# ---------- logistic-regression model ----------\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        s = int(math.isqrt(dim))\n",
    "        assert s * s == dim\n",
    "        self.W = nn.Parameter(torch.randn(s, s) * 0.01)   # 2-D → Muon path :contentReference[oaicite:3]{index=3}\n",
    "        self.b = nn.Parameter(torch.zeros(()))\n",
    "    def forward(self, x): return torch.sigmoid(x @ self.W.flatten() + self.b)\n",
    "\n",
    "# ---------- training helper (now takes *list* of optimisers) ----------\n",
    "def train(model, opts, epochs=15):\n",
    "    loss_fn = nn.BCELoss()                                # classic but stable :contentReference[oaicite:4]{index=4}\n",
    "    for ep in range(1, epochs + 1):\n",
    "        for xb, yb in loader:\n",
    "            loss = loss_fn(model(xb).squeeze(), yb)\n",
    "            for o in opts: o.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            for o in opts: o.step()\n",
    "        if ep % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                acc = ((model(X).squeeze() > 0.5) == y).float().mean()\n",
    "            print(f\"epoch {ep:2d} | loss={loss.item():.4f} | acc={acc:.3f}\")\n",
    "\n",
    "# ---------- run both experiments ----------\n",
    "init = LogReg(D).state_dict()\n",
    "\n",
    "print(\"===> SimpleMuon + AdamW(scalar)\")\n",
    "mu_model = LogReg(D); mu_model.load_state_dict(init)\n",
    "mu_opt   = SimpleMuon([mu_model.W])\n",
    "sc_opt   = torch.optim.AdamW([mu_model.b], lr=3e-4, betas=(0.9, 0.95),\n",
    "                             weight_decay=0.01)           # AdamW default :contentReference[oaicite:5]{index=5}\n",
    "train(mu_model, [mu_opt, sc_opt])\n",
    "\n",
    "print(\"\\n===> AdamW only\")\n",
    "ad_model = LogReg(D); ad_model.load_state_dict(init)\n",
    "ad_opt   = torch.optim.AdamW(ad_model.parameters(), lr=3e-4, betas=(0.9, 0.95),\n",
    "                             weight_decay=0.01)\n",
    "train(ad_model, [ad_opt])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hse25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
