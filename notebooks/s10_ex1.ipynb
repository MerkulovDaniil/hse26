{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: CG method for different quadratic problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x)=\\dfrac{1}{2}x^TAx-b^Tx\\longrightarrow \\min_{x\\in\\mathbb{R}^n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the code for iterations of the conjugate gradient method in the function `conjugate_gradient()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(228)\n",
    "\n",
    "# Parameters\n",
    "n = 100  # Dimension of x\n",
    "mu = 10\n",
    "L = 100\n",
    "\n",
    "\n",
    "def generate_problem(n=100, mu=mu, L=L, problem_type=\"clustered\"):\n",
    "    np.random.seed(228)\n",
    "    if problem_type == \"clustered\":\n",
    "        A = np.diagflat([mu*np.ones(n//4), (mu+ (L-mu)/3) * np.ones(n//4), (mu+ 2*(L-mu)/3)*np.ones(n//4), L*np.ones(n//4)])\n",
    "        U = np.random.rand(n, n)\n",
    "        Q, _ = np.linalg.qr(U)\n",
    "        A = Q.dot(A).dot(Q.T)\n",
    "        A = (A + A.T) * 0.5\n",
    "        x_opt = np.random.rand(n)\n",
    "        b = A@x_opt\n",
    "        x_0 = 5*np.random.randn(n)\n",
    "\n",
    "    elif problem_type == \"random\":\n",
    "        A = np.random.randn(n, n)\n",
    "        factual_L = max(np.linalg.eigvalsh(A.T@A))\n",
    "        A = A.T.dot(A)/factual_L*L + mu*np.eye(n)\n",
    "        x_opt = np.random.rand(n)\n",
    "        b = A@x_opt\n",
    "        x_0 = 3*np.random.randn(n)\n",
    "    \n",
    "    elif problem_type == \"uniform spectrum\":\n",
    "        A = np.diag(np.linspace(mu, L, n, endpoint=True))\n",
    "        x_opt = np.random.rand(n)\n",
    "        b = A@x_opt\n",
    "        x_0 = 3*np.random.randn(n)\n",
    "\n",
    "    elif problem_type == \"Hilbert\":\n",
    "        A = np.array([[1.0 / (i+j - 1) for i in range(1, n+1)] for j in range(1, n+1)])\n",
    "        b = np.ones(n)\n",
    "        x_0 = 3*np.random.randn(n)\n",
    "        x_opt = np.linalg.lstsq(A, b)[0]\n",
    "\n",
    "    elif problem_type == \"worst_cg\":\n",
    "        # Parameter t controls the condition number\n",
    "        t = 0.6  # Small t leads to worse conditioning\n",
    "        # Create tridiagonal matrix W\n",
    "        main_diag = np.ones(n)\n",
    "        main_diag[0] = t\n",
    "        main_diag[1:] = 1 + t\n",
    "        off_diag = np.sqrt(t) * np.ones(n-1)\n",
    "        A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n",
    "        \n",
    "        # Create b vector [1, 0, ..., 0]\n",
    "        b = np.zeros(n)\n",
    "        b[0] = 1\n",
    "        \n",
    "        # Since this is a specific problem, we compute x_opt explicitly\n",
    "        x_opt = np.linalg.solve(A, b)\n",
    "        x_0 = np.zeros(n)  # Start from zero vector\n",
    "        return A, b, x_0, x_opt\n",
    "\n",
    "    return A, b, x_0, x_opt\n",
    "\n",
    "# Optimization methods\n",
    "def gradient_descent(f, grad_f, x_0, step_size, iterations, x_opt):\n",
    "    x = x_0.copy()\n",
    "    f_opt = f(x_opt)\n",
    "    values, gradients = [], []\n",
    "    values.append(abs(f(x) - f_opt))\n",
    "    gradients.append(np.linalg.norm(grad_f(x)))\n",
    "    for _ in range(iterations):\n",
    "        x -= step_size * grad_f(x)\n",
    "        values.append(abs(f(x) - f_opt))\n",
    "        gradients.append(np.linalg.norm(grad_f(x)))\n",
    "    return values, gradients\n",
    "\n",
    "def steepest_descent(A, f, grad_f, x_0, iterations, x_opt):\n",
    "    x = x_0.copy()\n",
    "    f_opt = f(x_opt)\n",
    "    values, gradients = [], []\n",
    "    values.append(abs(f(x) - f_opt))\n",
    "    gradients.append(np.linalg.norm(grad_f(x)))\n",
    "    for _ in range(iterations):\n",
    "        grad = grad_f(x)\n",
    "        step_size = np.dot(grad.T, grad) / np.dot(grad.T, np.dot(A, grad))\n",
    "        x -= step_size * grad\n",
    "        values.append(abs(f(x) - f_opt))\n",
    "        gradients.append(np.linalg.norm(grad))\n",
    "    return values, gradients\n",
    "\n",
    "def conjugate_gradient(A, b, x_0, iterations, x_opt):\n",
    "    x = x_0.copy()\n",
    "    f = lambda x: 0.5 * x.T @ A @ x - b.T @ x\n",
    "    f_opt = f(x_opt)\n",
    "\n",
    "    r = b - np.dot(A, x)\n",
    "    d = r.copy()\n",
    "    values, gradients = [f(x) - f_opt], [np.linalg.norm(r)]\n",
    "    for _ in range(iterations - 1):\n",
    "        ## YOUR CODE HERE ##\n",
    "        pass\n",
    "    return values, gradients\n",
    "\n",
    "def run_experiment(params):\n",
    "    A, b, x_0, x_opt = generate_problem(n=params[\"n\"], mu=params[\"mu\"], L=params[\"L\"], problem_type=params[\"problem_type\"])\n",
    "    eigs = np.linalg.eigvalsh(A)\n",
    "    mu, L = min(eigs), max(eigs)\n",
    "\n",
    "    f = lambda x: 0.5 * x.T @ A @ x - b.T @ x\n",
    "    grad_f = lambda x: A@x - b\n",
    "\n",
    "    if mu <= 1e-2:\n",
    "        alpha = 1/L\n",
    "    else:\n",
    "        alpha = 2/(mu+L)  # Step size\n",
    "    beta = (np.sqrt(L) - np.sqrt(mu))/(np.sqrt(L) + np.sqrt(mu))  # Momentum parameter\n",
    "\n",
    "    results = {\n",
    "        \"methods\": {\n",
    "            \"Gradient Descent\": gradient_descent(f, grad_f, x_0, alpha, params[\"iterations\"], x_opt),\n",
    "            \"Steepest Descent\": steepest_descent(A, f, grad_f, x_0, params[\"iterations\"], x_opt),\n",
    "            \"Conjugate Gradients\": conjugate_gradient(A, b, x_0, params[\"iterations\"], x_opt),\n",
    "        },\n",
    "        \"problem\":{\n",
    "            \"eigs\": eigs,\n",
    "            \"params\": params\n",
    "        }\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_results(results):\n",
    "    linestyles = {\n",
    "        \"Gradient Descent\": \"r-\",\n",
    "        \"Steepest Descent\": \"b-.\",\n",
    "        \"Conjugate Gradients\": \"g--\"\n",
    "    }\n",
    "    plt.figure(figsize=(10, 3.5))\n",
    "    mu = results[\"problem\"][\"params\"][\"mu\"]\n",
    "    L = results[\"problem\"][\"params\"][\"L\"]\n",
    "    n = results[\"problem\"][\"params\"][\"n\"]\n",
    "    problem_type = results[\"problem\"][\"params\"][\"problem_type\"]\n",
    "    \n",
    "    if mu > 1e-2:\n",
    "        plt.suptitle(f\"Strongly convex quadratics. n={n}, {problem_type} matrix. \")\n",
    "    else:\n",
    "        plt.suptitle(f\"Convex quadratics. n={n}, {problem_type} matrix. \")\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    eigs = results[\"problem\"][\"eigs\"]\n",
    "    plt.scatter(np.arange(len(eigs)), eigs)\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Eigenvalues of A')\n",
    "    plt.grid(linestyle=\":\")\n",
    "    plt.title(\"Eigenvalues\")\n",
    "    if results[\"problem\"][\"params\"][\"problem_type\"] == \"Hilbert\":\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for method, result_  in results[\"methods\"].items():\n",
    "        plt.semilogy(result_[0], linestyles[method])\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel(r'$|f(x) -f^*|$')\n",
    "    plt.grid(linestyle=\":\")\n",
    "    plt.title(\"Function gap\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    for method, result_ in results[\"methods\"].items():\n",
    "        plt.semilogy(result_[1], linestyles[method], label=method)\n",
    "    plt.ylabel(r'$\\|\\nabla f(x)\\|_2$')\n",
    "    plt.grid(linestyle=\":\")\n",
    "    plt.title(\"Norm of Gradient\")\n",
    "\n",
    "    # Place the legend below the plots\n",
    "    plt.figlegend(loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.00))\n",
    "    # Adjust layout to make space for the legend below\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    # plt.savefig(f\"cg_{problem_type}_{mu}_{L}_{n}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the conjugate gradient method, let's look at the several experimental results of solving a quadratic problem for different matrices $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 60,\n",
    "    \"mu\": 1e-3,\n",
    "    \"L\": 100,\n",
    "    \"iterations\": 100,\n",
    "    \"problem_type\": \"random\",\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 60,\n",
    "    \"mu\": 10,\n",
    "    \"L\": 100,\n",
    "    \"iterations\": 100,\n",
    "    \"problem_type\": \"random\",\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 60,\n",
    "    \"mu\": 10,\n",
    "    \"L\": 1000,\n",
    "    \"iterations\": 500,\n",
    "    \"problem_type\": \"clustered\",\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 600,\n",
    "    \"mu\": 10,\n",
    "    \"L\": 1000,\n",
    "    \"iterations\": 500,\n",
    "    \"problem_type\": \"clustered\",\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "params = {\n",
    "    \"n\": 60,\n",
    "    \"mu\": 1,\n",
    "    \"L\": 10,\n",
    "    \"iterations\": 100,\n",
    "    \"problem_type\": \"Hilbert\",\n",
    "}\n",
    "\n",
    "results = run_experiment(params)\n",
    "plot_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hse25_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
