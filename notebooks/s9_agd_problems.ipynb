{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem illustration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison for quadratics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(228)\n",
        "\n",
        "# Parameters\n",
        "n = 100  # Dimension of x\n",
        "mu = 10\n",
        "L = 100\n",
        "\n",
        "def format_num(x):\n",
        "    \"\"\"\n",
        "    Formats a number into scientific notation with two decimal places\n",
        "    for the significand and removes the extra 0 in the exponent.\n",
        "    Example: 1.14e-01 becomes 1.14e-1.\n",
        "    \"\"\"\n",
        "    s = f\"{x:.2e}\"\n",
        "    s = s.replace(\"e+0\", \"e+\").replace(\"e-0\", \"e-\")\n",
        "    return\n",
        "\n",
        "def generate_problem(n=100, mu=mu, L=L, problem_type=\"clustered\"):\n",
        "    np.random.seed(228)\n",
        "    if problem_type == \"clustered\":\n",
        "        A = np.diagflat([mu * np.ones(n // 4), (mu + (L - mu) / 3) * np.ones(n // 4), \n",
        "                         (mu + 2 * (L - mu) / 3) * np.ones(n // 4), L * np.ones(n // 4)])\n",
        "        U = np.random.rand(n, n)\n",
        "        Q, _ = np.linalg.qr(U)\n",
        "        A = Q.dot(A).dot(Q.T)\n",
        "        A = (A + A.T) * 0.5\n",
        "        x_opt = np.random.rand(n)\n",
        "        b = A @ x_opt\n",
        "        x_0 = 5 * np.random.randn(n)\n",
        "\n",
        "    elif problem_type == \"random\":\n",
        "        A = np.random.randn(n, n)\n",
        "        factual_L = max(np.linalg.eigvalsh(A.T @ A))\n",
        "        A = A.T.dot(A) / factual_L * L + mu * np.eye(n)\n",
        "        x_opt = np.random.rand(n)\n",
        "        b = A @ x_opt\n",
        "        x_0 = 3 * np.random.randn(n)\n",
        "    \n",
        "    elif problem_type == \"uniform spectrum\":\n",
        "        A = np.diag(np.linspace(mu, L, n, endpoint=True))\n",
        "        x_opt = np.random.rand(n)\n",
        "        b = A @ x_opt\n",
        "        x_0 = 3 * np.random.randn(n)\n",
        "\n",
        "    elif problem_type == \"Hilbert\":\n",
        "        A = np.array([[1.0 / (i + j - 1) for i in range(1, n+1)] for j in range(1, n+1)])\n",
        "        b = np.ones(n)\n",
        "        x_0 = 3 * np.random.randn(n)\n",
        "        x_opt = np.linalg.lstsq(A, b, rcond=None)[0]\n",
        "\n",
        "    elif problem_type == \"worst_cg\":\n",
        "        t = 0.6  # Small t leads to worse conditioning\n",
        "        main_diag = np.ones(n)\n",
        "        main_diag[0] = t\n",
        "        main_diag[1:] = 1 + t\n",
        "        off_diag = np.sqrt(t) * np.ones(n-1)\n",
        "        A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n",
        "        b = np.zeros(n)\n",
        "        b[0] = 1\n",
        "        x_opt = np.linalg.solve(A, b)\n",
        "        x_0 = np.zeros(n)\n",
        "        return A, b, x_0, x_opt\n",
        "\n",
        "    return A, b, x_0, x_opt\n",
        "\n",
        "# Optimization methods now return (function_gap, domain_gap, grad_norm, alpha, beta)\n",
        "\n",
        "def gradient_descent(f, grad_f, x_0, step_size, iterations, x_opt):\n",
        "    x = x_0.copy()\n",
        "    f_opt = f(x_opt)\n",
        "    function_gap = [abs(f(x) - f_opt)]\n",
        "    domain_gap = [np.linalg.norm(x - x_opt)]\n",
        "    grad_norms = [np.linalg.norm(grad_f(x))]\n",
        "    for _ in range(iterations):\n",
        "        x = x - step_size * grad_f(x)\n",
        "        function_gap.append(abs(f(x) - f_opt))\n",
        "        domain_gap.append(np.linalg.norm(x - x_opt))\n",
        "        grad_norms.append(np.linalg.norm(grad_f(x)))\n",
        "    return function_gap, domain_gap, grad_norms, step_size, 0\n",
        "\n",
        "def heavy_ball(f, grad_f, x_0, iterations, x_opt, mu, L):\n",
        "    ### YOUR CODE HERE\n",
        "    return\n",
        "    return function_gap, domain_gap, grad_norms, alpha, beta\n",
        "\n",
        "def nesterov_accelerated_gradient(f, grad_f, x_0, iterations, x_opt, mu, L):\n",
        "    ### YOUR CODE HERE\n",
        "    return\n",
        "    return function_gap, domain_gap, grad_norms, alpha, beta\n",
        "\n",
        "def run_experiment(params):\n",
        "    A, b, x_0, x_opt = generate_problem(n=params[\"n\"], mu=params[\"mu\"], L=params[\"L\"], \n",
        "                                          problem_type=params[\"problem_type\"])\n",
        "    eigs = np.linalg.eigvalsh(A)\n",
        "    # Update mu and L based on the spectrum\n",
        "    mu_val, L_val = min(eigs), max(eigs)\n",
        "\n",
        "    f = lambda x: 0.5 * x.T @ A @ x - b.T @ x\n",
        "    grad_f = lambda x: A @ x - b\n",
        "\n",
        "    if mu_val <= 1e-2:\n",
        "        alpha = 1 / L_val\n",
        "    else:\n",
        "        alpha = 2 / (mu_val + L_val)\n",
        "    beta = (np.sqrt(L_val) - np.sqrt(mu_val)) / (np.sqrt(L_val) + np.sqrt(mu_val))\n",
        "\n",
        "    results = {\n",
        "        \"methods\": {\n",
        "            \"Gradient Descent\": gradient_descent(f, grad_f, x_0, alpha, params[\"iterations\"], x_opt),\n",
        "            \"Heavy Ball\": heavy_ball(f, grad_f, x_0, params[\"iterations\"], x_opt, mu_val, L_val),\n",
        "            \"NAG\": nesterov_accelerated_gradient(f, grad_f, x_0, params[\"iterations\"], x_opt, mu_val, L_val),\n",
        "        },\n",
        "        \"problem\": {\n",
        "            \"eigs\": eigs,\n",
        "            \"params\": params\n",
        "        }\n",
        "    }\n",
        "    return results\n",
        "\n",
        "def plot_results(results):\n",
        "    # Define linestyles for methods\n",
        "    linestyles = {\n",
        "        \"Gradient Descent\": \"r-\",\n",
        "        \"Heavy Ball\": \"g--\",\n",
        "        \"NAG\": \"m:\",\n",
        "    }\n",
        "    params = results[\"problem\"][\"params\"]\n",
        "    n = params[\"n\"]\n",
        "    problem_type = params[\"problem_type\"]\n",
        "    mu_val = params[\"mu\"]\n",
        "    L_val = params[\"L\"]\n",
        "\n",
        "    # Create a figure with 3 subplots for function gap, domain gap, and gradient norm\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.suptitle(f\"{'Strongly convex' if mu_val > 1e-2 else 'Convex'} quadratics: n={n}, {problem_type} matrix, μ={mu_val}, L={L_val}\")\n",
        "\n",
        "    # Plot 1: Function gap\n",
        "    plt.subplot(1, 3, 1)\n",
        "    for method, result in results[\"methods\"].items():\n",
        "        # result = (function_gap, domain_gap, grad_norms, alpha, beta)\n",
        "        plt.semilogy(result[0], linestyles[method])\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(r'$|f(x) - f^*|$')\n",
        "    plt.grid(linestyle=\":\")\n",
        "    plt.title(\"Function Gap\")\n",
        "\n",
        "    # Plot 2: Domain gap (||x - x*||)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    for method, result in results[\"methods\"].items():\n",
        "        plt.semilogy(result[1], linestyles[method])\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(r'$\\|x - x^*\\|_2$')\n",
        "    plt.grid(linestyle=\":\")\n",
        "    plt.title(\"Domain Gap\")\n",
        "\n",
        "    # Plot 3: Norm of Gradient\n",
        "    plt.subplot(1, 3, 3)\n",
        "    for method, result in results[\"methods\"].items():\n",
        "        if method == \"NAG\":\n",
        "            if mu_val > 1e-2:\n",
        "                label = f\"{method}. α={result[3]:.2e}, β={result[4]:.2e}\"\n",
        "            else:\n",
        "                label = f\"{method}\"\n",
        "        elif method == \"Heavy Ball\":\n",
        "            if mu_val > 1e-2:\n",
        "                label = f\"{method}. α={result[3]:.2e}, β={result[4]:.2e}\"\n",
        "            else:\n",
        "                label = f\"{method}\"\n",
        "        elif method == \"Gradient Descent\":\n",
        "            label = f\"{method}. α={result[3]:.2e}\"\n",
        "        plt.semilogy(result[2], linestyles[method], label=label)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(r'$\\|\\nabla f(x)\\|_2$')\n",
        "    plt.grid(linestyle=\":\")\n",
        "    plt.title(\"Gradient Norm\")\n",
        "\n",
        "    # Place the legend below the plots\n",
        "    plt.figlegend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.05))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"agd_{problem_type}_{mu_val}_{L_val}_{n}.pdf\", bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment parameters\n",
        "params = {\n",
        "    \"n\": 60,\n",
        "    \"mu\": 0,\n",
        "    \"L\": 10,\n",
        "    \"iterations\": 800,\n",
        "    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n",
        "}\n",
        "\n",
        "results = run_experiment(params)\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment parameters\n",
        "params = {\n",
        "    \"n\": 60,\n",
        "    \"mu\": 1,\n",
        "    \"L\": 10,\n",
        "    \"iterations\": 100,\n",
        "    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n",
        "}\n",
        "\n",
        "results = run_experiment(params)\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment parameters\n",
        "params = {\n",
        "    \"n\": 60,\n",
        "    \"mu\": 1,\n",
        "    \"L\": 1000,\n",
        "    \"iterations\": 1000,\n",
        "    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n",
        "}\n",
        "\n",
        "results = run_experiment(params)\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment parameters\n",
        "params = {\n",
        "    \"n\": 1000,\n",
        "    \"mu\": 1,\n",
        "    \"L\": 10,\n",
        "    \"iterations\": 100,\n",
        "    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n",
        "}\n",
        "\n",
        "results = run_experiment(params)\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment parameters\n",
        "params = {\n",
        "    \"n\": 1000,\n",
        "    \"mu\": 1,\n",
        "    \"L\": 1000,\n",
        "    \"iterations\": 1000,\n",
        "    \"problem_type\": \"random\",  # Change to \"clustered\", \"uniform spectrum\", or \"Hilbert\" as needed\n",
        "}\n",
        "\n",
        "results = run_experiment(params)\n",
        "plot_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logreg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:340: SyntaxWarning: invalid escape sequence '\\|'\n",
            "<>:340: SyntaxWarning: invalid escape sequence '\\|'\n",
            "/var/folders/6l/qhfv4nh50cqfd22s2mp1shlm0000gn/T/ipykernel_99724/1722798055.py:340: SyntaxWarning: invalid escape sequence '\\|'\n",
            "  plt.ylabel('$\\|x_k - x^*\\|$')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cvxpy as cp\n",
        "import jax\n",
        "from jax import numpy as jnp, grad\n",
        "from scipy.optimize import minimize_scalar\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, hessian\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from optax.losses import safe_softmax_cross_entropy as cros_entr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.optimize import minimize_scalar\n",
        "import sklearn.datasets as skldata\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(228)\n",
        "jax.random.PRNGKey(228)\n",
        "\n",
        "@jit\n",
        "def logistic_loss(w, X, y, mu=1):\n",
        "    m, n = X.shape\n",
        "    return jnp.sum(jnp.logaddexp(0, -y * (X @ w))) / m + mu / 2 * jnp.sum(w**2)\n",
        "\n",
        "def generate_problem(m=1000, n=300, mu=1):\n",
        "    X, y = skldata.make_classification(n_classes=2, n_features=n, n_samples=m, n_informative=n//2, random_state=0)\n",
        "    X = jnp.array(X)\n",
        "    y = jnp.array(y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def compute_optimal(X, y, mu):\n",
        "    w = cp.Variable(X.shape[1])\n",
        "    objective = cp.Minimize(cp.sum(cp.logistic(cp.multiply(-y, X @ w))) / len(y) + mu / 2 * cp.norm(w, 2)**2)\n",
        "    problem = cp.Problem(objective)\n",
        "    problem.solve()\n",
        "    return w.value, problem.value\n",
        "\n",
        "@jit\n",
        "def compute_accuracy(w, X, y):\n",
        "    # Compute predicted probabilities using the logistic (sigmoid) function\n",
        "    preds_probs = jax.nn.sigmoid(X @ w)\n",
        "    # Convert probabilities to class predictions: -1 if p < 0.5, else 1\n",
        "    preds = jnp.where(preds_probs < 0.5, 0, 1)\n",
        "    # Calculate accuracy as the average of correct predictions\n",
        "    accuracy = jnp.mean(preds == y)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "# @jit\n",
        "def compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu):\n",
        "    f = lambda w: logistic_loss(w, X_train, y_train, mu)\n",
        "    metrics = {\n",
        "        \"f_gap\": [jnp.abs(f(x) - f_star) for x in trajectory],\n",
        "        \"x_gap\": [jnp.linalg.norm(x - x_star) for x in trajectory],\n",
        "        \"time\": times,\n",
        "        \"train_acc\": [compute_accuracy(x, X_train, y_train) for x in trajectory],\n",
        "        \"test_acc\": [compute_accuracy(x, X_test, y_test) for x in trajectory],\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "def gradient_descent(w_0, X, y, learning_rate=0.01, num_iters=100, mu=0):\n",
        "    trajectory = [w_0]\n",
        "    times = [0]\n",
        "    w = w_0\n",
        "    f = lambda w: logistic_loss(w, X, y, mu)\n",
        "    iter_start = time.time()\n",
        "    for i in range(num_iters):\n",
        "        grad_val = grad(f)(w)\n",
        "        if learning_rate == \"linesearch\":\n",
        "            # Simple line search implementation\n",
        "            phi = lambda alpha: f(w - alpha*grad_val)\n",
        "            result = minimize_scalar(fun=phi, \n",
        "                                     bounds=(1e-3, 2e2)\n",
        "                              )\n",
        "            step_size = result.x\n",
        "        else:\n",
        "            step_size = learning_rate\n",
        "        w -= step_size * grad_val\n",
        "        iter_time = time.time()\n",
        "        trajectory.append(w)\n",
        "        times.append(iter_time - iter_start)\n",
        "    return trajectory, times\n",
        "\n",
        "def heavy_ball_method(w_0, X, y, learning_rate=0.01, momentum=0.9, num_iters=100, mu=0):\n",
        "    \"\"\"\n",
        "    Polyak Heavy-Ball Method implementation:\n",
        "    \n",
        "    Given:\n",
        "        x^+ = x - α ∇f(x)  (Gradient step)\n",
        "        d_k = β_k (x_k - x_{k-1})  (Momentum term)\n",
        "    \n",
        "    The update is:\n",
        "        x_{k+1} = x_k^+ + d_k = x_k - α ∇f(x_k) + d_k\n",
        "    \n",
        "    Parameters:\n",
        "        w_0         : initial point\n",
        "        X, y        : data used in the objective function (e.g. logistic loss)\n",
        "        learning_rate: α, step size for the gradient step\n",
        "        momentum   : β, momentum coefficient (can be constant or adjusted per iteration)\n",
        "        num_iters   : number of iterations to run\n",
        "        mu          : strong convexity parameter (not used in this explicit update)\n",
        "    \n",
        "    Returns:\n",
        "        trajectory  : list of iterates x_k (after the gradient update)\n",
        "        times       : list of times (elapsed) at each iteration\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE        \n",
        "    return\n",
        "    return trajectory, times\n",
        "\n",
        "def nesterov_accelerated_gradient(w_0, X, y, learning_rate=0.01, momentum=0.9, num_iters=100, mu=0):\n",
        "    \"\"\"\n",
        "    Nesterov Accelerated Gradient (NAG) implementation using explicit momentum:\n",
        "    \n",
        "    Given:\n",
        "        x^+ = x - α ∇f(x)  (Gradient step)\n",
        "        d_k = β_k (x_k - x_{k-1})  (Momentum term)\n",
        "    \n",
        "    The update is:\n",
        "        x_{k+1} = (x_k + d_k)^+ = (x_k + d_k) - α ∇f(x_k + d_k)\n",
        "    \n",
        "    Parameters:\n",
        "        w_0         : initial point\n",
        "        X, y        : data used in the objective function (e.g. logistic loss)\n",
        "        learning_rate: α, step size for the gradient step\n",
        "        momentum   : β, momentum coefficient (can be constant or adjusted per iteration)\n",
        "        num_iters   : number of iterations to run\n",
        "        mu          : strong convexity parameter (not used in this explicit update)\n",
        "    \n",
        "    Returns:\n",
        "        trajectory  : list of iterates x_k (after the gradient update)\n",
        "        times       : list of times (elapsed) at each iteration\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE        \n",
        "    return\n",
        "    return trajectory, times\n",
        "\n",
        "def run_experiments(params):\n",
        "    mu = params[\"mu\"]\n",
        "    m, n = params[\"m\"], params[\"n\"]\n",
        "    methods = params[\"methods\"]\n",
        "    results = {}\n",
        "\n",
        "    X_train, y_train, X_test, y_test = generate_problem(m, n, mu)\n",
        "    n_features = X_train.shape[1]  # Number of features\n",
        "    params[\"n_features\"] = n_features\n",
        "    \n",
        "    x_0 = jax.random.normal(jax.random.PRNGKey(0), (n_features, ))\n",
        "    x_star, f_star = compute_optimal(X_train, y_train, mu)\n",
        "\n",
        "    for method in methods:\n",
        "        if method[\"method\"] == \"GD\":\n",
        "            learning_rate = method[\"learning_rate\"]\n",
        "            iterations = method[\"iterations\"]\n",
        "            trajectory, times = gradient_descent(x_0, X_train, y_train, learning_rate, iterations, mu)\n",
        "            label = method[\"method\"] + \" \" + str(learning_rate)\n",
        "            results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n",
        "        elif method[\"method\"] == \"Heavy Ball\":\n",
        "            learning_rate   = method.get(\"learning_rate\", 0.01)\n",
        "            momentum        = method.get(\"momentum\", 0.9)\n",
        "            iterations = method[\"iterations\"]\n",
        "            trajectory, times = heavy_ball_method(x_0, X_train, y_train, learning_rate, momentum, iterations, mu)\n",
        "            # label = method[\"method\"] + \" \" + str(learning_rate)\n",
        "            label = f\"{method['method']}. α={learning_rate:.2e}. β={momentum:.2e}\"\n",
        "            results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n",
        "        \n",
        "        elif method[\"method\"] == \"NAG\":\n",
        "            learning_rate   = method.get(\"learning_rate\", 0.01)\n",
        "            momentum        = method.get(\"momentum\", 0.9)\n",
        "            iterations = method[\"iterations\"]\n",
        "            trajectory, times = nesterov_accelerated_gradient(x_0, X_train, y_train, learning_rate, momentum, iterations, mu)\n",
        "            label = method[\"method\"] + \" \" + str(learning_rate)\n",
        "            label = f\"{method['method']}. α={learning_rate:.2e}. β={momentum:.2e}\"\n",
        "            results[label] = compute_metrics(trajectory, x_star, f_star, times, X_train, y_train, X_test, y_test, mu)\n",
        "\n",
        "    return results, params\n",
        "\n",
        "def plot_results(results, params):\n",
        "    plt.figure(figsize=(11, 2.6))\n",
        "    mu = params[\"mu\"]\n",
        "    \n",
        "    if mu > 1e-2:\n",
        "        plt.suptitle(f\"Strongly convex binary logistic regression. mu={mu}.\")\n",
        "    else:\n",
        "        plt.suptitle(f\"Convex binary logistic regression. mu={mu}.\")\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    for method, metrics in results.items():\n",
        "        plt.plot(metrics['f_gap'])\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel(r'$|f(x) -f^*|$')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(linestyle=\":\")\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    for method, metrics in results.items():\n",
        "        plt.plot(metrics['x_gap'], label=method)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('$\\|x_k - x^*\\|$')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(linestyle=\":\")\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    for method, metrics in results.items():\n",
        "        plt.plot(metrics[\"train_acc\"])\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Train accuracy')\n",
        "    # plt.yscale('log')\n",
        "    plt.grid(linestyle=\":\")\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    for method, metrics in results.items():\n",
        "        plt.plot(metrics[\"test_acc\"])\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Test accuracy')\n",
        "    # plt.yscale('log')\n",
        "    plt.grid(linestyle=\":\")\n",
        "\n",
        "    # Place the legend below the plots\n",
        "    plt.figlegend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.00))\n",
        "    # Adjust layout to make space for the legend below\n",
        "    filename = \"\"\n",
        "    for method, metrics in results.items():\n",
        "        filename += method\n",
        "    filename += f\"_{mu}.pdf\"\n",
        "    plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"mu\": 0,\n",
        "    \"m\": 1000,\n",
        "    \"n\": 100,\n",
        "    \"methods\": [\n",
        "        {\n",
        "            \"method\": \"GD\",\n",
        "            \"learning_rate\":9e4,\n",
        "            \"iterations\": 400,\n",
        "        },\n",
        "        {\n",
        "            \"method\": \"Heavy Ball\",\n",
        "            \"learning_rate\": 9e-1,\n",
        "            \"iterations\": 400,\n",
        "            \"momentum\": 0.99,\n",
        "        },\n",
        "        {\n",
        "            \"method\": \"NAG\",\n",
        "            \"learning_rate\": 9e-1,\n",
        "            \"iterations\": 400,\n",
        "            \"momentum\": 0.99,\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "results, params = run_experiments(params)\n",
        "plot_results(results, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"mu\": 1,\n",
        "    \"m\": 1000,\n",
        "    \"n\": 100,\n",
        "    \"methods\": [\n",
        "        {\n",
        "            \"method\": \"GD\",\n",
        "            \"learning_rate\": 5e-2,\n",
        "            \"iterations\": 300,\n",
        "        },\n",
        "        {\n",
        "            \"method\": \"Heavy Ball\",\n",
        "            \"learning_rate\": 5e-2,\n",
        "            \"iterations\": 300,\n",
        "            \"momentum\": 0.9,\n",
        "        },\n",
        "        {\n",
        "            \"method\": \"NAG\",\n",
        "            \"learning_rate\": 5e-2,\n",
        "            \"iterations\": 300,\n",
        "            \"momentum\": 0.9,\n",
        "        },\n",
        "    ]\n",
        "}\n",
        "\n",
        "results, params = run_experiments(params)\n",
        "plot_results(results, params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hse25_3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
